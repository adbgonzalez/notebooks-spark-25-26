{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baf23f8-79a5-4b7e-96d3-0790839a46fc",
   "metadata": {},
   "source": [
    "# Caso 1: Open-Meteo\n",
    "Neste caso práctico imos obter datos meteorolóxicos desde unha API pública (Open-Meteo). O proxecto terá os seguintes compoñentes:\n",
    "- Produtor Kafka. Programa en Python que accederá á API a través do módulo *requests* e producirá os resultados nun *topic* ao que accederá despois o noso programa en Spark.\n",
    "- Programa en Spark. Programa que recompila os datos de Kafka e realiza agregacións por xanelas de tempo. En función de onde se envíen os resultados teremos dúas variantes:\n",
    "  - Á consola.\n",
    "  - A ficheiro Parquet.\n",
    "- Por último, teremos un programa *echo* en *Streamlit* que mostrará de forma visual os datos recompilados.\n",
    "\n",
    "## productor_open_meteo.py\n",
    "Necesitamos as seguintes importacións:\n",
    "- **KafkaProducer** do módulo **kafka**.\n",
    "- **dumps** do paquete **json**.\n",
    "- **requests**: para realizar peticións á API.\n",
    "- **time**.\n",
    "- **datetime**.\n",
    "\n",
    "Os pasos a seguir serán os seguintes:\n",
    "1. Crear un produtor Kafka.\n",
    "2. Facer unha chamada á API de Open-Meteo.\n",
    "3. Transformar os datos a JSON e formatealos.\n",
    "4. Usar o método `send()` da clase *KafkaProducer* para escribir os datos nun *topic*.\n",
    "\n",
    "O primeiro será crear o produtor Kafka indicando o **bootstrap-server** e a codificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad1ed2-bcb8-4867-9e5d-ceb944c45199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:9092'],\n",
    "    value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfef2f-e8ca-4993-bee3-d948bbe315c6",
   "metadata": {},
   "source": [
    "Imos empregar a API de Open-Meteo. O primeiro paso sería consultar a [documentación oficial](https://open-meteo.com/en/docs). Na sección **API Documentation** vemos un listado dos parámetros dispoñibles, entre eles *latitude* e *longitude*, que son obrigatorios.\n",
    "\n",
    "Vemos que as variables están distribuídas en varios grupos: *daily weather*, *hourly weather*, *current weather*... Interésanos este último.\n",
    "\n",
    "Para comunicarnos coa API imos empregar o módulo **requests**. Especificamos a URL da API e os parámetros (pares clave-valor) e chamamos ao método **get()**. Convertimos a resposta a *json* para explorar a súa estrutura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8535227-74ce-43d5-a1af-4223d20e4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"latitude\": 42.3125,\n",
      "  \"longitude\": -7.875,\n",
      "  \"generationtime_ms\": 0.06401538848876953,\n",
      "  \"utc_offset_seconds\": 0,\n",
      "  \"timezone\": \"GMT\",\n",
      "  \"timezone_abbreviation\": \"GMT\",\n",
      "  \"elevation\": 147.0,\n",
      "  \"current_weather_units\": {\n",
      "    \"time\": \"iso8601\",\n",
      "    \"interval\": \"seconds\",\n",
      "    \"temperature\": \"\\u00b0C\",\n",
      "    \"windspeed\": \"km/h\",\n",
      "    \"winddirection\": \"\\u00b0\",\n",
      "    \"is_day\": \"\",\n",
      "    \"weathercode\": \"wmo code\"\n",
      "  },\n",
      "  \"current_weather\": {\n",
      "    \"time\": \"2026-02-10T11:30\",\n",
      "    \"interval\": 900,\n",
      "    \"temperature\": 16.2,\n",
      "    \"windspeed\": 13.6,\n",
      "    \"winddirection\": 230,\n",
      "    \"is_day\": 1,\n",
      "    \"weathercode\": 61\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 42.33669,        # Coordenadas de Ourense\n",
    "    \"longitude\": -7.86407,\n",
    "    \"current_weather\": \"true\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.ok:\n",
    "    data = response.json()\n",
    "    print(json.dumps(data, indent=2))\n",
    "else:\n",
    "    print(\"Erro:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35ed4e-1d6b-4d8e-9247-aecc3710da3d",
   "metadata": {},
   "source": [
    "Unha vez que temos a resposta, o seguinte paso é obter os datos que nos interesan, no noso caso *current_weather*. Engadímoslle o nome da cidade e un *timestamp* normalizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5171986-7de2-448b-a691-c7279763b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '2026-02-10T11:30', 'interval': 900, 'temperature': 16.2, 'windspeed': 13.6, 'winddirection': 230, 'is_day': 1, 'weathercode': 61, 'city': 'Ourense', 'timestamp': '2026-02-10T11:43:56.073797'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "data = response.json()[\"current_weather\"]\n",
    "\n",
    "data[\"city\"]=\"Ourense\"\n",
    "data[\"timestamp\"] = datetime.utcnow().isoformat()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8164ff-3c9d-47b7-8b31-1a27e395816e",
   "metadata": {},
   "source": [
    "Unha vez que teñamos os datos no formato que queremos, almacenados nunha variable (no exemplo *data*), escribímolos no *topic* indicado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fe3d7-6072-444a-bf3a-45b5c8dd3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "#...\n",
    "\n",
    "producer.send(\"topic\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbf8e3-7b5d-44ad-bd23-7c62b134589f",
   "metadata": {},
   "source": [
    "Pódese facer o mesmo con varias localizacións empregando un bucle.\n",
    "\n",
    "## Programa Spark\n",
    "A estrutura do programa en *Spark* é moi similar á dos notebooks vistos previamente:\n",
    "1. Inicializamos a sesión de Spark: temos que incluír os paquetes necesarios para usar *Kafka*.\n",
    "2. Definimos o esquema do *DataFrame* inicial.\n",
    "3. Definimos o fluxo de entrada. Neste caso temos que indicar, entre outros, o **bootstrap-server** e o nome do **topic** do que se lerán os datos.\n",
    "4. Creamos un *DataFrame* novo transformando os datos iniciais. Hai que ter en conta as limitacións para distintos tipos de agregacións (especificadas no documento 00 e na [documentación oficial](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)).\n",
    "5. Iniciamos o procesamento, como sempre, indicando o destino e o modo.\n",
    "\n",
    "### Exemplo de inicialización da sesión de Spark:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenMeteoStreamingClean\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "### Exemplo de definición de esquema:\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenMeteoStreamingClean\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Exemplo de definición de fluxo de entrada:\n",
    "\n",
    "```python\n",
    "# Ejemplo Open-Meteo\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-1:9092\") \\\n",
    "    .option(\"subscribe\", \"open-meteo-weather\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Ejemplo general\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"servidor-kafka\") \\\n",
    "    .option(\"subscribe\", \"nombre-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "```\n",
    "\n",
    "### Exemplo de transformación de datos:\n",
    "```python\n",
    "# Transformación simple: filtrado e limpieza.\n",
    "df_parsed = df_kafka.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\n",
    "        col(\"data.city\"),\n",
    "        col(\"data.temperature\"),\n",
    "        col(\"data.windspeed\"),\n",
    "        col(\"data.winddirection\"),\n",
    "        to_timestamp(col(\"data.local_timestamp\")).alias(\"event_time\")\n",
    "    )\n",
    "\n",
    "#Transformación adicional empregando windowing e watermarking:\n",
    "# Agrupación por xanela e cidade\n",
    "df_grouped = df_parsed \\\n",
    "    .withWatermark(\"event_time\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"city\")\n",
    "    ).agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        avg(\"windspeed\").alias(\"avg_wind\")\n",
    "    )\n",
    "\n",
    "```\n",
    "### Exemplo de inicio do procesamento en streaming:\n",
    "#### Saída a consola\n",
    "```python\n",
    "# Mostrar en consola\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "#### Saída a un arquivo parquet\n",
    "```python\n",
    "# Almacenar en parquet:\n",
    "query=df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://spark-master:9000/user/jovyan/weather_aggregated/\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master:9000/user/jovyan/checkpoint_weather_aggregated/\") \\\n",
    "    .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d7366-cc69-4b5f-8644-7d71d9361687",
   "metadata": {},
   "source": [
    "Neste cartafol hai exemplos de produtor e de programa en Spark. Lembrade que, para que funcione ao executalos con *spark-submit*, hai que engadir a seguinte opción:  \n",
    "`--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76559dc8-34b6-4626-b645-318988cb410a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
