{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4079f44-0788-4243-af2c-fc7ea9dea98c",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "Os **DataFrames** son unha parte fundamental da **API estruturada de Spark** (*Spark Structured API*). Esta API permite manipular datos **non estruturados**, **semi-estruturados** (por exemplo, CSV ou JSON) e **altamente estruturados** (por exemplo, Parquet ou táboas).\n",
    "\n",
    "Dentro da API estruturada considéranse tres tipos principais de coleccións distribuídas:\n",
    "\n",
    "- **Datasets**: son a versión con **tipado máis forte** dos DataFrames. Permiten comprobación de tipos en tempo de compilación e ofrecen maior seguridade. Só están dispoñibles en linguaxes baseadas na JVM, como **Scala** e **Java**.\n",
    "\n",
    "- **DataFrames**: coleccións distribuídas con forma de **táboa**, con filas e columnas ben definidas, onde cada fila é un rexistro e cada columna ten un nome e un tipo. A diferenza dos Datasets, o tipado é **menos estrito** e a validación complétase principalmente en **tempo de execución**.\n",
    "\n",
    "- **Táboas e vistas SQL**: pódense crear **táboas** ou **vistas temporais** a partir de DataFrames, permitindo executar consultas SQL sobre elas. A función `spark.sql()` permite lanzar consultas SQL directamente cando estas táboas/vistas están rexistradas no catálogo.\n",
    "\n",
    "---\n",
    "\n",
    "## Proceso de execución na API estruturada\n",
    "\n",
    "O proceso que se segue para executar un traballo usando DataFrames, Datasets ou SQL é o seguinte:\n",
    "\n",
    "1. Escríbese o código para manipular **DataFrames / Datasets / SQL**.\n",
    "2. Se o código é válido, Spark constrúe un **Plan Lóxico** (*Logical Plan*).\n",
    "3. Spark transforma o Plan Lóxico nun **Plan Físico** (*Physical Plan*), aplicando optimizacións durante o proceso.\n",
    "4. Spark executa o Plan Físico no clúster, converténdoo en tarefas distribuídas que se procesan en paralelo.\n",
    "\n",
    "---\n",
    "\n",
    "## O Plan Lóxico\n",
    "\n",
    "O **Plan Lóxico** representa unha serie de transformacións abstractas, sen facer referencia a *executores*, *drivers* ou recursos físicos do clúster.  \n",
    "\n",
    "A partir do código do usuario xérase inicialmente un **plan lóxico sen resolver** (*Unresolved Logical Plan*). Este plan contén referencias a columnas, táboas ou funcións, pero aínda non se validou se existen ou se son correctas.\n",
    "\n",
    "Despois, Spark analiza ese plan co apoio do **Catálogo** (*Catalog*), un repositorio que contén metadatos sobre:\n",
    "- táboas\n",
    "- vistas\n",
    "- columnas e tipos\n",
    "- funcións rexistradas\n",
    "- bases de datos\n",
    "\n",
    "Este proceso dá lugar a un **plan lóxico resolto** (*Resolved Logical Plan*), no que xa se comprobaron nomes e tipos. A continuación, Spark aplica optimizacións (por exemplo eliminación de columnas non usadas, reordenación de filtros, simplificación de expresións...) para obter o **Plan Lóxico Optimizado** (*Optimized Logical Plan*).\n",
    "\n",
    "![Plan lóxico](./images/logicalplan.png)\n",
    "\n",
    "---\n",
    "\n",
    "## O Plan Físico\n",
    "\n",
    "O **Plan Físico** (tamén chamado *Spark Plan*) describe como se executará realmente o Plan Lóxico no clúster. Neste punto Spark decide:\n",
    "\n",
    "- que estratexia de execución se vai empregar (por exemplo, tipo de join)\n",
    "- como se van repartir os datos entre particións\n",
    "- cando se vai producir un shuffle\n",
    "- que operadores físicos se van usar\n",
    "\n",
    "Para isto, Spark adoita xerar **varias estratexias alternativas** e escoller a mellor empregando un **modelo de custo** (baseado en estimacións do tamaño de datos, estatísticas e custo de operacións).\n",
    "\n",
    "![Plan físico](./images/physicalplan.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Resultado final\n",
    "\n",
    "O resultado final do proceso é un conxunto de operacións físicas que Spark executa de forma distribuída no clúster. Internamente, estas operacións tradúcense en **tarefa distribuídas** que traballan sobre estruturas de baixo nivel (historicamente baseadas en *RDDs*, aínda que en execución moderna tamén se apoian en operadores optimizados e en xestión interna de memoria).\n",
    "\n",
    "En definitiva, Spark transforma consultas realizadas sobre **DataFrames / Datasets / SQL** nun conxunto de operacións distribuídas executadas en paralelo no clúster, aplicando optimizacións para mellorar o rendemento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d3d662-7f60-4462-a752-4f86428df707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-46a39b19-2835-4903-b42a-3bfdcad488fb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.1.0/delta-spark_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.1.0!delta-spark_2.12.jar (1382ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.7/spark-sql-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7!spark-sql-kafka-0-10_2.12.jar (422ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.5.1!kafka-clients.jar (993ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.1.0/delta-storage-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.1.0!delta-storage.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (248ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.7/spark-token-provider-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7!spark-token-provider-kafka-0-10_2.12.jar (103ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (109ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (123ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (3080ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2071ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.5!snappy-java.jar(bundle) (418ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.5.5-1!zstd-jni.jar (893ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (157ms)\n",
      ":: resolution report :: resolve 8278ms :: artifacts dl 10275ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   15  |   15  |   3   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-46a39b19-2835-4903-b42a-3bfdcad488fb\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (68764kB/48ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializamos SparkSession e SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"03-Dataframes\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)  # Verifica a versión de Spark\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bc0cb-2041-4de4-9b39-3b8327ffbe4a",
   "metadata": {},
   "source": [
    "## Creación de DataFrames\n",
    "\n",
    "Pódense crear **DataFrames** a partir de diversas fontes. Un dos métodos máis habituais consiste en construílos a partir dun **RDD**, convertendo os seus elementos en filas e asignándolles un esquema (explícito ou inferido).\n",
    "\n",
    "### A partir dun RDD: `toDF()`\n",
    "\n",
    "O método `toDF()` permite converter un RDD nun DataFrame. Para poder utilizalo en PySpark, é necesario importar previamente as funcións implicadas e, en moitos casos, indicar os nomes das columnas.\n",
    "\n",
    "O proceso pode facerse de dúas maneiras principais:\n",
    "\n",
    "- Convertendo un RDD de tuplas e indicando o nome das columnas.\n",
    "- Convertendo un RDD de obxectos/fichas e definindo un esquema de forma explícita (con `StructType`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ae0545-2f1e-41cf-a251-2cfdb860c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame con números consecutivos usando spark.range()\n",
    "\n",
    "# Xeramos un DataFrame con valores do 0 ao 49 e convertímolo para asignarlle un nome á columna\n",
    "df = spark.range(50).toDF(\"number\")\n",
    "\n",
    "# Amosamos o contido do DataFrame xerado\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa9bccc-f6a7-45bd-8676-76a78cf0d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame creado a partir dun RDD de tuplas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| nome|idade|\n",
      "+---+-----+-----+\n",
      "|  1|  Ana|   22|\n",
      "|  2|Brais|   25|\n",
      "|  3|Carla|   21|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame a partir dun RDD de tuplas usando toDF()\n",
    "\n",
    "# Creamos un RDD de tuplas (cada tupla será unha fila)\n",
    "rdd = sc.parallelize([\n",
    "    (1, \"Ana\", 22),\n",
    "    (2, \"Brais\", 25),\n",
    "    (3, \"Carla\", 21)\n",
    "])\n",
    "\n",
    "# Convertimos o RDD nun DataFrame indicando os nomes das columnas\n",
    "df = rdd.toDF([\"id\", \"nome\", \"idade\"])\n",
    "\n",
    "# Amosamos o DataFrame resultante\n",
    "print(\"DataFrame creado a partir dun RDD de tuplas:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85ee2a-b7d4-4f04-8996-e8993a3a6577",
   "metadata": {},
   "source": [
    "### A partir dun arquivo (lectura con `spark.read`)\n",
    "\n",
    "Un dos métodos máis habituais para crear DataFrames é cargalos desde ficheiros almacenados no sistema de ficheiros local, HDFS, S3, etc. Para iso utilízase o obxecto `spark.read`, que ofrece métodos específicos para distintos formatos.\n",
    "\n",
    "O patrón xeral é sempre o mesmo:\n",
    "\n",
    "1. Selecciónase o formato (`json`, `csv`, `parquet`, etc.)\n",
    "2. Indícase a ruta ao ficheiro ou directorio\n",
    "3. Opcionalmente configúranse opcións (por exemplo, `header`, `inferSchema`, `sep`, etc.)\n",
    "4. Obtense un DataFrame que xa pode ser consultado con `show()`, `printSchema()`, `select()`, etc.\n",
    "\n",
    "En moitos formatos (por exemplo CSV ou JSON) Spark pode inferir o esquema automaticamente, aínda que tamén se pode definir un esquema explícito para mellorar rendemento e evitar erros.\n",
    "\n",
    "- json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8350a3a2-298b-4232-9db0-d573131142db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame a partir dun ficheiro JSON almacenado en HDFS\n",
    "\n",
    "# Cargamos un ficheiro JSON desde HDFS e creamos automaticamente un DataFrame\n",
    "df2 = spark.read.json(\"hdfs:///data/flight-data/json/2010-summary.json\")\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame para verificar que os datos se cargaron correctamente\n",
    "df2.show()\n",
    "\n",
    "# Amosamos o esquema inferido por Spark (nomes de columnas e tipos de datos)\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece556d5-e35e-4025-b59c-66e14a57d825",
   "metadata": {},
   "source": [
    "- csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf968800-72bd-46d1-b85e-4cec13b7594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n",
      "Número de particións do DataFrame:  1\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame a partir dun ficheiro CSV en HDFS e controlar o número de particións\n",
    "\n",
    "# Cargamos un ficheiro CSV desde HDFS indicando que a primeira liña contén os nomes das columnas (header)\n",
    "# Despois reducimos o número de particións a 5 usando coalesce para evitar un shuffle innecesario\n",
    "df_csv = spark.read.option(\"header\", True).csv(\"hdfs:///data/flight-data/csv/2015-summary.csv\").coalesce(5)\n",
    "\n",
    "# Amosamos as 10 primeiras filas do DataFrame para comprobar a lectura e o contido\n",
    "df_csv.show(10)\n",
    "\n",
    "# Amosamos o esquema do DataFrame (os tipos poden quedar como string se non se infire o esquema)\n",
    "df_csv.printSchema()\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame converténdoo a RDD\n",
    "num_particions = df_csv.rdd.getNumPartitions()\n",
    "\n",
    "# Amosamos o resultado final: número de particións nas que está dividido o DataFrame\n",
    "print(\"Número de particións do DataFrame: \", num_particions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304fc21-2ae7-4491-a6b5-9e4dc2cce4a8",
   "metadata": {},
   "source": [
    "- parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859c6885-2349-4f67-8326-759b239c9f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame a partir dun ficheiro Parquet almacenado en HDFS\n",
    "\n",
    "# Cargamos un ficheiro Parquet desde HDFS e creamos o DataFrame\n",
    "# Parquet é un formato columnar e inclúe o esquema, polo que Spark non precisa inferilo\n",
    "df_parquet = spark.read.parquet(\"hdfs:///data/flight-data/parquet/2010-summary.parquet\")\n",
    "\n",
    "# Amosamos as 10 primeiras filas do DataFrame para comprobar a lectura e o contido\n",
    "df_parquet.show(10)\n",
    "\n",
    "# Amosamos o esquema do DataFrame (en Parquet xa vén definido dentro do propio ficheiro)\n",
    "df_parquet.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4fe5e-64d7-44a7-86a7-24d7614fffaf",
   "metadata": {},
   "source": [
    "### Esquemas en DataFrames\n",
    "\n",
    "Aínda que os **DataFrames** non sexan tan fortemente tipados como os **Datasets**, Spark permite asignar un **tipo de dato** a cada columna (campo) mediante un **esquema** (*schema*). A diferenza dos Datasets, a comprobación de tipos realízase principalmente en **tempo de execución**, pero contar cun esquema explícito mellora a calidade e fiabilidade do procesamento.\n",
    "\n",
    "Para coñecer o esquema dun DataFrame pódese usar o método `printSchema()`.\n",
    "\n",
    "Con respecto ao esquema, existen dúas opcións principais:\n",
    "- deixar que Spark o **infira automaticamente** (por exemplo, usando `inferSchema=True` en CSV)\n",
    "- **especificar explicitamente o esquema**, definindo o nome, tipo e nulabilidade de cada campo\n",
    "\n",
    "Definir o esquema de forma explícita adoita ser recomendable porque:\n",
    "- evita erros de inferencia (por exemplo, números que se len como cadeas)\n",
    "- mellora o rendemento, xa que Spark non precisa analizar os datos para deducir tipos\n",
    "- garante consistencia se os datos veñen de fontes con variacións ou inconsistencias\n",
    "\n",
    "A continuación móstrase un exemplo no que se define un esquema manualmente usando `StructType` e `StructField`, e despois se le un CSV empregando ese esquema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fc57f2-5cf1-4d2d-ab13-418772ae9883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Definir un esquema explícito e ler un CSV aplicando ese esquema\n",
    "\n",
    "# Importamos as clases necesarias para definir esquemas\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Definimos o esquema indicando o nome, tipo e se cada campo admite valores nulos (nullable)\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Lemos o arquivo CSV desde HDFS aplicando o esquema especificado\n",
    "# Ao fornecer schema, Spark non precisa inferir tipos e asigna directamente os definidos\n",
    "df = spark.read.csv(\n",
    "    \"hdfs:///data/retail-data/all/online-retail-dataset.csv\",\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Amosamos o esquema do DataFrame para comprobar que se aplicou correctamente\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b104d1c-5c97-4239-a444-2e0ad8698394",
   "metadata": {},
   "source": [
    "Outra opción para asignar tipos ás columnas dun DataFrame é deixar que Spark **infira o esquema automaticamente**. Isto resulta cómodo cando non se coñece previamente a estrutura exacta dos datos ou cando se quere facer unha proba rápida.\n",
    "\n",
    "Para iso emprégase a opción:\n",
    "\n",
    "- `inferSchema=True` (ou `\"true\"`)\n",
    "\n",
    "Con esta opción, Spark analiza unha mostra dos datos e intenta deducir o tipo máis axeitado para cada columna (por exemplo `IntegerType`, `DoubleType`, etc.).\n",
    "\n",
    "Esta alternativa ten algunhas consideracións:\n",
    "- é máis lenta que definir un esquema explícito, porque Spark debe examinar os datos\n",
    "- pode inferir tipos incorrectos se hai valores inconsistentes ou faltan datos\n",
    "- é útil para exploración inicial ou prototipado, pero en produción adoita preferirse un esquema explícito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a39d8be-da09-4cd0-844f-a4b6d965e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "Número de particións do DataFrame:  1\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Ler un CSV inferindo automaticamente o esquema e controlando o número de particións\n",
    "\n",
    "# Lemos o CSV desde HDFS indicando que a primeira liña é a cabeceira\n",
    "# Activamos a inferencia do esquema para que Spark detecte automaticamente os tipos das columnas\n",
    "# Reducimos o número de particións a 5 usando coalesce para evitar un shuffle innecesario\n",
    "df_csv = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"hdfs:///data/flight-data/csv/2015-summary.csv\")\n",
    "        .coalesce(5)\n",
    ")\n",
    "\n",
    "# Amosamos as 10 primeiras filas do DataFrame para comprobar a lectura\n",
    "df_csv.show(10)\n",
    "\n",
    "# Amosamos o esquema inferido por Spark (nomes e tipos das columnas)\n",
    "df_csv.printSchema()\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame converténdoo a RDD\n",
    "num_particions = df_csv.rdd.getNumPartitions()\n",
    "\n",
    "# Amosamos o resultado final: número de particións nas que está dividido o DataFrame\n",
    "print(\"Número de particións do DataFrame: \", num_particions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7c0a1-0546-48c0-8c15-87f165df2256",
   "metadata": {},
   "source": [
    "## Persistencia de datos\n",
    "\n",
    "Spark permite almacenar os datos procesados en distintos formatos de ficheiro empregando a API de escritura de DataFrames, accesible mediante `DataFrame.write`. Segundo o formato desexado, pódense empregar métodos como `write.json()`, `write.csv()`, `write.parquet()`, etc.\n",
    "\n",
    "A escritura de datos segue un patrón xeral:\n",
    "1. Selecciónase o formato de saída (JSON, CSV, Parquet…).\n",
    "2. Indícase a ruta de destino.\n",
    "3. Configúranse opcións adicionais (por exemplo, cabeceira, separador, compresión…).\n",
    "4. Establécese o modo de escritura, que determina como actuar se xa existe un directorio de saída.\n",
    "\n",
    "### Opcións frecuentes en `DataFrame.write`\n",
    "\n",
    "Algúns parámetros habituais ao gardar DataFrames son:\n",
    "\n",
    "- `header`: permite indicar se se garda ou non a cabeceira (especialmente relevante en CSV).\n",
    "- `mode`: permite definir o comportamento cando o destino xa existe.\n",
    "  - `mode(\"error\")` ou `mode(\"errorifexists\")`: falla se xa existe (por defecto).\n",
    "  - `mode(\"overwrite\")`: sobrescribe a saída.\n",
    "  - `mode(\"append\")`: engade datos á saída existente.\n",
    "  - `mode(\"ignore\")`: ignora a escritura se xa existe.\n",
    "\n",
    "> NOTA: Os arquivos deben almacearse en HDFS. Se non existe pódese crear un directorio `output`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569230d5-c07e-4e8e-846f-1c29856dc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032cd9ae-3735-40c0-b30a-58ef6f954fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gardado en CSV en:  /output/datos_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Gardar un DataFrame en formato CSV incluíndo cabeceira e sobrescribindo a saída\n",
    "\n",
    "# Gardamos o DataFrame en CSV no directorio de saída\n",
    "# Spark creará un directorio chamado datos_csv con varios ficheiros part-...\n",
    "df_csv.write.csv(\"/output/datos_csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Amosamos unha mensaxe final para confirmar a escritura\n",
    "print(\"DataFrame gardado en CSV en: \", \"/output/datos_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cad27dc-9e67-4e05-a4c0-58baf3acb935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gardado en JSON en:  /output/datos_json\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Gardar un DataFrame en formato JSON sobrescribindo a saída\n",
    "\n",
    "# Gardamos o DataFrame en JSON\n",
    "# Spark creará un directorio chamado datos_json con varios ficheiros part-...\n",
    "df_csv.write.json(\"/output/datos_json\", mode=\"overwrite\")\n",
    "\n",
    "# Amosamos unha mensaxe final para confirmar a escritura\n",
    "print(\"DataFrame gardado en JSON en: \", \"/output/datos_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c831630c-3146-4883-86a5-a4f368a4a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gardado en Parquet en:  /output/datos_parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Gardar un DataFrame en formato Parquet (recomendado para Big Data)\n",
    "\n",
    "# Parquet é un formato columnar, moi eficiente en compresión e lectura selectiva\n",
    "# Spark crea un directorio con ficheiros part-...\n",
    "df_csv.write.parquet(\"/output/datos_parquet\", mode=\"overwrite\")\n",
    "\n",
    "# Amosamos unha mensaxe final para confirmar a escritura\n",
    "print(\"DataFrame gardado en Parquet en: \", \"/output/datos_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ab1cb9-a0ad-437b-9943-072d59f6117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gardado en CSV comprimido (gzip) en:  /output/datos_csv_gzip\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Gardar un CSV cun separador distinto e compresión\n",
    "\n",
    "# Gardamos o DataFrame en CSV usando punto e coma como separador e comprimindo en gzip\n",
    "df_csv.write.option(\"header\", True) \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .option(\"compression\", \"gzip\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .csv(\"/output/datos_csv_gzip\")\n",
    "\n",
    "# Amosamos unha mensaxe final para confirmar a escritura\n",
    "print(\"DataFrame gardado en CSV comprimido (gzip) en: \", \"/output/datos_csv_gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29a81626-2b10-48ff-bf40-a4de5c00c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gardado en CSV cunha soa partición en:  /output/datos_csv_un_ficheiro\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Gardar o DataFrame nun único ficheiro (coalesce(1))\n",
    "# Nota: Isto reduce o paralelismo e non se recomenda en datasets grandes\n",
    "\n",
    "# Reducimos o DataFrame a unha única partición para xerar un único ficheiro part-...\n",
    "df_un_ficheiro = df_csv.coalesce(1)\n",
    "\n",
    "# Gardamos en CSV\n",
    "df_un_ficheiro.write.csv(\"/output/datos_csv_un_ficheiro\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Amosamos unha mensaxe final para confirmar a escritura\n",
    "print(\"DataFrame gardado en CSV cunha soa partición en: \", \"/output/datos_csv_un_ficheiro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b655c0-4f20-4e4f-8791-fe38521e5c34",
   "metadata": {},
   "source": [
    "## Transformacións en DataFrames\n",
    "\n",
    "A continuación móstranse algunhas das principais transformacións dispoñibles para **DataFrames** en Spark. Estas operacións permiten seleccionar columnas, filtrar filas, crear columnas novas, agrupar datos ou ordenar resultados, entre outras accións típicas do procesamento e análise de datos.\n",
    "\n",
    "### `select()`\n",
    "\n",
    "A transformación `select()` permite escoller columnas dun DataFrame, de forma equivalente á cláusula **SELECT** das bases de datos relacionais. O resultado é un novo DataFrame que contén só as columnas indicadas (ou expresións calculadas a partir delas).\n",
    "\n",
    "Existen dúas variantes habituais:\n",
    "- **`select()`**: permite seleccionar columnas polo nome ou usando expresións.\n",
    "- **`selectExpr()`**: permite seleccionar columnas empregando expresións SQL como cadeas de texto (útil para operacións rápidas ou expresións complexas).\n",
    "\n",
    "A continuación móstranse exemplos de uso de `select()` e `selectExpr()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78a9ef5d-9a02-4034-b843-8a9d07ca8f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "|    United States|          Singapore|\n",
      "|    United States|            Grenada|\n",
      "|       Costa Rica|      United States|\n",
      "|          Senegal|      United States|\n",
      "|          Moldova|      United States|\n",
      "+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Seleccionar columnas dun DataFrame usando select()\n",
    "\n",
    "# Supoñemos un DataFrame chamado df_csv\n",
    "# Seleccionamos dúas columnas do DataFrame\n",
    "df_sel = df_csv.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\")\n",
    "\n",
    "# Amosamos as primeiras filas do novo DataFrame\n",
    "df_sel.show(10)\n",
    "\n",
    "# Amosamos o esquema do resultado para comprobar as columnas seleccionadas\n",
    "df_sel.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3ae7f-f10c-47bc-84b6-d177daa761db",
   "metadata": {},
   "source": [
    "#### A función `expr()`\n",
    "\n",
    "A función `expr()` permite escribir expresións en formato SQL dentro da API de DataFrames. Isto resulta útil cando se quere empregar unha sintaxe familiar de SQL para realizar operacións como:\n",
    "- renomear columnas con `AS` (alias)\n",
    "- aplicar operacións aritméticas\n",
    "- empregar funcións SQL\n",
    "- crear columnas calculadas\n",
    "\n",
    "`expr()` intégrase con `select()`, `withColumn()`, `filter()`, etc., e permite construír operacións complexas cunha sintaxe compacta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250ce9cf-9c23-479d-9557-b4d3f20b1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|        Egypt|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar expr() para renomear unha columna (alias) nun select()\n",
    "\n",
    "# Importamos as funcións necesarias para traballar con expresións e columnas\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# Seleccionamos a columna DEST_COUNTRY_NAME e renomeámola como destination usando AS\n",
    "df_csv.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d4452-2ecd-4dc4-8e56-9234212863a1",
   "metadata": {},
   "source": [
    "#### `selectExpr`\n",
    "Como xa se comentou, `selectExpr` permite empregar expresións SQl facilitando  o renomeado dos campos entre outras cousas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18ee9db7-da3f-4722-92ff-6514fb365536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|      Destino|       Origen|\n",
      "+-------------+-------------+\n",
      "|United States|      Romania|\n",
      "|United States|      Croatia|\n",
      "|United States|      Ireland|\n",
      "|        Egypt|United States|\n",
      "|United States|        India|\n",
      "+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar selectExpr() para renomear columnas con alias\n",
    "\n",
    "# Seleccionamos dúas columnas e renomeámolas directamente usando expresións SQL\n",
    "df_csv.selectExpr(\n",
    "    \"DEST_COUNTRY_NAME AS Destino\",\n",
    "    \"ORIGIN_COUNTRY_NAME AS Origen\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d164ba28-83f8-46f3-afa8-3818d0211364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar selectExpr() para engadir unha columna calculada mantendo todas as columnas orixinais\n",
    "\n",
    "# Seleccionamos todas as columnas do DataFrame (con \"*\")\n",
    "# e engadimos unha columna booleana que indica se o destino e a orixe son iguais\n",
    "df_csv.selectExpr(\n",
    "    \"*\",\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) AS withinCountry\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948f67f-a1a6-4d80-b70c-2633b68dc1fd",
   "metadata": {},
   "source": [
    "### Engadir columnas: `withColumn()`\n",
    "\n",
    "Aínda que se poden crear columnas novas usando `select()` ou `selectExpr()`, para **engadir columnas mantendo todas as existentes** adoita ser máis sinxelo e expresivo empregar o método `withColumn()`.\n",
    "\n",
    "`withColumn()` crea un novo DataFrame engadindo (ou substituíndo) unha columna, a partir dunha expresión.  \n",
    "\n",
    "Características principais:\n",
    "- Se a columna indicada **non existe**, engádese como columna nova.\n",
    "- Se a columna indicada **xa existe**, substitúese polo novo cálculo.\n",
    "- O DataFrame orixinal non se modifica; devólvese un novo DataFrame.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.withColumn(\"nome_columna\", expresión)`\n",
    "\n",
    "A expresión pode construírse con funcións como `col()`, `expr()`, operacións aritméticas, condicións, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5828a9b4-c48b-4114-a0d1-d1d761b2f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|count_x2|\n",
      "+-----------------+-------------------+-----+--------+\n",
      "|    United States|            Romania|   15|      30|\n",
      "|    United States|            Croatia|    1|       2|\n",
      "|    United States|            Ireland|  344|     688|\n",
      "|            Egypt|      United States|   15|      30|\n",
      "|    United States|              India|   62|     124|\n",
      "+-----------------+-------------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Engadir unha columna nova usando withColumn()\n",
    "\n",
    "# Importamos as funcións necesarias\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Engadimos unha columna nova chamada count_x2, que é o dobre da columna count\n",
    "df_novo = df_csv.withColumn(\"count_x2\", col(\"count\") * 2)\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame resultante\n",
    "df_novo.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29168119-5372-4d9c-a418-8e35d99124b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Engadir unha columna booleana indicando se a orixe e o destino son iguais\n",
    "\n",
    "# Importamos expr para poder escribir a condición en formato SQL\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Engadimos a columna withinCountry indicando se DEST_COUNTRY_NAME coincide con ORIGIN_COUNTRY_NAME\n",
    "df_within = df_csv.withColumn(\"withinCountry\", expr(\"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "# Amosamos as primeiras filas do resultado\n",
    "df_within.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba7673-7530-48b4-821f-b250d7b13b11",
   "metadata": {},
   "source": [
    "### Renomear columnas: `withColumnRenamed()`\n",
    "\n",
    "Do mesmo xeito que `withColumn()` facilita engadir ou substituír columnas, `withColumnRenamed()` permite **renomear columnas dun DataFrame** de forma directa, actuando como un *alias* aplicado a unha columna xa existente.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.withColumnRenamed(\"nome_antigo\", \"nome_novo\")`\n",
    "\n",
    "O DataFrame orixinal non se modifica; devólvese un novo DataFrame co cambio aplicado.\n",
    "\n",
    "---\n",
    "\n",
    "#### Utilidade de `withColumnRenamed()` na limpeza de datos\n",
    "\n",
    "Renomear columnas é unha das tarefas máis frecuentes durante a **limpeza e preparación de datos** (*data cleaning*), especialmente cando os datos veñen de fontes externas (CSV, Excel, APIs, sistemas legacy...) e presentan problemas como:\n",
    "\n",
    "- **Nomes pouco claros ou inconsistentes** (por exemplo `col1`, `campo_x`, `VAL`, etc.)\n",
    "- **Espazos en branco** (por exemplo `\"Invoice No\"`)\n",
    "- **Caracteres especiais** (por exemplo `\"Unit-Price (€)\"`)\n",
    "- **Maiúsculas/minúsculas inconsistentes** (`\"Country\"`, `\"COUNTRY\"`, `\"country\"`)\n",
    "- **Nomes moi longos ou con abreviaturas pouco intuitivas**\n",
    "- **Columnas con nomes que chocan con palabras reservadas** ou que dificultan escribir expresións\n",
    "\n",
    "Renomear columnas resolve estes problemas e facilita:\n",
    "- escribir código máis limpo e lexible (`df.select(\"unit_price\")` en vez de `df.select(\"Unit Price (€)\")`)\n",
    "- evitar erros en consultas, joins e transformacións\n",
    "- aplicar convencións consistentes (por exemplo `snake_case`)\n",
    "- manter coherencia entre datasets cando se fan unións ou joins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "742cbf5f-a238-490b-a292-f86f31197430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "|United States|            Ireland|  344|\n",
      "|        Egypt|      United States|   15|\n",
      "|United States|              India|   62|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Renomear unha columna de forma directa usando withColumnRenamed()\n",
    "\n",
    "# Renomeamos a columna DEST_COUNTRY_NAME a \"destination\"\n",
    "df_renamed = df_csv.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"destination\")\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame resultante\n",
    "df_renamed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "364dcc4c-b15a-47ad-a154-563c30226059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "|      destino|        orixe|num_voos|\n",
      "+-------------+-------------+--------+\n",
      "|United States|      Romania|      15|\n",
      "|United States|      Croatia|       1|\n",
      "|United States|      Ireland|     344|\n",
      "|        Egypt|United States|      15|\n",
      "|United States|        India|      62|\n",
      "+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Renomear varias columnas encadeando chamadas a withColumnRenamed()\n",
    "\n",
    "# Renomeamos varias columnas para facer o DataFrame máis limpo e consistente\n",
    "df_clean = (\n",
    "    df_csv\n",
    "        .withColumnRenamed(\"DEST_COUNTRY_NAME\", \"destino\")\n",
    "        .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"orixe\")\n",
    "        .withColumnRenamed(\"count\", \"num_voos\")\n",
    ")\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame resultante\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d964ed-788f-485a-81da-ace4214fe93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------+\n",
      "|invoice_no|customer_name|unit_price_eur|\n",
      "+----------+-------------+--------------+\n",
      "|         1|          Ana|            22|\n",
      "+----------+-------------+--------------+\n",
      "\n",
      "root\n",
      " |-- invoice_no: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- unit_price_eur: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Un caso típico de limpeza: eliminar espazos e estandarizar nomes (snake_case)\n",
    "\n",
    "# Supoñemos un DataFrame con columnas con espazos ou caracteres problemáticos\n",
    "df_exemplo = spark.createDataFrame(\n",
    "    [(1, \"Ana\", 22)],\n",
    "    [\"Invoice No\", \"Customer Name\", \"Unit Price (€)\"]\n",
    ")\n",
    "\n",
    "# Renomeamos columnas para evitar espazos e caracteres especiais\n",
    "df_exemplo_clean = (\n",
    "    df_exemplo\n",
    "        .withColumnRenamed(\"Invoice No\", \"invoice_no\")\n",
    "        .withColumnRenamed(\"Customer Name\", \"customer_name\")\n",
    "        .withColumnRenamed(\"Unit Price (€)\", \"unit_price_eur\")\n",
    ")\n",
    "\n",
    "# Amosamos o resultado final e o seu esquema para comprobar os cambios\n",
    "df_exemplo_clean.show()\n",
    "df_exemplo_clean.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea7fb4-8bee-438b-810e-be151228151d",
   "metadata": {},
   "source": [
    "### Eliminar columnas: `drop()`\n",
    "\n",
    "A función `drop()` permite eliminar unha ou varias columnas dun DataFrame. É unha operación habitual na limpeza e preparación de datos cando hai campos que non se van usar, que son redundantes, que conteñen demasiados nulos ou que poden causar problemas (por exemplo, columnas auxiliares xeradas en pasos previos).\n",
    "\n",
    "Características principais:\n",
    "- Elimina columnas polo seu nome.\n",
    "- Permite eliminar unha soa columna ou varias nunha mesma chamada.\n",
    "- O DataFrame orixinal non se modifica; devólvese un novo DataFrame sen esas columnas.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.drop(\"columna\")`\n",
    "- `df.drop(\"col1\", \"col2\", \"col3\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "103259d3-b335-4a24-890d-1363d0fee56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Eliminar unha columna dun DataFrame usando drop()\n",
    "\n",
    "# Eliminamos a columna count do DataFrame\n",
    "df_sen_count = df_csv.drop(\"count\")\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame resultante\n",
    "df_sen_count.show(5)\n",
    "\n",
    "# Amosamos o esquema final para comprobar que a columna desapareceu\n",
    "df_sen_count.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dc61686-9a2f-4378-9fb9-448f272c8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Eliminar varias columnas á vez usando drop()\n",
    "\n",
    "# Eliminamos dúas columnas do DataFrame nunha única chamada\n",
    "df_sen_cols = df_csv.drop(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\")\n",
    "\n",
    "# Amosamos as primeiras filas do DataFrame resultante\n",
    "df_sen_cols.show(5)\n",
    "\n",
    "# Amosamos o esquema final para comprobar que as columnas eliminadas xa non existen\n",
    "df_sen_cols.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f137720-8b7e-4431-b41a-db67c7981ba9",
   "metadata": {},
   "source": [
    "### Filtrar filas: `where()` / `filter()`\n",
    "\n",
    "As funcións `where()` e `filter()` permiten filtrar filas dun DataFrame segundo un criterio, de forma equivalente á cláusula **WHERE** en SQL. Ambas funcións son equivalentes (son sinónimos), polo que se pode empregar a que resulte máis cómoda.\n",
    "\n",
    "Características principais:\n",
    "- Permiten quedar só coas filas que cumpren unha condición.\n",
    "- A condición pode expresarse:\n",
    "  - como unha cadea SQL (por exemplo `\"count > 10\"`)\n",
    "  - como unha expresión construída con `col()`, operadores e funcións de Spark\n",
    "- É habitual encadear varios filtros para expresar condicións máis complexas.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.filter(condición)`\n",
    "- `df.where(condición)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd7bd2fc-3bb8-467e-8144-1852a9b51690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|    United States|       Sint Maarten|  325|\n",
      "|    United States|   Marshall Islands|   39|\n",
      "|           Guyana|      United States|   64|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Filtrar filas usando unha condición en formato SQL (cadea de texto)\n",
    "\n",
    "# Filtramos as filas onde o número de voos (count) é maior que 10\n",
    "df_filtrado = df_csv.filter(\"count > 10\")\n",
    "\n",
    "# Amosamos as primeiras filas do resultado filtrado\n",
    "df_filtrado.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d5dd435-95b6-4f4e-ac82-99987199fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|    United States|       Sint Maarten|  325|\n",
      "|    United States|   Marshall Islands|   39|\n",
      "|           Guyana|      United States|   64|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Filtrar filas usando expresións con col()\n",
    "\n",
    "# Importamos col para construir condicións con columnas\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filtramos as filas onde count é maior que 10\n",
    "df_filtrado = df_csv.filter(col(\"count\") > 10)\n",
    "\n",
    "# Amosamos as primeiras filas do resultado filtrado\n",
    "df_filtrado.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aec0b9a4-e8e4-4268-bbb5-d35f5a98a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|    United States|       Sint Maarten|  325|\n",
      "|    United States|   Marshall Islands|   39|\n",
      "|           Guyana|      United States|   64|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Filtrar usando varias condicións combinadas\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filtramos as filas onde:\n",
    "# - count é maior que 10\n",
    "# - e o país de destino é distinto do país de orixe\n",
    "df_filtrado_complexo = df_csv.filter(\n",
    "    (col(\"count\") > 10) & (col(\"DEST_COUNTRY_NAME\") != col(\"ORIGIN_COUNTRY_NAME\"))\n",
    ")\n",
    "\n",
    "# Amosamos as primeiras filas do resultado\n",
    "df_filtrado_complexo.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7890804c-42da-4a6e-9197-0e0f51a2eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar where() (equivalente a filter()) con sintaxe SQL\n",
    "\n",
    "# Filtramos as filas onde o destino é \"United States\"\n",
    "df_usa = df_csv.where(\"DEST_COUNTRY_NAME = 'United States' and ORIGIN_COUNTRY_NAME='Romania'\")\n",
    "\n",
    "# Amosamos as primeiras filas do resultado\n",
    "df_usa.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a03f7-be1d-4f5d-9d30-c26f58b748b2",
   "metadata": {},
   "source": [
    "### Obter filas únicas: `distinct()`\n",
    "\n",
    "A función `distinct()` permite eliminar filas duplicadas dun DataFrame e devolver só as filas únicas. É equivalente a aplicar un `SELECT DISTINCT` en SQL.\n",
    "\n",
    "Características principais:\n",
    "- Elimina duplicados considerando **todas as columnas** do DataFrame.\n",
    "- É útil na limpeza de datos cando existen rexistros repetidos.\n",
    "- Require un proceso de *shuffle* para comparar filas entre particións, polo que pode ser custosa en datasets grandes.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.distinct()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ba2c1a4-2bd9-4c65-af75-eeef9403a4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame orixinal (con duplicados):\n",
      "+-----+-----+--------+\n",
      "| nome|idade|  cidade|\n",
      "+-----+-----+--------+\n",
      "|  Ana|   22| Ourense|\n",
      "|Brais|   25|    Lugo|\n",
      "|  Ana|   22| Ourense|\n",
      "|Carla|   21|A Coruña|\n",
      "|Brais|   25|    Lugo|\n",
      "+-----+-----+--------+\n",
      "\n",
      "DataFrame sen duplicados (distinct):\n",
      "+-----+-----+--------+\n",
      "| nome|idade|  cidade|\n",
      "+-----+-----+--------+\n",
      "|  Ana|   22| Ourense|\n",
      "|Brais|   25|    Lugo|\n",
      "|Carla|   21|A Coruña|\n",
      "+-----+-----+--------+\n",
      "\n",
      "Número de filas orixinais:  5\n",
      "Número de filas únicas:  3\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame con filas duplicadas e eliminalas usando distinct()\n",
    "\n",
    "# Creamos un DataFrame manualmente cunhas cantas filas repetidas\n",
    "df_repes = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Ana\", 22, \"Ourense\"),\n",
    "        (\"Brais\", 25, \"Lugo\"),\n",
    "        (\"Ana\", 22, \"Ourense\"),   # fila repetida\n",
    "        (\"Carla\", 21, \"A Coruña\"),\n",
    "        (\"Brais\", 25, \"Lugo\")     # fila repetida\n",
    "    ],\n",
    "    [\"nome\", \"idade\", \"cidade\"]\n",
    ")\n",
    "\n",
    "# Amosamos o DataFrame orixinal para ver as filas duplicadas\n",
    "print(\"DataFrame orixinal (con duplicados):\")\n",
    "df_repes.show()\n",
    "\n",
    "# Aplicamos distinct() para eliminar as filas duplicadas (considerando todas as columnas)\n",
    "df_unico = df_repes.distinct()\n",
    "\n",
    "# Amosamos o DataFrame resultante para comprobar que xa non hai repetidos\n",
    "print(\"DataFrame sen duplicados (distinct):\")\n",
    "df_unico.show()\n",
    "\n",
    "# Calculamos cantas filas había antes e cantas hai despois para verificar o efecto\n",
    "num_orixinais = df_repes.count()\n",
    "num_unicas = df_unico.count()\n",
    "\n",
    "# Amosamos o resultado final: comparación de filas orixinais vs. únicas\n",
    "print(\"Número de filas orixinais: \", num_orixinais)\n",
    "print(\"Número de filas únicas: \", num_unicas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf287a1-28fe-4966-871a-20b617260db5",
   "metadata": {},
   "source": [
    "### Ordenar filas: `sort()` / `orderBy()`\n",
    "\n",
    "As funcións `sort()` e `orderBy()` permiten ordenar as filas dun DataFrame segundo unha ou varias columnas. Ambas son equivalentes (sinónimos), polo que se pode empregar a que resulte máis cómoda.\n",
    "\n",
    "Por defecto, a ordenación é **ascendente**, pero pódese especificar a orde usando:\n",
    "- `asc()` para orde ascendente\n",
    "- `desc()` para orde descendente\n",
    "\n",
    "Tamén se poden ordenar varias columnas á vez, indicando unha lista de columnas ou expresións.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.sort(\"columna\")`\n",
    "- `df.sort(col(\"columna\").desc())`\n",
    "- `df.orderBy(\"col1\", \"col2\")`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70ad527a-25f7-4712-b510-c56e7f6e9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_csv ordenado por count (ascendente):\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|         Suriname|      United States|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|           Zambia|      United States|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Ordenar o DataFrame df_csv por unha columna en orde ascendente (por defecto)\n",
    "\n",
    "# Ordenamos df_csv pola columna count en orde ascendente\n",
    "df_ordenado_count = df_csv.sort(\"count\")\n",
    "\n",
    "# Amosamos o resultado final: ordenación ascendente por count\n",
    "print(\"DataFrame df_csv ordenado por count (ascendente):\")\n",
    "df_ordenado_count.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad2b6b23-5ba4-4ddd-8c6d-832782108fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_csv ordenado por count (descendente):\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "|   United Kingdom|      United States|  2025|\n",
      "|    United States|     United Kingdom|  1970|\n",
      "|            Japan|      United States|  1548|\n",
      "|    United States|              Japan|  1496|\n",
      "|          Germany|      United States|  1468|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Ordenar o DataFrame df_csv por unha columna en orde descendente usando desc()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ordenamos df_csv pola columna count en orde descendente\n",
    "df_ordenado_count_desc = df_csv.sort(col(\"count\").desc())\n",
    "\n",
    "# Amosamos o resultado final: ordenación descendente por count\n",
    "print(\"DataFrame df_csv ordenado por count (descendente):\")\n",
    "df_ordenado_count_desc.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da32c72-3fe1-4ab5-b440-5aa52a92a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_csv ordenado por count (descendente):\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "|   United Kingdom|      United States|  2025|\n",
      "|    United States|     United Kingdom|  1970|\n",
      "|            Japan|      United States|  1548|\n",
      "|    United States|              Japan|  1496|\n",
      "|          Germany|      United States|  1468|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Ordenar o DataFrame df_csv por unha columna en orde descendente usando desc()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ordenamos df_csv pola columna count en orde descendente\n",
    "df_ordenado_count_desc = df_csv.sort(col(\"count\").desc())\n",
    "\n",
    "# Amosamos o resultado final: ordenación descendente por count\n",
    "print(\"DataFrame df_csv ordenado por count (descendente):\")\n",
    "df_ordenado_count_desc.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f6edc9a-a438-43ad-934b-0b2280256f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_csv ordenado por destino (asc) e count (desc):\n",
      "+-------------------+-------------------+-----+\n",
      "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-------------------+-----+\n",
      "|            Algeria|      United States|    4|\n",
      "|             Angola|      United States|   15|\n",
      "|           Anguilla|      United States|   41|\n",
      "|Antigua and Barbuda|      United States|  126|\n",
      "|          Argentina|      United States|  180|\n",
      "|              Aruba|      United States|  346|\n",
      "|          Australia|      United States|  329|\n",
      "|            Austria|      United States|   62|\n",
      "|         Azerbaijan|      United States|   21|\n",
      "|            Bahrain|      United States|   19|\n",
      "+-------------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Ordenar o DataFrame df_csv por varias columnas (destino ascendente e count descendente)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ordenamos por destino en orde ascendente e por count en orde descendente\n",
    "df_ordenado_mult = df_csv.orderBy(col(\"DEST_COUNTRY_NAME\").asc(), col(\"count\").desc())\n",
    "\n",
    "# Amosamos o resultado final: ordenación por múltiples columnas\n",
    "print(\"DataFrame df_csv ordenado por destino (asc) e count (desc):\")\n",
    "df_ordenado_mult.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251727-0550-4cb6-b33d-4a02ffa8a5e2",
   "metadata": {},
   "source": [
    "### Limitar filas: `limit()`\n",
    "\n",
    "A función `limit()` permite limitar o número de filas devoltas por un DataFrame, devolvendo só as primeiras `n` filas.\n",
    "\n",
    "É equivalente a usar `LIMIT n` en SQL e resulta útil para:\n",
    "- inspeccionar rapidamente unha mostra de datos\n",
    "- facer probas sen procesar todo o dataset\n",
    "- reducir a cantidade de datos nunha fase inicial dun fluxo de traballo\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.limit(n)`\n",
    "\n",
    "O resultado é un novo DataFrame con, como máximo, `n` filas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "500a5f77-718a-43dc-8ecc-430315279853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 5 filas do DataFrame df_csv:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Limitar o número de filas dun DataFrame usando limit()\n",
    "\n",
    "# Limitamos df_csv ás primeiras 5 filas\n",
    "df_limitado = df_csv.limit(5)\n",
    "\n",
    "# Amosamos o resultado final: só 5 filas\n",
    "print(\"Primeiras 5 filas do DataFrame df_csv:\")\n",
    "df_limitado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ae89e-f20f-47c2-bdd8-42ad7eaa8af3",
   "metadata": {},
   "source": [
    "## Accións en DataFrames\n",
    "\n",
    "As **accións** (*actions*) en DataFrames son operacións que provocan a execución real do traballo en Spark. Mentres que as transformacións só constrúen un plan lóxico, as accións fan que Spark:\n",
    "\n",
    "1. xere e optimice o plan lóxico (Catalyst)\n",
    "2. xere un plan físico (estratexia de execución)\n",
    "3. lance un ou varios *jobs* no clúster\n",
    "4. devolva un resultado ao driver ou escriba a saída a disco\n",
    "\n",
    "En resumo: as accións son o punto no que Spark deixa de “planificar” e pasa a “executar”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05e15131-0c22-46f3-a86f-58a3110cf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un DataFrame de referencia (df_csv) para probar accións\n",
    "\n",
    "# Supoñemos que df_csv xa está cargado desde CSV\n",
    "# Amosamos o esquema para confirmar as columnas dispoñibles\n",
    "df_csv.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157ca33-232b-4bb5-8b6d-ce4804065bc1",
   "metadata": {},
   "source": [
    "### `show()`\n",
    "\n",
    "`show()` é unha acción que amosa por pantalla un número determinado de filas. Úsase para inspeccionar rapidamente os datos. Por defecto amosa 20 filas.\n",
    "\n",
    "Ao chamala, Spark executa o plan necesario para obter esas filas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1fb41db-09b3-48a9-bdac-49155584f8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Amosar as primeiras 5 filas do DataFrame usando show()\n",
    "\n",
    "df_csv.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8cb7b0-f34c-466f-ae3e-01ead8fb5c21",
   "metadata": {},
   "source": [
    "### `collect()`\n",
    "\n",
    "`collect()` devolve todas as filas do DataFrame ao driver como unha lista de obxectos `Row`.  \n",
    "É unha acción perigosa en datasets grandes, porque pode traer moitos datos á memoria do driver.\n",
    "\n",
    "Só se recomenda cando se sabe que o DataFrame é pequeno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b96e9c7-af0a-4542-812c-23428ce6a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas recollidas con collect():  [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Recoller un número pequeno de filas usando limit() + collect()\n",
    "\n",
    "# Limitamos a 5 filas para evitar traer todo o dataset ao driver\n",
    "filas = df_csv.limit(5).collect()\n",
    "\n",
    "# Amosamos o resultado final: lista de Rows\n",
    "print(\"Filas recollidas con collect(): \", filas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c12034-5b0b-4db5-b9eb-118019ad73ac",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "\n",
    "`count()` devolve o número total de filas do DataFrame.  \n",
    "É unha acción frecuente para obter estatísticas rápidas e validar transformacións.\n",
    "\n",
    "Ao executarse, Spark pode necesitar ler moitos datos, polo que en datasets grandes tamén pode ser unha operación custosa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e31b6e1-6d4a-47d8-8cd6-e877ce87fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de filas en df_csv:  256\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar o número de filas do DataFrame usando count()\n",
    "\n",
    "num_filas = df_csv.count()\n",
    "\n",
    "print(\"Número total de filas en df_csv: \", num_filas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c46b36-a923-49ec-9408-22f359f7ff90",
   "metadata": {},
   "source": [
    "### `take(n)`\n",
    "\n",
    "`take(n)` devolve as primeiras `n` filas ao driver como unha lista de `Row`.  \n",
    "É semellante a `collect()`, pero máis seguro porque limita o número de filas devoltas.\n",
    "\n",
    "É unha boa alternativa para obter unha pequena mostra de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3b71ffc-0a46-4167-83c8-7b8688dc9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 5 filas obtidas con take():  [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Obter as primeiras 5 filas usando take()\n",
    "\n",
    "filas = df_csv.take(5)\n",
    "\n",
    "print(\"Primeiras 5 filas obtidas con take(): \", filas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70bc02-3856-4437-9c3c-30fbb9408f4e",
   "metadata": {},
   "source": [
    "### `first()` / `head()`\n",
    "\n",
    "`first()` devolve a primeira fila do DataFrame.  \n",
    "`head()` é equivalente e tamén permite indicar un número de filas (como `take(n)`).\n",
    "\n",
    "Son accións útiles para inspeccionar rapidamente a estrutura dun rexistro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aecdc9aa-0204-433f-aeef-fcb178c807d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira fila do DataFrame:  Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Obter a primeira fila do DataFrame usando first()\n",
    "\n",
    "primeira_fila = df_csv.first()\n",
    "\n",
    "print(\"Primeira fila do DataFrame: \", primeira_fila)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e7edc-ae28-44a3-9461-1d610eab3007",
   "metadata": {},
   "source": [
    "### `foreach(func)`\n",
    "\n",
    "`foreach()` aplica unha función a cada fila do DataFrame, executándose nos executores.  \n",
    "Non devolve resultado ao driver, pero desencadea execución.\n",
    "\n",
    "Resulta útil para operacións con efectos secundarios (por exemplo escribir en logs ou sistemas externos), aínda que en xeral é máis común empregar métodos de escritura (`write`) en vez de `foreach()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b719638e-9192-4a8b-af28-cd811eb6fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Aplicar foreach() para imprimir un número limitado de filas (uso didáctico)\n",
    "\n",
    "# Limitamos a poucas filas para non imprimir demasiado\n",
    "df_csv.limit(3).foreach(lambda row: print(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282af63-3432-4214-b59d-dc3ce26b5d7c",
   "metadata": {},
   "source": [
    "> Non se amosa nada porque o foreach se executa nos executores e non no Driver\n",
    "\n",
    "> A función `write` vista previamente tamén é unha acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cee1d5-a219-415a-b694-f58c006eac67",
   "metadata": {},
   "source": [
    "## Traballando con particións en DataFrames\n",
    "\n",
    "Aínda que os **DataFrames** pertencen á API estruturada de Spark, é posible acceder á representación subxacente como **RDD** mediante o atributo `rdd`. Isto permite aplicar funcións propias da API de RDDs cando se quere inspeccionar ou controlar detalles de baixo nivel, como o número de particións, o contido de cada partición, etc.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.rdd`\n",
    "\n",
    "A partir dese momento, pódense usar operacións típicas de RDD, por exemplo:\n",
    "- `getNumPartitions()`\n",
    "- `glom()`\n",
    "- `mapPartitions()`\n",
    "- etc.\n",
    "\n",
    "---\n",
    "\n",
    "### `getNumPartitions()`\n",
    "\n",
    "`getNumPartitions()` permite obter o número de particións dun DataFrame a través do seu RDD subxacente.\n",
    "\n",
    "É útil para:\n",
    "- comprobar como se repartiron os datos ao ler un ficheiro\n",
    "- verificar cambios despois de `repartition()` ou `coalesce()`\n",
    "- analizar se existe suficiente paralelismo para procesar os datos\n",
    "\n",
    "A sintaxe é:\n",
    "`df.rdd.getNumPartitions()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "486b1a99-86f7-400e-ac09-443603768947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de particións de df_csv:  1\n"
     ]
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()\n",
    "# Exemplo: Obter o número de particións dun DataFrame usando df.rdd.getNumPartitions()\n",
    "\n",
    "# Supoñemos que df_csv é un DataFrame xa cargado\n",
    "num_particions = df_csv.rdd.getNumPartitions()\n",
    "\n",
    "# Amosamos o resultado final: número de particións do DataFrame\n",
    "print(\"Número de particións de df_csv: \", num_particions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dda32-ec51-477d-b123-4728ca783346",
   "metadata": {},
   "source": [
    "### `repartition()`\n",
    "\n",
    "A transformación `repartition()` permite cambiar o número de particións dun **DataFrame**. Para facelo, Spark realiza un proceso de **shuffle** (*baraxe*), redistribuíndo os datos entre os executores para equilibrar as particións.\n",
    "\n",
    "Isto implica que `repartition()` é unha operación máis custosa que `coalesce()`, pero resulta útil cando se quere:\n",
    "- aumentar o paralelismo (máis particións)\n",
    "- equilibrar particións descompensadas\n",
    "- preparar datos para operacións posteriores custosas (agregacións, joins, escritura...)\n",
    "\n",
    "`repartition()` permite varias opcións. A primeira (a máis sinxela) consiste en indicar só o número de particións desexado.\n",
    "\n",
    "A sintaxe xeral é:\n",
    "`df.repartition(num_particions)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae24358-9fc2-4776-b9e9-3572d33175e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Cambiar o número de particións dun DataFrame indicando só o número desexado\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame antes do cambio\n",
    "num_inicial = df_csv.rdd.getNumPartitions()\n",
    "print(\"Número inicial de particións en df_csv: \", num_inicial)\n",
    "\n",
    "# Reparticionamos o DataFrame a 8 particións (isto provoca shuffle)\n",
    "df_rep = df_csv.repartition(8)\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame despois do cambio\n",
    "num_final = df_rep.rdd.getNumPartitions()\n",
    "print(\"Número de particións tras repartition(8): \", num_final)\n",
    "\n",
    "# Amosamos algunhas filas para forzar execución e comprobar que o DataFrame é válido\n",
    "print(\"Mostra de filas do DataFrame reparticionado:\")\n",
    "df_rep.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48caf381-e21c-4d64-ad24-4cd625f5824d",
   "metadata": {},
   "source": [
    "Ademais de indicar só o número de particións, `repartition()` permite especificar unha ou varias **columnas** para reparticionar os datos. Neste caso, Spark fai un shuffle e redistribúe as filas de maneira que:\n",
    "\n",
    "- as filas coa mesma clave (valor na columna indicada) tenden a ir á mesma partición\n",
    "- isto pode mellorar o rendemento de operacións posteriores como:\n",
    "  - `groupBy()` sobre esa mesma columna\n",
    "  - `join()` cando ambos DataFrames se particionan pola mesma clave\n",
    "  - escrituras particionadas ou organización por clave\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.repartition(\"columna\")`\n",
    "- `df.repartition(num_particions, \"columna\")`\n",
    "- `df.repartition(\"col1\", \"col2\")`\n",
    "\n",
    "Neste apartado móstrase o caso máis sinxelo: indicar só a columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365198f-1422-4cb1-a6c6-d487227449ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Reparticionar un DataFrame usando unha columna como clave de particionamento\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame antes do cambio\n",
    "num_inicial = df_csv.rdd.getNumPartitions()\n",
    "print(\"Número inicial de particións en df_csv: \", num_inicial)\n",
    "\n",
    "# Reparticionamos o DataFrame usando a columna DEST_COUNTRY_NAME como clave\n",
    "# Isto provoca shuffle e reorganiza os datos para agrupalos por destino\n",
    "df_rep_col = df_csv.repartition(\"DEST_COUNTRY_NAME\")\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame despois do cambio\n",
    "num_final = df_rep_col.rdd.getNumPartitions()\n",
    "print(\"Número de particións tras repartition('DEST_COUNTRY_NAME'): \", num_final)\n",
    "\n",
    "# Amosamos algunhas filas para forzar execución e comprobar que o DataFrame é válido\n",
    "print(\"Mostra de filas do DataFrame reparticionado por DEST_COUNTRY_NAME:\")\n",
    "df_rep_col.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e5284-7e53-4c6f-84a7-63323ffc8ad8",
   "metadata": {},
   "source": [
    "Tamén é posible indicar **ambas cousas ao mesmo tempo**: o número de particións desexado e a columna (ou columnas) pola que se quere reparticionar.\n",
    "\n",
    "Neste caso Spark:\n",
    "- crea exactamente o número de particións indicado\n",
    "- distribúe as filas aplicando unha función de particionamento baseada na columna (hash partitioning)\n",
    "- realiza un shuffle para mover os datos e conseguir ese reparto\n",
    "\n",
    "Esta opción úsase cando se quere controlar o paralelismo e, ao mesmo tempo, preparar o DataFrame para operacións posteriores por clave (por exemplo, `groupBy()` ou `join()`).\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.repartition(num_particions, \"columna\")`\n",
    "- `df.repartition(num_particions, \"col1\", \"col2\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25f5d7e5-d8ea-4691-b755-559e222be532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número inicial de particións en df_csv:  1\n",
      "Número de particións tras repartition(8, 'DEST_COUNTRY_NAME'):  8\n",
      "Mostra de filas do DataFrame reparticionado por DEST_COUNTRY_NAME en 8 particións:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Senegal|      United States|   40|\n",
      "|          Algeria|      United States|    4|\n",
      "|          Iceland|      United States|  181|\n",
      "|         Suriname|      United States|    1|\n",
      "|          Ecuador|      United States|  268|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Reparticionar un DataFrame indicando número de particións e columna de particionamento\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame antes do cambio\n",
    "num_inicial = df_csv.rdd.getNumPartitions()\n",
    "print(\"Número inicial de particións en df_csv: \", num_inicial)\n",
    "\n",
    "# Reparticionamos o DataFrame a 8 particións usando DEST_COUNTRY_NAME como clave\n",
    "# Isto provoca shuffle e organiza os datos por destino en 8 particións\n",
    "df_rep_num_col = df_csv.repartition(8, \"DEST_COUNTRY_NAME\")\n",
    "\n",
    "# Consultamos cantas particións ten o DataFrame despois do cambio\n",
    "num_final = df_rep_num_col.rdd.getNumPartitions()\n",
    "print(\"Número de particións tras repartition(8, 'DEST_COUNTRY_NAME'): \", num_final)\n",
    "\n",
    "# Amosamos algunhas filas para forzar execución e comprobar que o DataFrame é válido\n",
    "print(\"Mostra de filas do DataFrame reparticionado por DEST_COUNTRY_NAME en 8 particións:\")\n",
    "df_rep_num_col.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9f893-84ba-4039-97ce-2f00c98308e2",
   "metadata": {},
   "source": [
    "## Agregacións\n",
    "\n",
    "Spark permite empregar diversas funcións de **agregación** con DataFrames para obter métricas resumidas (contaxes, sumas, medias, mínimos, máximos, etc.). Estas operacións son habituais na análise de datos e tamén na preparación de datasets (por exemplo, para detectar valores nulos, distribucións ou outliers).\n",
    "\n",
    "Para os exemplos desta sección empregarase o seguinte DataFrame, cargado a partir de varios CSV e co esquema inferido automaticamente. Ademais, almacénase en caché para reutilizalo en varias operacións e créase unha vista temporal para poder consultalo con SQL se se precisa.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b257295-194b-4347-9076-2defa2c6c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Número de filas do DataFrame (para materializar a caché):  541909\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Cargar un DataFrame para traballar con agregacións\n",
    "\n",
    "# Cargamos todos os CSV do directorio e inferimos o esquema automaticamente\n",
    "df_ag = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"hdfs:///data/retail-data/all/*.csv\")\n",
    "        .coalesce(5)\n",
    ")\n",
    "\n",
    "# Gardamos o DataFrame en caché para acelerar accións repetidas sobre o mesmo contido\n",
    "df_ag.cache()\n",
    "\n",
    "# Rexistramos unha vista temporal para poder executar consultas SQL sobre este DataFrame\n",
    "df_ag.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "# Amosamos o esquema para ver nomes de columnas e tipos\n",
    "df_ag.printSchema()\n",
    "\n",
    "# Amosamos unha mostra de filas para comprobar que os datos se cargaron correctamente\n",
    "df_ag.show(5)\n",
    "\n",
    "# Forzamos a materialización da caché cunha acción sinxela\n",
    "print(\"Número de filas do DataFrame (para materializar a caché): \", df_ag.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff1584-8f39-4514-afab-2dd3b5b8f16b",
   "metadata": {},
   "source": [
    "A continuación, algunhas das máis importantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e178d-3c39-4d88-b9c6-f506108c2185",
   "metadata": {},
   "source": [
    "### `count`\n",
    "\n",
    "A agregación `count` permite contar rexistros. Pódese empregar de varias formas, dependendo do que se queira contar exactamente:\n",
    "\n",
    "- Contar valores non nulos dunha columna concreta.\n",
    "- Contar todas as filas usando `count(\"*\")`.\n",
    "- Contar todas as filas contando un literal (por exemplo, `count(lit(1))`), equivalente a contar filas.\n",
    "\n",
    "Estas opcións son moi útiles para:\n",
    "- saber cantos rexistros hai\n",
    "- comprobar cantos valores válidos hai nunha columna (non nulos)\n",
    "- comparar contaxes entre columnas para detectar nulos\n",
    "\n",
    "A continuación móstranse exemplos das tres formas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a363aa0-b2ef-42eb-8dc7-acdfbb8a2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaxe de valores non nulos en CustomerID: \n",
      "+----------------+\n",
      "|count_customerid|\n",
      "+----------------+\n",
      "|          406829|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar valores non nulos dunha columna concreta usando count(columna)\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Contamos cantos valores non nulos hai na columna CustomerID\n",
    "resultado = df_ag.select(count(\"CustomerID\").alias(\"count_customerid\"))\n",
    "\n",
    "# Amosamos o resultado final\n",
    "print(\"Contaxe de valores non nulos en CustomerID: \")\n",
    "resultado.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5126474c-94b3-404d-af03-c9578f1777a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaxe total de filas (count('*')): \n",
      "+-----------------+\n",
      "|count_total_filas|\n",
      "+-----------------+\n",
      "|           541909|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar todas as filas usando count(\"*\")\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Contamos todas as filas do DataFrame\n",
    "resultado = df_ag.select(count(\"*\").alias(\"count_total_filas\"))\n",
    "\n",
    "# Amosamos o resultado final\n",
    "print(\"Contaxe total de filas (count('*')): \")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a804cca7-96fc-459a-9146-cadc3f8ea096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaxe total de filas (count(lit(1))): \n",
      "+---------------------+\n",
      "|count_total_filas_lit|\n",
      "+---------------------+\n",
      "|               541909|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar todas as filas contando un literal (1)\n",
    "\n",
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "# Contamos todas as filas contando un literal por fila\n",
    "resultado = df_ag.select(count(lit(1)).alias(\"count_total_filas_lit\"))\n",
    "\n",
    "# Amosamos o resultado final\n",
    "print(\"Contaxe total de filas (count(lit(1))): \")\n",
    "\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7753ca-1755-4ce9-b559-5fc29671cbf0",
   "metadata": {},
   "source": [
    "### `countDistinct()`\n",
    "\n",
    "`countDistinct()` permite contar o número de valores **distintos** que aparecen nunha columna. É equivalente a facer un `COUNT(DISTINCT columna)` en SQL.\n",
    "\n",
    "É útil para:\n",
    "- saber cantos valores diferentes existen nunha columna (por exemplo, cantos países, cantos clientes, cantas facturas...)\n",
    "- detectar cardinalidade (se unha columna ten poucos valores repetidos ou moitísimos valores distintos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f4998b-74e9-495a-8565-e10f89a6cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de países distintos (countDistinct('Country')):\n",
      "+----------------+\n",
      "|paises_distintos|\n",
      "+----------------+\n",
      "|              38|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar cantos valores distintos hai na columna Country usando countDistinct()\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Calculamos o número de países distintos no DataFrame\n",
    "resultado = df_ag.select(countDistinct(\"Country\").alias(\"paises_distintos\"))\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Número de países distintos (countDistinct('Country')):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b91befd-44ed-467d-a0b1-c7418c8d3fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de clientes distintos (countDistinct('CustomerID')):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|clientes_distintos|\n",
      "+------------------+\n",
      "|              4372|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Contar cantos clientes distintos hai (CustomerID) usando countDistinct()\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Calculamos o número de clientes distintos (CustomerID)\n",
    "# Se hai valores nulos, non contan como un valor distinto\n",
    "resultado = df_ag.select(countDistinct(\"CustomerID\").alias(\"clientes_distintos\"))\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Número de clientes distintos (countDistinct('CustomerID')):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05c09-ceb4-458a-b1d3-7675a5dd10f7",
   "metadata": {},
   "source": [
    "### `approx_count_distinct()`\n",
    "\n",
    "`approx_count_distinct()` permite estimar (aproximar) o número de valores distintos dunha columna. É especialmente útil cando se traballa con volumes grandes de datos, porque:\n",
    "\n",
    "- é máis rápido e require menos recursos ca `countDistinct()`\n",
    "- devolve un resultado aproximado cun erro controlable\n",
    "\n",
    "Esta función baséase nun algoritmo probabilístico de cardinalidade (aproximación), polo que o valor obtido pode diferir lixeiramente do real, pero adoita ser suficiente para análises exploratorias e métricas rápidas.\n",
    "\n",
    "Opcionalmente pódese indicar `rsd` (*relative standard deviation*), que controla o erro relativo máximo aproximado:\n",
    "- valores máis pequenos → máis precisión, pero máis custo\n",
    "- valores máis grandes → menos custo, pero menos precisión\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `approx_count_distinct(\"columna\")`\n",
    "- `approx_count_distinct(\"columna\", rsd=0.05)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec96f67-3c14-436e-9cc2-0b300c6e1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número aproximado de países distintos (approx_count_distinct('Country')):\n",
      "+----------------------+\n",
      "|paises_distintos_aprox|\n",
      "+----------------------+\n",
      "|                    37|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Estimar o número de valores distintos na columna Country usando approx_count_distinct()\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "# Estimamos o número de países distintos (erro aproximado por defecto)\n",
    "resultado = df_ag.select(approx_count_distinct(\"Country\").alias(\"paises_distintos_aprox\"))\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Número aproximado de países distintos (approx_count_distinct('Country')):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26545ea3-4ad1-4027-9423-1f9afcede118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de approx_count_distinct('CustomerID') con distinta precisión:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------+\n",
      "|clientes_aprox_defecto|clientes_aprox_rsd_0_01|\n",
      "+----------------------+-----------------------+\n",
      "|                  4473|                   4369|\n",
      "+----------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Comparar aproximación por defecto vs. aproximación máis precisa (rsd máis pequeno)\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "# Calculamos dúas estimacións: unha por defecto e outra con maior precisión\n",
    "resultado = df_ag.select(\n",
    "    approx_count_distinct(\"CustomerID\").alias(\"clientes_aprox_defecto\"),\n",
    "    approx_count_distinct(\"CustomerID\", rsd=0.01).alias(\"clientes_aprox_rsd_0_01\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Comparación de approx_count_distinct('CustomerID') con distinta precisión:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85951668-7fe7-4fe2-b43b-0749aae9dd63",
   "metadata": {},
   "source": [
    "### `first()` / `last()`\n",
    "\n",
    "As funcións de agregación `first()` e `last()` permiten obter o **primeiro** ou o **último** valor dunha columna dentro dun conxunto de filas.\n",
    "\n",
    "Son útiles cando se quere:\n",
    "- obter un valor representativo dun grupo (por exemplo, tras un `groupBy()`)\n",
    "- conservar un valor asociado a unha clave cando non é relevante agregalo (con coidado)\n",
    "- facer probas rápidas de agregación\n",
    "\n",
    "Consideración importante:\n",
    "- Sen unha ordenación explícita, o concepto de “primeiro” e “último” pode depender da distribución dos datos e do plan de execución. Para ter un resultado determinista, adoita ser necesario **ordenar previamente** ou empregar funcións de xanela (*window functions*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ac3d1-3568-4c50-b5de-ed8739810bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Obter o primeiro e o último valor dunha columna usando first() e last()\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "# Calculamos o primeiro e o último valor da columna Country no conxunto completo de datos\n",
    "resultado = df_ag.select(\n",
    "    first(\"Country\").alias(\"primeiro_pais\"),\n",
    "    last(\"Country\").alias(\"ultimo_pais\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Primeiro e último valor da columna Country (first/last):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c9ceff-f9f7-45a1-9d46-110f5f0cdd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiro e último Country por CustomerID (first/last):\n",
      "+----------+--------------+--------------+\n",
      "|CustomerID| primeiro_pais|   ultimo_pais|\n",
      "+----------+--------------+--------------+\n",
      "|      NULL|United Kingdom|United Kingdom|\n",
      "|     12346|United Kingdom|United Kingdom|\n",
      "|     12347|       Iceland|       Iceland|\n",
      "|     12348|       Finland|       Finland|\n",
      "|     12349|         Italy|         Italy|\n",
      "|     12350|        Norway|        Norway|\n",
      "|     12352|        Norway|        Norway|\n",
      "|     12353|       Bahrain|       Bahrain|\n",
      "|     12354|         Spain|         Spain|\n",
      "|     12355|       Bahrain|       Bahrain|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar first() e last() despois dun groupBy() (primeiro e último país por CustomerID)\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "# Agrupamos por CustomerID e obtemos un país \"primeiro\" e \"último\" asociado a cada cliente\n",
    "# Só se mostran algúns resultados para non xerar unha saída moi grande\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"CustomerID\")\n",
    "        .agg(\n",
    "            first(\"Country\").alias(\"primeiro_pais\"),\n",
    "            last(\"Country\").alias(\"ultimo_pais\")\n",
    "        )\n",
    "        .limit(10)\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Primeiro e último Country por CustomerID (first/last):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269b012-23a9-445b-afda-743886804dfe",
   "metadata": {},
   "source": [
    "### `min()` / `max()`\n",
    "\n",
    "As funcións de agregación `min()` e `max()` permiten obter o valor **mínimo** e **máximo** dunha columna.\n",
    "\n",
    "Son útiles cando se quere:\n",
    "- coñecer rangos de valores (por exemplo, cantidades, prezos, datas)\n",
    "- detectar posibles outliers (valores extremos)\n",
    "- validar que os datos están dentro de límites esperados\n",
    "\n",
    "Estas funcións pódense aplicar sobre columnas numéricas e tamén sobre columnas de tipo data/hora (por exemplo, para obter a data máis antiga e a máis recente).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edb3565-19a1-47b4-a2c6-66aee0ba3a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor mínimo e máximo da columna Quantity (min/max):\n",
      "+------------+------------+\n",
      "|quantity_min|quantity_max|\n",
      "+------------+------------+\n",
      "|      -80995|       80995|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Obter o valor mínimo e máximo da columna Quantity usando min() e max()\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Calculamos o mínimo e o máximo de Quantity\n",
    "resultado = df_ag.select(\n",
    "    min(\"Quantity\").alias(\"quantity_min\"),\n",
    "    max(\"Quantity\").alias(\"quantity_max\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Valor mínimo e máximo da columna Quantity (min/max):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f5d860-1230-4a4c-b492-6553c7da9f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data máis antiga e máis recente da columna InvoiceDate (min/max):\n",
      "+----------------+----------------+\n",
      "|invoice_date_min|invoice_date_max|\n",
      "+----------------+----------------+\n",
      "| 1/10/2011 10:04|   9/9/2011 9:52|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Obter a data máis antiga e a máis recente da columna InvoiceDate\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Calculamos o mínimo e o máximo de InvoiceDate (se a columna é TimestampType)\n",
    "resultado = df_ag.select(\n",
    "    min(\"InvoiceDate\").alias(\"invoice_date_min\"),\n",
    "    max(\"InvoiceDate\").alias(\"invoice_date_max\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Data máis antiga e máis recente da columna InvoiceDate (min/max):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd387bd-4911-477f-9394-4f0647ac8be0",
   "metadata": {},
   "source": [
    "### `sum()` / `sum_distinct()`\n",
    "\n",
    "A función `sum()` realiza a suma de todos os valores dunha columna (ignorando valores nulos). É unha agregación habitual para obter totais, importes acumulados, cantidades totais, etc.\n",
    "\n",
    "A función `sum_distinct()` (tamén chamada `sumDistinct`, pero está `deprecated`) calcula a suma dos valores **distintos** dunha columna, é dicir:\n",
    "1. elimina valores repetidos\n",
    "2. suma só unha vez cada valor único\n",
    "\n",
    "Isto pode ser útil cando existen duplicados e se quere evitar que un mesmo valor conte máis dunha vez. Cómpre ter coidado, porque `sum_distinct()` non elimina filas duplicadas completas; elimina duplicados só con respecto ao valor desa columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55209eef-0dd1-476c-b1f2-108eab5b2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma total da columna Quantity (sum):\n",
      "+-------------+\n",
      "|suma_quantity|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Sumar todos os valores da columna Quantity usando sum()\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Calculamos a suma total de Quantity\n",
    "resultado = df_ag.select(sum(\"Quantity\").alias(\"suma_quantity\"))\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Suma total da columna Quantity (sum):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9539d537-a8f1-48f8-aab1-83390a9cbf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación sum(Quantity) vs sum_distinct(Quantity):\n",
      "+-------------------+----------------------+\n",
      "|suma_total_quantity|suma_distinta_quantity|\n",
      "+-------------------+----------------------+\n",
      "|            5176450|                 29310|\n",
      "+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Comparar sum() e sumDistinct() na columna Quantity\n",
    "\n",
    "from pyspark.sql.functions import sum, sum_distinct\n",
    "\n",
    "# Calculamos a suma total e a suma de valores distintos\n",
    "resultado = df_ag.select(\n",
    "    sum(\"Quantity\").alias(\"suma_total_quantity\"),\n",
    "    sum_distinct(\"Quantity\").alias(\"suma_distinta_quantity\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Comparación sum(Quantity) vs sum_distinct(Quantity):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb756f-4b86-4b38-8622-8ee61f58cc99",
   "metadata": {},
   "source": [
    "### `avg()`\n",
    "\n",
    "A función de agregación `avg()` permite calcular a **media aritmética** dos valores dunha columna.  \n",
    "É equivalente a `AVG(columna)` en SQL e ignora automaticamente os valores nulos.\n",
    "\n",
    "Resulta útil cando se quere:\n",
    "- obter valores medios (prezo medio, cantidade media, importe medio…)\n",
    "- analizar tendencias xerais dos datos\n",
    "- comparar medias entre distintos grupos (en combinación con `groupBy()`)\n",
    "\n",
    "Internamente, `avg()` equivale a facer `sum(columna) / count(columna_non_nula)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1573811-11c9-430f-beda-1bdb17024f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media aritmética da columna UnitPrice (avg):\n",
      "+-----------------+\n",
      "| media_unit_price|\n",
      "+-----------------+\n",
      "|4.611113626082961|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular a media do prezo unitario (UnitPrice)\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculamos a media de UnitPrice\n",
    "resultado = df_ag.select(avg(\"UnitPrice\").alias(\"media_unit_price\"))\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Media aritmética da columna UnitPrice (avg):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b62dfe-7278-479e-99c3-51969cbf28cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media da columna Quantity por país (groupBy + avg):\n",
      "+-----------+------------------+\n",
      "|    Country|    media_quantity|\n",
      "+-----------+------------------+\n",
      "|     Sweden| 77.13636363636364|\n",
      "|  Singapore| 22.85589519650655|\n",
      "|    Germany|12.369457609268036|\n",
      "|        RSA| 6.068965517241379|\n",
      "|     France| 12.91106696272058|\n",
      "|     Greece|10.657534246575343|\n",
      "|    Belgium| 11.18994683421943|\n",
      "|    Finland|15.346762589928058|\n",
      "|      Malta| 7.433070866141732|\n",
      "|Unspecified|7.3991031390134525|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular a media por grupo usando groupBy() + avg()\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculamos a cantidade media por país\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"Country\")\n",
    "         .agg(avg(\"Quantity\").alias(\"media_quantity\"))\n",
    "         .limit(10)\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Media da columna Quantity por país (groupBy + avg):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ea6b2-b34d-419f-ba54-55adf43a5b23",
   "metadata": {},
   "source": [
    "### Outros parámetros estatísticos\n",
    "#### Varianza e desviación estándar\n",
    "\n",
    "Ademais das agregacións básicas (suma, media, mínimo, máximo…), Spark permite calcular **parámetros estatísticos** que describen a dispersión dos datos, como a **varianza** e a **desviación estándar**.\n",
    "\n",
    "Estes indicadores son útiles cando se quere:\n",
    "- medir canto se afastan os valores da media\n",
    "- detectar datos moi dispersos ou moi concentrados\n",
    "- identificar posibles outliers\n",
    "- comparar a variabilidade entre distintos grupos\n",
    "\n",
    "Spark ofrece varias funcións relacionadas:\n",
    "\n",
    "- `variance()` / `var_pop()` / `var_samp()`\n",
    "- `stddev()` / `stddev_pop()` / `stddev_samp()`\n",
    "\n",
    "Por defecto:\n",
    "- `variance()` equivale a varianza muestral (`var_samp`)\n",
    "- `stddev()` equivale á desviación estándar muestral (`stddev_samp`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc1fa0b6-3202-46a3-901d-a71e93aadedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varianza e desviación estándar da columna Quantity:\n",
      "+-----------------+-----------------------+\n",
      "|varianza_quantity|desviacion_std_quantity|\n",
      "+-----------------+-----------------------+\n",
      "|47559.39140929905|     218.08115785023486|\n",
      "+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular a varianza e a desviación estándar da columna Quantity\n",
    "\n",
    "from pyspark.sql.functions import variance, stddev\n",
    "\n",
    "# Calculamos a varianza e a desviación estándar de Quantity\n",
    "resultado = df_ag.select(\n",
    "    variance(\"Quantity\").alias(\"varianza_quantity\"),\n",
    "    stddev(\"Quantity\").alias(\"desviacion_std_quantity\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Varianza e desviación estándar da columna Quantity:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba038e73-f7ea-49cd-8a89-3b7cb3b4b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Diferenciar entre versión poboacional e muestral\n",
    "\n",
    "from pyspark.sql.functions import var_pop, var_samp, stddev_pop, stddev_samp\n",
    "\n",
    "# Calculamos distintas versións de varianza e desviación estándar\n",
    "resultado = df_ag.select(\n",
    "    var_pop(\"Quantity\").alias(\"varianza_poboacional\"),\n",
    "    var_samp(\"Quantity\").alias(\"varianza_muestral\"),\n",
    "    stddev_pop(\"Quantity\").alias(\"desviacion_std_poboacional\"),\n",
    "    stddev_samp(\"Quantity\").alias(\"desviacion_std_muestral\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Varianza e desviación estándar (poboacional vs muestral) da columna Quantity:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a920e-9779-465a-81d5-237995645302",
   "metadata": {},
   "source": [
    "En resumo:\n",
    "- a **varianza** mide a dispersión dos valores respecto da media\n",
    "- a **desviación estándar** é a raíz cadrada da varianza e interprétase na mesma unidade que os datos orixinais\n",
    "\n",
    "Ambas métricas son fundamentais para entender a distribución dos datos e complementar a información que achega a media.\n",
    "\n",
    "#### oblicuidade, curtose, covarianza e correlación\n",
    "\n",
    "Spark tamén permite calcular métricas estatísticas máis avanzadas:\n",
    "\n",
    "- **Oblicuidade (skewness)**: mide a asimetría da distribución.\n",
    "  - valor > 0: cola máis longa á dereita\n",
    "  - valor < 0: cola máis longa á esquerda\n",
    "  - valor ≈ 0: distribución aproximadamente simétrica\n",
    "\n",
    "- **Curtose (kurtosis)**: mide o “grao de apuntamento” e o peso das colas da distribución respecto dunha distribución normal.\n",
    "  - valores altos: colas máis pesadas e máis presenza de extremos\n",
    "  - valores baixos: colas máis lixeiras\n",
    "\n",
    "- **Covarianza (covar)**: mide como varían conxuntamente dúas variables.\n",
    "  - signo positivo: cando unha sobe, a outra tende a subir\n",
    "  - signo negativo: cando unha sobe, a outra tende a baixar\n",
    "  - a magnitude depende da escala, polo que non é tan doada de comparar directamente\n",
    "\n",
    "- **Correlación (corr)**: mide a relación lineal entre dúas variables de forma normalizada entre -1 e 1.\n",
    "  - 1: relación lineal positiva perfecta\n",
    "  - -1: relación lineal negativa perfecta\n",
    "  - 0: sen relación lineal (ou moi débil)\n",
    "\n",
    "Estas métricas son útiles cando se quere:\n",
    "- describir a forma dunha distribución (oblicuidade e curtose)\n",
    "- analizar relacións entre variables numéricas (covarianza e correlación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bb3281f-faf2-4eae-8399-93356df00984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oblicuidade e curtose da columna Quantity:\n",
      "+--------------------+------------------+\n",
      "|oblicuidade_quantity|  curtose_quantity|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular oblicuidade (skewness) e curtose (kurtosis) da columna Quantity\n",
    "\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "# Calculamos oblicuidade e curtose de Quantity\n",
    "resultado = df_ag.select(\n",
    "    skewness(\"Quantity\").alias(\"oblicuidade_quantity\"),\n",
    "    kurtosis(\"Quantity\").alias(\"curtose_quantity\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Oblicuidade e curtose da columna Quantity:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f1421-2686-4bc5-b107-e0d818d1b654",
   "metadata": {},
   "source": [
    "Para covarianza e correlación, Spark ofrece dúas formas habituais:\n",
    "- funcións de agregación: `covar_samp`, `covar_pop` e `corr`\n",
    "- métodos sobre `DataFrameStatFunctions` mediante `df.stat` (por exemplo `df.stat.corr(...)`)\n",
    "\n",
    "A continuación móstrase un exemplo empregando funcións de agregación, que encaixa ben coa sección de agregacións.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35992291-793f-4686-a19d-7b2ba7c87743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covarianza e correlación entre Quantity e UnitPrice:\n",
      "+-------------------+----------------------+--------------------+\n",
      "|covarianza_muestral|covarianza_poboacional|         correlacion|\n",
      "+-------------------+----------------------+--------------------+\n",
      "|-26.058761257936965|   -26.058713170968012|-0.00123492454487...|\n",
      "+-------------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular covarianza e correlación entre Quantity e UnitPrice\n",
    "\n",
    "from pyspark.sql.functions import covar_samp, covar_pop, corr\n",
    "\n",
    "# Calculamos covarianza (muestral e poboacional) e correlación (Pearson) entre dúas columnas numéricas\n",
    "resultado = df_ag.select(\n",
    "    covar_samp(\"Quantity\", \"UnitPrice\").alias(\"covarianza_muestral\"),\n",
    "    covar_pop(\"Quantity\", \"UnitPrice\").alias(\"covarianza_poboacional\"),\n",
    "    corr(\"Quantity\", \"UnitPrice\").alias(\"correlacion\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Covarianza e correlación entre Quantity e UnitPrice:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca2f4e-ff70-4722-b8eb-974a860b220b",
   "metadata": {},
   "source": [
    "### `groupBy()`\n",
    "\n",
    "Para realizar agrupamentos en Spark con DataFrames adoitan empregarse dúas fases:\n",
    "\n",
    "- **Agrupar**: emprégase `groupBy()` indicando a columna (ou columnas) pola que se quere agrupar.\n",
    "- **Agregar**: indícase que cálculo se quere facer en cada grupo, empregando funcións de agregación como `count`, `sum`, `avg`, `min`, `max`, etc., normalmente a través de `agg()`.\n",
    "\n",
    "A idea é equivalente ao patrón SQL:\n",
    "`GROUP BY ...` + funcións agregadas (`COUNT`, `SUM`, `AVG`, ...)\n",
    "\n",
    "A sintaxe xeral é:\n",
    "- `df.groupBy(\"col\").count()`\n",
    "- `df.groupBy(\"col\").agg(<agregacións>)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ded3e6-f7b4-4ca0-bb31-3aa7feaa0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de rexistros por país (groupBy('Country').count()):\n",
      "+-----------+-----+\n",
      "|    Country|count|\n",
      "+-----------+-----+\n",
      "|     Sweden|  462|\n",
      "|  Singapore|  229|\n",
      "|    Germany| 9495|\n",
      "|        RSA|   58|\n",
      "|     France| 8557|\n",
      "|     Greece|  146|\n",
      "|    Belgium| 2069|\n",
      "|    Finland|  695|\n",
      "|      Malta|  127|\n",
      "|Unspecified|  446|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Agrupar por Country e contar cantas filas hai por país usando groupBy() + count()\n",
    "\n",
    "# Agrupamos pola columna Country e contamos filas en cada grupo\n",
    "resultado = df_ag.groupBy(\"Country\").count()\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Número de rexistros por país (groupBy('Country').count()):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c8a3b35-a2a4-4555-8392-a8f8547e350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas por país (count, sum, avg) usando groupBy + agg:\n",
      "+-----------+-------------+-------------+------------------+\n",
      "|    Country|num_rexistros|suma_quantity|  media_unit_price|\n",
      "+-----------+-------------+-------------+------------------+\n",
      "|     Sweden|          462|        35637| 3.910887445887447|\n",
      "|  Singapore|          229|         5234|109.64580786026204|\n",
      "|    Germany|         9495|       117448| 3.966929963138558|\n",
      "|        RSA|           58|          352| 4.277586206896552|\n",
      "|     France|         8557|       110480| 5.028864087881328|\n",
      "|     Greece|          146|         1556| 4.885547945205478|\n",
      "|    Belgium|         2069|        23152| 3.644335427742861|\n",
      "|    Finland|          695|        10666|  5.44870503597123|\n",
      "|      Malta|          127|          944| 5.244173228346455|\n",
      "|Unspecified|          446|         3300| 2.699573991031391|\n",
      "+-----------+-------------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Agrupar por Country e calcular varias agregacións usando agg()\n",
    "\n",
    "from pyspark.sql.functions import count, sum, avg\n",
    "\n",
    "# Agrupamos pola columna Country e calculamos varias métricas\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"Country\")\n",
    "         .agg(\n",
    "             count(\"*\").alias(\"num_rexistros\"),\n",
    "             sum(\"Quantity\").alias(\"suma_quantity\"),\n",
    "             avg(\"UnitPrice\").alias(\"media_unit_price\")\n",
    "         )\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Métricas por país (count, sum, avg) usando groupBy + agg:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66c420-de3b-4bb7-bacd-4f3cc0e5ea45",
   "metadata": {},
   "source": [
    "### Agrupamento con expresións (expr)\n",
    "\n",
    "Nalgúns casos pode resultar útil expresar as agregacións como **expresións SQL** usando `expr()`. Isto permite escribir operacións nun formato moi compacto, especialmente cando xa se coñece a sintaxe SQL.\n",
    "\n",
    "Esta técnica é recomendable cando:\n",
    "- se quere escribir agregacións rapidamente con sintaxe SQL\n",
    "- se está a traducir unha consulta SQL a DataFrame API\n",
    "- se prefire unha forma compacta de especificar as funcións de agregación\n",
    "\n",
    "Como alternativa máis habitual (e máis fácil de manter) pódense empregar funcións Python (`avg`, `stddev_pop`, etc.) dentro de `agg()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d13e8fc1-329b-47f0-a166-7404c4412f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media e desviación estándar poboacional de Quantity por InvoiceNo (groupBy + expr):\n",
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   563020|16.041666666666668|  10.212244148841895|\n",
      "|   565747|              10.3|   3.163858403911275|\n",
      "|   566248|               9.0|   8.703447592764606|\n",
      "|   566431| 14.11111111111111|   6.911254017815104|\n",
      "|   567163|              14.5|  11.280514172678478|\n",
      "|   567695|             -42.0|                 0.0|\n",
      "|   567879|10.206896551724139|  6.4774372877616235|\n",
      "|   568222|10.636363636363637|   2.705977466570403|\n",
      "|   568711|             15.25|   9.575359001102779|\n",
      "|   569020|11.541666666666666|   8.467974800518848|\n",
      "|   569560|            10.625|  3.7893765978060294|\n",
      "|   569823|1.4782608695652173|  0.9869980199409517|\n",
      "|   570234|3.5833333333333335|   2.542691050398726|\n",
      "|   570264|             -22.0|                 0.0|\n",
      "|   570281|              48.0|                 0.0|\n",
      "|   570592| 7.438356164383562|  11.910610328345681|\n",
      "|   571010|             -16.0|                 0.0|\n",
      "|   571906|               3.0|                 0.0|\n",
      "|   572049|              8.05|   7.559596550081228|\n",
      "|   572458|14.038461538461538|   10.04022972933147|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Agrupar por InvoiceNo e agregar usando expr() con funcións SQL\n",
    "\n",
    "# Importamos expr para escribir agregacións en formato SQL\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Agrupamos por InvoiceNo e calculamos:\n",
    "# - a media de Quantity\n",
    "# - a desviación estándar poboacional de Quantity\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"InvoiceNo\")\n",
    "         .agg(\n",
    "             expr(\"avg(Quantity)\"),\n",
    "             expr(\"stddev_pop(Quantity)\")\n",
    "         )\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Media e desviación estándar poboacional de Quantity por InvoiceNo (groupBy + expr):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74077c-7626-4e1e-9664-88ca20d30364",
   "metadata": {},
   "source": [
    "## Agregacións sobre tipos complexos\n",
    "\n",
    "Spark tamén permite realizar agregacións que non devolven un número, senón **coleccións** (tipos complexos) como listas ou conxuntos. Estas operacións son útiles cando se quere reunir valores asociados a un grupo, por exemplo:\n",
    "\n",
    "- obter todos os elementos relacionados cunha clave\n",
    "- construír listas de categorías por grupo\n",
    "- eliminar duplicados dentro dun grupo mantendo só valores únicos\n",
    "\n",
    "Dúas funcións moi habituais son:\n",
    "\n",
    "- `collect_list(col)`: recolle todos os valores dunha columna nunha lista (mantén repetidos).\n",
    "- `collect_set(col)`: recolle todos os valores únicos dunha columna (elimina repetidos; a orde non está garantida).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a8795e4-659c-4223-a06a-78189ad48cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de StockCode por InvoiceNo (collect_list, mantén repetidos):\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|InvoiceNo|stockcodes_list                                                                                                      |\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|536366   |[22633, 22632]                                                                                                       |\n",
      "|536367   |[84879, 22745, 22748, 22749, 22310, 84969, 22623, 22622, 21754, 21755, 21777, 48187]                                 |\n",
      "|536371   |[22086]                                                                                                              |\n",
      "|536374   |[21258]                                                                                                              |\n",
      "|536375   |[85123A, 71053, 84406B, 20679, 37370, 21871, 21071, 21068, 82483, 82486, 82482, 82494L, 84029G, 84029E, 22752, 21730]|\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Agrupar por InvoiceNo e recoller todos os StockCode asociados usando collect_list()\n",
    "\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Agrupamos por InvoiceNo e creamos unha lista con todos os StockCode dese grupo\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"InvoiceNo\")\n",
    "         .agg(collect_list(\"StockCode\").alias(\"stockcodes_list\"))\n",
    "         .limit(5)\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Lista de StockCode por InvoiceNo (collect_list, mantén repetidos):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1800ffb6-c6b4-4db8-976b-fc8fc104578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conxunto de StockCode por InvoiceNo (collect_set, elimina repetidos):\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|InvoiceNo|stockcodes_set                                                                                                       |\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|536366   |[22632, 22633]                                                                                                       |\n",
      "|536367   |[22310, 22622, 21755, 22623, 84879, 84969, 21777, 22748, 48187, 22745, 21754, 22749]                                 |\n",
      "|536371   |[22086]                                                                                                              |\n",
      "|536374   |[21258]                                                                                                              |\n",
      "|536375   |[84029E, 21730, 82483, 82482, 20679, 71053, 37370, 82486, 85123A, 84406B, 21071, 21068, 82494L, 84029G, 21871, 22752]|\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Agrupar por InvoiceNo e recoller só os StockCode únicos usando collect_set()\n",
    "\n",
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "# Agrupamos por InvoiceNo e creamos un conxunto (sen repetidos) de StockCode dese grupo\n",
    "resultado = (\n",
    "    df_ag.groupBy(\"InvoiceNo\")\n",
    "         .agg(collect_set(\"StockCode\").alias(\"stockcodes_set\"))\n",
    "         .limit(5)\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Conxunto de StockCode por InvoiceNo (collect_set, elimina repetidos):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba27a3-5f3d-40af-9fb5-1dec06b44594",
   "metadata": {},
   "source": [
    "## SQL sobre DataFrames en Spark\n",
    "\n",
    "Spark inclúe o módulo **Spark SQL**, que permite executar consultas SQL sobre datos estruturados (DataFrames/Datasets) usando `spark.sql(...)`. Isto non é “novo” de Spark 2.6: Spark SQL existe desde as primeiras versións de Spark, e o método `createOrReplaceTempView()` está dispoñible en PySpark **desde Spark 2.0.0**. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "A idea xeral é:\n",
    "\n",
    "1. Créase ou cárgase un DataFrame.\n",
    "2. Rexístrase como **vista temporal** no catálogo da sesión con `createOrReplaceTempView(\"nome\")`.\n",
    "3. Execútase unha consulta con `spark.sql(\"...\")`, que devolve outro DataFrame.\n",
    "4. Unha acción como `show()` materializa o resultado.\n",
    "\n",
    "Isto permite combinar a API de DataFrames con SQL, o que é útil cando:\n",
    "- se quere empregar SQL por claridade ou por familiaridade,\n",
    "- se está a migrar consultas SQL existentes,\n",
    "- se quere mesturar transformacións en DataFrame API e pasos puntuais en SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "833f2e37-5558-4cc1-bdfc-196a428d07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas da vista 'vuelos' nas que count = 1 (consulta SQL):\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear unha vista temporal a partir de df_csv e consultar con spark.sql()\n",
    "\n",
    "# Creamos o dataframe\n",
    "df_csv = spark.read.option(\"header\", True).csv(\"hdfs:///data/flight-data/csv/2015-summary.csv\").coalesce(5)\n",
    "\n",
    "# Rexistramos df_csv como vista temporal chamada \"vuelos\"\n",
    "# A vista é local á SparkSession actual\n",
    "df_csv.createOrReplaceTempView(\"vuelos\")\n",
    "\n",
    "# Executamos unha consulta SQL sobre a vista e obtemos un novo DataFrame co resultado\n",
    "resultado = spark.sql(\"SELECT * FROM vuelos WHERE count = 1\")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Filas da vista 'vuelos' nas que count = 1 (consulta SQL):\")\n",
    "\n",
    "# Amosamos unha mostra do resultado\n",
    "resultado.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5781719-28d2-4b0c-86c7-7a5cf40234b8",
   "metadata": {},
   "source": [
    "### Limitacións e detalles importantes\n",
    "\n",
    "1) A vista é temporal e non persiste\n",
    "    - `createOrReplaceTempView()` crea unha **vista temporal local**: non escribe nada a disco.\n",
    "    - A súa vida útil está ligada á **SparkSession**: cando remata a sesión, a vista desaparece. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "2) “OrReplace” sobrescribe a vista se xa existe\n",
    "    - Se xa existía unha vista co mesmo nome, substitúese pola nova definición (por iso é práctica para notebooks).\n",
    "\n",
    "3) Non é unha táboa persistente\n",
    "    - Unha vista temporal non é unha táboa real no metastore. Se se pecha a sesión, pérdese.\n",
    "    - Para persistir datos, úsase `df.write...` e/ou creación de táboas no catálogo (segundo configuración do contorno).\n",
    "\n",
    "4) SQL de Spark non é exactamente “calquera SQL”\n",
    "    - Spark SQL é moi potente, pero ten particularidades e extensións propias.\n",
    "    - En Spark 3.x hai unha tendencia a maior compatibilidade ANSI (configurable), pero non se debe asumir que todas as funcións e sintaxe dun SGBD relacional van funcionar igual. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "5) “Primeiro rexistro a vista, logo consulto” tamén é lazy\n",
    "    - `spark.sql(...)` devolve un DataFrame; non executa ata que apareza unha acción (`show`, `count`, `write`, etc.).\n",
    "\n",
    "6) Alternativa: vista temporal global\n",
    "    - Existe tamén `createOrReplaceGlobalTempView()`, cuxa vida útil está ligada á aplicación e se consulta baixo o esquema `global_temp`. (Útil cando se quere compartir a vista entre sesións dentro da mesma aplicación). :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "7) Recomendación práctica: poñer LIMIT nas consultas exploratorias\n",
    "    - Para exploración, `LIMIT` reduce saída e custo de traer resultados ao driver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c3afc43-fd52-4a76-ace2-91c6e15412a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostra (LIMIT 5) de filas onde count = 1:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Usar LIMIT na consulta para exploración (equivalente a show(n), pero a nivel SQL)\n",
    "\n",
    "# Executamos a consulta con LIMIT para obter unha mostra pequena\n",
    "resultado = spark.sql(\"SELECT * FROM vuelos WHERE count = 1 LIMIT 5\")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Mostra (LIMIT 5) de filas onde count = 1:\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0daf6acc-f261-45be-9258-804221788de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta sobre a vista temporal global global_temp.vuelos_global (count = 1):\n",
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|    1|\n",
      "|    United States|    1|\n",
      "|          Moldova|    1|\n",
      "|            Malta|    1|\n",
      "|    United States|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Vista temporal global (opcional), accesible como global_temp.<nome>\n",
    "\n",
    "# Creamos unha vista temporal global\n",
    "df_csv.createOrReplaceGlobalTempView(\"vuelos_global\")\n",
    "\n",
    "# Consultámola usando o esquema global_temp\n",
    "resultado = spark.sql(\"SELECT DEST_COUNTRY_NAME, count FROM global_temp.vuelos_global WHERE count = 1\")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Consulta sobre a vista temporal global global_temp.vuelos_global (count = 1):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b99f-3eab-4b50-8676-cfb80759bfda",
   "metadata": {},
   "source": [
    "### Boas prácticas ao mesturar DataFrames e SQL\n",
    "\n",
    "- Usar SQL para consultas claras e “de lectura” (seleccións, filtros, agregacións).\n",
    "- Preferir DataFrame API cando se quere máis control programático (funcións, condicións dinámicas, reutilización de código).\n",
    "- Manter nomes de columnas “limpos” (sen espazos nin caracteres raros) para evitar ter que escapalos en SQL.\n",
    "- Para depurar, `df.explain()` pode axudar a ver o plan de execución (tamén sobre resultados de `spark.sql()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8350faf-3056-424e-87b0-f2a8ba9b151f",
   "metadata": {},
   "source": [
    "## Exportar a Pandas\n",
    "\n",
    "Pódese converter un DataFrame de Spark nun DataFrame de Pandas empregando o método `toPandas()`. Isto trae os datos ao proceso do **driver** e crea un obxecto `pandas.DataFrame`, o que facilita análises locais, gráficos e traballo con librarías do ecosistema Python.\n",
    "\n",
    "É importante ter en conta que `toPandas()` é unha acción e require que todos os datos quepan na memoria do driver. Por ese motivo, adoita empregarse sobre:\n",
    "- subconxuntos pequenos (por exemplo usando `limit()`)\n",
    "- mostras (por exemplo usando `sample()`)\n",
    "- agregacións xa resumidas\n",
    "\n",
    "Antes de exportar a Pandas, resulta útil lembrar como se pode xerar un DataFrame de Spark cunha estrutura (esquema) específica, definindo columnas e tipos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9f2420e-aada-4499-92b9-ea7bb603a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostra do DataFrame lido con esquema explícito:\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|       NULL|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|       NULL|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|       NULL|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|       NULL|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|       NULL|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|       NULL|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|       NULL|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Suma de Quantity por Country:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|           Country|sum(Quantity)|\n",
      "+------------------+-------------+\n",
      "|            Sweden|        35637|\n",
      "|         Singapore|         5234|\n",
      "|           Germany|       117448|\n",
      "|            France|       110480|\n",
      "|            Greece|         1556|\n",
      "|European Community|          497|\n",
      "|           Belgium|        23152|\n",
      "|           Finland|        10666|\n",
      "|             Malta|          944|\n",
      "|       Unspecified|         3300|\n",
      "|             Italy|         7999|\n",
      "|              EIRE|       142637|\n",
      "|         Lithuania|          652|\n",
      "|            Norway|        19247|\n",
      "|             Spain|        26824|\n",
      "|           Denmark|         8188|\n",
      "|         Hong Kong|         4769|\n",
      "|           Iceland|         2458|\n",
      "|            Israel|         4353|\n",
      "|   Channel Islands|         9479|\n",
      "+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame en Pandas (resultado agregado):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>sum(Quantity)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>35637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>5234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>117448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RSA</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>110480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Greece</td>\n",
       "      <td>1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>23152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finland</td>\n",
       "      <td>10666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malta</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unspecified</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Italy</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EIRE</td>\n",
       "      <td>142637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Norway</td>\n",
       "      <td>19247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Spain</td>\n",
       "      <td>26824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>4769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Israel</td>\n",
       "      <td>4353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Channel Islands</td>\n",
       "      <td>9479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>USA</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>6317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>30325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Canada</td>\n",
       "      <td>2763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Japan</td>\n",
       "      <td>25218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Poland</td>\n",
       "      <td>3653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>16180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Australia</td>\n",
       "      <td>83653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Austria</td>\n",
       "      <td>4827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4263829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>200128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>European Community</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lithuania</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lebanon</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bahrain</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Country  sum(Quantity)\n",
       "0                 Sweden          35637\n",
       "1              Singapore           5234\n",
       "2                Germany         117448\n",
       "3                    RSA            352\n",
       "4                 France         110480\n",
       "5                 Greece           1556\n",
       "6                Belgium          23152\n",
       "7                Finland          10666\n",
       "8                  Malta            944\n",
       "9            Unspecified           3300\n",
       "10                 Italy           7999\n",
       "11                  EIRE         142637\n",
       "12                Norway          19247\n",
       "13                 Spain          26824\n",
       "14               Denmark           8188\n",
       "15             Hong Kong           4769\n",
       "16                Israel           4353\n",
       "17               Iceland           2458\n",
       "18       Channel Islands           9479\n",
       "19                   USA           1034\n",
       "20                Cyprus           6317\n",
       "21           Switzerland          30325\n",
       "22  United Arab Emirates            982\n",
       "23                Canada           2763\n",
       "24        Czech Republic            592\n",
       "25                 Japan          25218\n",
       "26                Poland           3653\n",
       "27              Portugal          16180\n",
       "28             Australia          83653\n",
       "29               Austria           4827\n",
       "30        United Kingdom        4263829\n",
       "31           Netherlands         200128\n",
       "32    European Community            497\n",
       "33             Lithuania            652\n",
       "34          Saudi Arabia             75\n",
       "35                Brazil            356\n",
       "36               Lebanon            386\n",
       "37               Bahrain            260"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Exemplo: Ler un CSV con esquema explícito, agregar por país, exportar a Pandas e visualizar resultados\n",
    "\n",
    "# Importamos os tipos necesarios para definir un esquema explícito en Spark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Definimos o esquema (nome, tipo e nulabilidade de cada columna)\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Lemos o CSV desde HDFS aplicando o esquema especificado (evítase inferSchema e garántense tipos)\n",
    "df_struct = spark.read.csv(\n",
    "    \"hdfs:///data/retail-data/all/online-retail-dataset.csv\",\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Amosamos unha mostra das primeiras filas para comprobar que a lectura foi correcta\n",
    "print(\"Mostra do DataFrame lido con esquema explícito:\")\n",
    "df_struct.show(10)\n",
    "\n",
    "# Agrupamos por Country e calculamos a suma total de Quantity para cada país\n",
    "df2 = df_struct.groupBy(\"Country\").sum(\"Quantity\")\n",
    "\n",
    "# Amosamos o resultado da agregación\n",
    "print(\"Suma de Quantity por Country:\")\n",
    "df2.show()\n",
    "\n",
    "# Convertimos o resultado agregado a Pandas (o resultado xa é pequeno e adoita caber no driver)\n",
    "df2_pandas = df2.toPandas()\n",
    "\n",
    "# Amosamos o DataFrame de Pandas no notebook\n",
    "print(\"DataFrame en Pandas (resultado agregado):\")\n",
    "display(df2_pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78cc6675-d94c-422b-be09-e83436e32f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplo: Crear un gráfico en Pandas/Matplotlib a partir do DataFrame obtido\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Renomeamos a columna agregada para que sexa máis cómoda de usar e visualizar\n",
    "# Spark adoita chamar a esta columna \"sum(Quantity)\"\n",
    "df_plot = df2_pandas.rename(columns={\"sum(Quantity)\": \"sum_quantity\"})\n",
    "\n",
    "# Ordenamos por suma descendente e quedamos cos 10 primeiros países para que o gráfico sexa lexible\n",
    "df_top10 = df_plot.sort_values(\"sum_quantity\", ascending=False).head(10)\n",
    "\n",
    "# Creamos un gráfico de barras horizontal (máis lexible con nomes de países)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(df_top10[\"Country\"], df_top10[\"sum_quantity\"])\n",
    "plt.xlabel(\"Suma total de Quantity\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.title(\"Top 10 países por suma de Quantity\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
