{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd43e5-4d09-4e94-aa83-84ea1e2c7bc2",
   "metadata": {},
   "source": [
    "## DataFrames: operacións avanzadas\n",
    "\n",
    "Agora que xa se coñecen as operacións básicas con DataFrames, é o momento de ver operacións máis complexas e habituais en proxectos reais, como por exemplo:\n",
    "\n",
    "- joins\n",
    "- agrupacións complexas\n",
    "- operacións con xanelas (window functions)\n",
    "- rollups\n",
    "- cubos\n",
    "- pivotes\n",
    "\n",
    "Nalgúns exemplos (especialmente cando hai datas en formato texto), pode ser necesario axustar a configuración da sesión de Spark para que o parseo e o tratamento de datas sexa consistente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82059df1-f06a-4c9d-bbc7-719eb537b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4e105918-a4af-4746-995c-fcf6d4b1a7f4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.1.0/delta-spark_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.1.0!delta-spark_2.12.jar (560ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.7/spark-sql-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7!spark-sql-kafka-0-10_2.12.jar (152ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.5.1!kafka-clients.jar (575ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.1.0/delta-storage-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.1.0!delta-storage.jar (79ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (109ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.7/spark-token-provider-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7!spark-token-provider-kafka-0-10_2.12.jar (79ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (5086ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2319ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.5!snappy-java.jar(bundle) (279ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (72ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.5.5-1!zstd-jni.jar (562ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (148ms)\n",
      ":: resolution report :: resolve 11654ms :: artifacts dl 10283ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   15  |   15  |   3   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4e105918-a4af-4746-995c-fcf6d4b1a7f4\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (68764kB/461ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zona horaria da sesión (spark.sql.session.timeZone):\n",
      "UTC\n",
      "Política de parseo de datas (spark.sql.legacy.timeParserPolicy):\n",
      "LEGACY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 36462)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"04-Dataframes-02\") \\\n",
    ".config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Amosamos a configuración aplicada\n",
    "print(\"Zona horaria da sesión (spark.sql.session.timeZone):\")\n",
    "print(spark.conf.get(\"spark.sql.session.timeZone\"))\n",
    "\n",
    "print(\"Política de parseo de datas (spark.sql.legacy.timeParserPolicy):\")\n",
    "print(spark.conf.get(\"spark.sql.legacy.timeParserPolicy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f021c-e3de-4573-a068-664fca37fc0a",
   "metadata": {},
   "source": [
    "## Joins en DataFrames\n",
    "\n",
    "Os **joins** permiten combinar datos de dous DataFrames a partir dunha condición, normalmente baseada nunha ou varias columnas comúns. O funcionamento é equivalente aos joins das bases de datos relacionais, pero aplicado a DataFrames distribuídos.\n",
    "\n",
    "Antes de ver os distintos tipos de join, créanse uns DataFrames de exemplo cos que se vai traballar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba1662c-5db6-4949-a659-03c413af278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame person:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------+---------------+\n",
      "| id|            name|graduate|          marks|\n",
      "+---+----------------+--------+---------------+\n",
      "|  0|   Bill Chambers|       0|          [100]|\n",
      "|  1|   Matel Zaharla|       1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|       1|     [250, 100]|\n",
      "+---+----------------+--------+---------------+\n",
      "\n",
      "DataFrame graduate_program:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1| Ph. D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n",
      "DataFrame spark_status:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear DataFrames de exemplo para traballar con joins\n",
    "\n",
    "# DataFrame de persoas\n",
    "# - id: identificador da persoa\n",
    "# - name: nome\n",
    "# - graduate: identificador do programa (clave de unión)\n",
    "# - marks: lista de identificadores de status\n",
    "person = (\n",
    "    spark.createDataFrame([\n",
    "        (0, \"Bill Chambers\", 0, [100]),\n",
    "        (1, \"Matel Zaharla\", 1, [500, 250, 100]),\n",
    "        (2, \"Michael Armbrust\", 1, [250, 100])\n",
    "    ])\n",
    "    .toDF(\"id\", \"name\", \"graduate\", \"marks\")\n",
    ")\n",
    "\n",
    "print(\"DataFrame person:\")\n",
    "person.show()\n",
    "\n",
    "# DataFrame cos programas de posgrao\n",
    "# - id: identificador do programa (clave de unión)\n",
    "# - degree: tipo de título\n",
    "# - department: departamento\n",
    "# - school: universidade\n",
    "graduate_program = (\n",
    "    spark.createDataFrame([\n",
    "        (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "        (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "        (1, \"Ph. D.\", \"EECS\", \"UC Berkeley\")\n",
    "    ])\n",
    "    .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    ")\n",
    "\n",
    "print(\"DataFrame graduate_program:\")\n",
    "graduate_program.show()\n",
    "\n",
    "\n",
    "# DataFrame cos status asociados ás marcas\n",
    "spark_status = (\n",
    "    spark.createDataFrame([\n",
    "        (500, \"Vice President\"),\n",
    "        (250, \"PMC Member\"),\n",
    "        (100, \"Contributor\")\n",
    "    ])\n",
    "    .toDF(\"id\", \"status\")\n",
    ")\n",
    "\n",
    "print(\"DataFrame spark_status:\")\n",
    "spark_status.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c5e48-beab-4131-8c2b-10d5621ec3ac",
   "metadata": {},
   "source": [
    "### Inner join\n",
    "\n",
    "O **inner join** avalía a condición de unión en ambos DataFrames e devolve **unicamente** as filas nas que a avaliación é verdadeira, é dicir, aquelas filas que teñen correspondencia nos dous DataFrames.\n",
    "\n",
    "As filas que non teñen clave coincidente en algún dos DataFrames **non aparecen** no resultado final.\n",
    "\n",
    "É o tipo de join máis habitual cando se quere traballar só cos rexistros que existen en ambos conxuntos de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b2c737a-fd08-473d-a393-eb25dbb40f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join entre person e graduate_program (só filas con coincidencia na clave):\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate|          marks| id| degree|          department|     school|\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|       0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matel Zaharla|       1|[500, 250, 100]|  1| Ph. D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|       1|     [250, 100]|  1| Ph. D.|                EECS|UC Berkeley|\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Inner join entre person e graduate_program usando unha expresión de unión\n",
    "\n",
    "# Definimos a condición de unión:\n",
    "# - person.graduate contén o id do programa asociado á persoa\n",
    "# - graduate_program.id é o id do programa\n",
    "join_expression = person[\"graduate\"] == graduate_program[\"id\"]\n",
    "\n",
    "# Realizamos o inner join:\n",
    "# Só se devolverán as filas nas que exista coincidencia entre person.graduate e graduate_program.id\n",
    "resultado = person.join(graduate_program, join_expression)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Inner join entre person e graduate_program (só filas con coincidencia na clave):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba44b1-893a-43be-9fbe-6c9e809d716e",
   "metadata": {},
   "source": [
    "### Outer join (full outer)\n",
    "\n",
    "O **outer join** (ou *full outer join*) avalía a condición de unión en ambos DataFrames e devolve:\n",
    "\n",
    "- as filas nas que a condición é verdadeira (hai coincidencia),\n",
    "- e tamén as filas nas que a condición é falsa (non hai coincidencia).\n",
    "\n",
    "Cando unha fila dun dos DataFrames non ten correspondencia no outro, os campos correspondentes complétanse con `NULL`.\n",
    "\n",
    "Este tipo de join é útil cando se quere **conservar toda a información de ambos DataFrames**, mesmo cando non hai coincidencia entre eles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f821d31-9684-4896-ae31-c617fbeb1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Full outer join entre person e graduate_program\n",
    "\n",
    "# Indicamos explicitamente o tipo de join\n",
    "join_type = \"outer\"\n",
    "\n",
    "# Realizamos o outer join:\n",
    "# Mantéñense todas as filas de ambos DataFrames, usando NULL onde non exista valor\n",
    "resultado = person.join(graduate_program, join_expression, join_type)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Outer join entre person e graduate_program (mantén todas as filas):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571df2e3-226a-4696-a0b3-f4c8712d9d7f",
   "metadata": {},
   "source": [
    "### Left outer join\n",
    "\n",
    "O **left outer join** avalía a condición de unión en ambos DataFrames e devolve:\n",
    "\n",
    "- **todas as filas** do DataFrame da esquerda (`person`)\n",
    "- unidas ás filas do DataFrame da dereita (`graduate_program`) **só cando hai coincidencia** (condición verdadeira)\n",
    "\n",
    "Cando unha fila do DataFrame esquerdo non ten correspondencia no dereito, os campos do DataFrame dereito complétanse con `NULL`.\n",
    "\n",
    "Este tipo de join é útil cando se quere manter como referencia o DataFrame da esquerda e engadir información do dereito cando exista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e0163-23e8-494c-b2ff-3bf6320ebbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Left outer join entre person e graduate_program\n",
    "\n",
    "# Indicamos explicitamente o tipo de join\n",
    "join_type = \"left_outer\"\n",
    "\n",
    "# Realizamos o left outer join:\n",
    "# Mantéñense todas as filas de person, e só se engaden datos de graduate_program cando hai match\n",
    "resultado = person.join(graduate_program, join_expression, join_type)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Left outer join entre person e graduate_program (mantén todas as filas do esquerdo):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f14ec1-7fc1-4a4f-a9c6-17f3a502ebe2",
   "metadata": {},
   "source": [
    "### Right outer join\n",
    "\n",
    "O **right outer join** avalía a condición de unión en ambos DataFrames e devolve:\n",
    "\n",
    "- **todas as filas** do DataFrame da dereita (`graduate_program`)\n",
    "- unidas ás filas do DataFrame da esquerda (`person`) **só cando hai coincidencia** (condición verdadeira)\n",
    "\n",
    "Cando unha fila do DataFrame dereito non ten correspondencia no esquerdo, os campos do DataFrame esquerdo complétanse con `NULL`.\n",
    "\n",
    "Este tipo de join é útil cando se quere manter como referencia o DataFrame da dereita e engadir información do esquerdo cando exista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf9686-525b-46b5-a03d-a3556bae4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Right outer join entre person e graduate_program\n",
    "\n",
    "# Indicamos explicitamente o tipo de join\n",
    "join_type = \"right_outer\"\n",
    "\n",
    "# Realizamos o right outer join:\n",
    "# Mantéñense todas as filas de graduate_program, e só se engaden datos de person cando hai match\n",
    "resultado = person.join(graduate_program, join_expression, join_type)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Right outer join entre person e graduate_program (mantén todas as filas do dereito):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0671797-adb3-481b-b276-064963d75e2b",
   "metadata": {},
   "source": [
    "### Left semi join\n",
    "\n",
    "O **left semi join** é un tipo especial de join que **non incorpora columnas do segundo DataFrame**.  \n",
    "Limítase a devolver as filas do **primeiro DataFrame** (`person`) que **teñen correspondencia** no segundo (`graduate_program`) segundo a condición de unión.\n",
    "\n",
    "Características importantes:\n",
    "- só se mostran columnas do DataFrame esquerdo\n",
    "- serve para comprobar existencia de coincidencia (equivalente a un `WHERE EXISTS` en SQL)\n",
    "- é máis eficiente que un join normal cando só interesa saber se hai match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f44992-4ad4-4397-bef0-1d871653780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Left semi join entre person e graduate_program\n",
    "\n",
    "# Indicamos explicitamente o tipo de join\n",
    "join_type = \"left_semi\"\n",
    "\n",
    "# Realizamos o left semi join:\n",
    "# Devolve só as filas de person que teñen match en graduate_program\n",
    "resultado = person.join(graduate_program, join_expression, join_type)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Left semi join entre person e graduate_program (só filas do DataFrame esquerdo con match):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d0913-ab77-496c-bf83-e6713e2256ae",
   "metadata": {},
   "source": [
    "### Left anti join\n",
    "\n",
    "O **left anti join** é o oposto do *left semi join*.  \n",
    "Devolve as filas do **primeiro DataFrame** (`person`) que **non teñen correspondencia** no segundo (`graduate_program`) segundo a condición de unión.\n",
    "\n",
    "Características importantes:\n",
    "- só se mostran columnas do DataFrame esquerdo\n",
    "- serve para obter “os que non existen no outro DataFrame” (equivalente a un `WHERE NOT EXISTS` en SQL)\n",
    "- é útil para detectar rexistros orfos, discrepancias entre fontes ou faltas de referencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c4e2a-24fa-461c-9bf8-1fcd8642330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Left anti join entre person e graduate_program\n",
    "\n",
    "# Indicamos explicitamente o tipo de join\n",
    "join_type = \"left_anti\"\n",
    "\n",
    "# Realizamos o left anti join:\n",
    "# Devolve só as filas de person que NON teñen match en graduate_program\n",
    "resultado = person.join(graduate_program, join_expression, join_type)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Left anti join entre person e graduate_program (filas do esquerdo sen match):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be531b6-320a-4587-9bd3-a98937e19619",
   "metadata": {},
   "source": [
    "### Cross join\n",
    "\n",
    "O **cross join** realiza un **produto cartesiano** entre dous DataFrames.  \n",
    "Cada fila do primeiro DataFrame combínase con **todas** as filas do segundo DataFrame.\n",
    "\n",
    "O número de filas do resultado será:\n",
    "`n_filas_df1 × n_filas_df2`\n",
    "\n",
    "Este tipo de join non utiliza ningunha condición de unión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60a056-316c-4a95-9841-e2b3f976aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Cross join entre person e graduate_program\n",
    "\n",
    "# Realizamos un produto cartesiano entre ambos DataFrames\n",
    "resultado = person.crossJoin(graduate_program)\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Cross join entre person e graduate_program (produto cartesiano):\")\n",
    "\n",
    "# Amosamos o resultado do join\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89db5c1-e2ae-4fef-90ad-82b85c349619",
   "metadata": {},
   "source": [
    "### Nota de rendemento (broadcast join)\n",
    "\n",
    "Cando un dos DataFrames é pequeno (por exemplo, táboas de referencia), é recomendable empregar un **broadcast join** para evitar shuffle.\n",
    "\n",
    "Spark copiará o DataFrame pequeno a todos os executores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "274d6afc-0bef-44ca-9191-e7c5230ef008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join usando broadcast para optimizar rendemento:\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate|          marks| id| degree|          department|     school|\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|       0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matel Zaharla|       1|[500, 250, 100]|  1| Ph. D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|       1|     [250, 100]|  1| Ph. D.|                EECS|UC Berkeley|\n",
      "+---+----------------+--------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Broadcast join cando graduate_program é pequeno\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "resultado = person.join(\n",
    "    broadcast(graduate_program),\n",
    "    join_expression,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Inner join usando broadcast para optimizar rendemento:\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d807cb4-bc02-40ea-9433-3ab250717880",
   "metadata": {},
   "source": [
    "## Operacións con xanelas (window functions)\n",
    "\n",
    "As **operacións con xanelas** permiten calcular valores agregados mantendo as filas orixinais. Isto é especialmente útil cando se quere obter métricas “por grupo” (por exemplo por país, por cliente, por día…) sen perder o detalle de cada rexistro.\n",
    "\n",
    "A diferenza dun `groupBy()` clásico:\n",
    "- nun `groupBy()`, as filas agrúpanse e o resultado devolve **unha fila por grupo** (redúcese o número de filas).\n",
    "- nunha función de xanela, calcúlase un valor para **cada fila de entrada**, tomando como referencia un conxunto de filas relacionado con ela, ao que se chama **marco** (frame).\n",
    "\n",
    "Cada fila pode pertencer a máis dun marco, por exemplo en cálculos por intervalo temporal (medias móbiles), onde a fila dun día pode entrar nos marcos de varios días adxacentes.\n",
    "\n",
    "![Funcións con xanelas](./images/windows.png)\n",
    "\n",
    "A especificación dunha xanela defínese normalmente con dúas partes:\n",
    "- `partitionBy(...)`: define como se agrupan as filas (equivalente á clave do groupBy, pero sen colapsar filas)\n",
    "- `orderBy(...)`: define a orde dentro de cada partición (habitual cando se traballa con tempo ou ranking)\n",
    "\n",
    "A continuación prepárase un DataFrame engadindo unha columna `date` a partir de `InvoiceDate`, eliminando horas e minutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400679e4-240b-46b2-956d-caa9ad3479df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema de df_ag:\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Cargar o DataFrame base e preparalo para traballar con xanelas engadindo unha columna date\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format\n",
    "\n",
    "# Cargamos os CSV e inferimos o esquema\n",
    "df_ag = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"hdfs:///data/retail-data/all/*.csv\")\n",
    "        .coalesce(5)\n",
    ")\n",
    "\n",
    "# Gardamos en caché para reutilización en exemplos posteriores\n",
    "df_ag.cache()\n",
    "\n",
    "# Rexistramos unha vista temporal (opcional) para consultas SQL\n",
    "df_ag.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "# Amosamos o esquema para ver os tipos das columnas\n",
    "print(\"Esquema de df_ag:\")\n",
    "df_ag.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d154a42-23bc-42fa-94d3-bed7ca40bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostra de df_con_fecha (con columna date e sen InvoiceDate):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear unha columna 'date' sen hora/minuto a partir de InvoiceDate\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format\n",
    "\n",
    "# Convertimos InvoiceDate a timestamp usando un patrón explícito e despois formateámolo como 'yyyy-MM-dd'\n",
    "df_con_fecha = df_ag.withColumn(\n",
    "    \"date\",\n",
    "    date_format(\n",
    "        to_timestamp(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"),\n",
    "        \"yyyy-MM-dd\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Eliminamos a columna InvoiceDate para quedarnos só coa data simplificada\n",
    "df_con_fecha = df_con_fecha.drop(\"InvoiceDate\")\n",
    "\n",
    "# Rexistramos unha vista temporal para traballar tamén con SQL se se quere\n",
    "df_con_fecha.createOrReplaceTempView(\"dfConFecha\")\n",
    "\n",
    "# Amosamos unha mostra para comprobar o resultado\n",
    "print(\"Mostra de df_con_fecha (con columna date e sen InvoiceDate):\")\n",
    "df_con_fecha.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c3de9-bacb-42d2-aebd-9073206baa0a",
   "metadata": {},
   "source": [
    "O primeiro paso para empregar unha función de xanela é crear unha **especificación de xanela** (*window specification*).\n",
    "\n",
    "- `partitionBy(...)` non está relacionado co concepto de particións (repartition/coalesce) visto antes. Aquí úsase para indicar como se “dividen” as filas en **grupos lóxicos** sobre os que se aplicará a xanela.\n",
    "- `orderBy(...)` determina a orde das filas dentro de cada partición lóxica.\n",
    "- `rowsBetween(...)` define que filas se inclúen no **marco** (*frame*) para cada fila de entrada. É dicir, indica o rango de filas (anteriores, seguintes, etc.) que se teñen en conta ao calcular unha agregación para esa fila.\n",
    "\n",
    "No seguinte exemplo o marco inclúe todas as filas desde o inicio da partición (*unboundedPreceding*) ata a fila actual (*currentRow*). Isto permite calcular agregacións “acumuladas” segundo a orde definida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cd2b46-d8e4-4088-8d99-b4d8aa84fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowSpec definido: partitionBy(CustomerId, date), orderBy(Quantity desc), rowsBetween(unboundedPreceding, currentRow)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Definir unha especificación de xanela para CustomerId e date ordenando por Quantity (desc)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Definimos a xanela:\n",
    "# - Partición lóxica por CustomerId e date (cada cliente e día)\n",
    "# - Orde interna por Quantity descendente (máis cantidade primeiro)\n",
    "# - Marco: desde o inicio da partición ata a fila actual\n",
    "windowSpec = (\n",
    "    Window\n",
    "        .partitionBy(\"CustomerId\", \"date\")\n",
    "        .orderBy(desc(\"Quantity\"))\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "print(\"WindowSpec definido: partitionBy(CustomerId, date), orderBy(Quantity desc), rowsBetween(unboundedPreceding, currentRow)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0614b4d-36ac-4aec-ad0e-63454e26db62",
   "metadata": {},
   "source": [
    "O seguinte paso é definir unha **agregación de xanela**. Neste caso, calcúlase a cantidade máxima (`max`) dentro do marco definido pola xanela.\n",
    "\n",
    "Ao empregar `.over(windowSpec)`, a función de agregación pasa a calcularse **para cada fila**, en vez de devolver unha única fila por grupo como faría un `groupBy()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6c507d-64e5-4289-81b1-21ed1e6fe1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna de xanela definida: maxPurchaseQuantity = max(Quantity) over(windowSpec)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Definir unha agregación de xanela (máxima cantidade) usando over(windowSpec)\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Creamos unha expresión de columna que calcula o máximo de Quantity dentro do marco da xanela\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "\n",
    "print(\"Columna de xanela definida: maxPurchaseQuantity = max(Quantity) over(windowSpec)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50441-d85c-4baa-ae7f-21e6d50e9db2",
   "metadata": {},
   "source": [
    "As funcións `rank()` e `dense_rank()` permiten crear un ranking dentro de cada partición lóxica seguindo a orde definida no `orderBy(...)`.\n",
    "\n",
    "- `rank()`: se hai empates, asigna o mesmo rango e deixa “ocos” na secuencia.\n",
    "- `dense_rank()`: se hai empates, asigna o mesmo rango pero non deixa “ocos”.\n",
    "\n",
    "Ambas devolven columnas (expresións) que se poden empregar en `select()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a69e0c-21e8-480d-9c3e-c5f7c925eb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de xanela definidas: purchaseRank e purchaseDenseRank\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Definir rank e dense_rank sobre a xanela\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "# Definimos as columnas de ranking segundo a xanela\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "\n",
    "print(\"Columnas de xanela definidas: purchaseRank e purchaseDenseRank\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ae0bc-8836-4140-9559-c9a76491336d",
   "metadata": {},
   "source": [
    "Antes de aplicar as xanelas, pode amosarse unha mostra do DataFrame para comprobar que existen as columnas `CustomerId`, `date` e `Quantity`. A continuación aplícanse as columnas de xanela nun `select()` para visualizar os valores calculados.\n",
    "\n",
    "Neste exemplo:\n",
    "- fíltranse filas con `CustomerId` non nulo\n",
    "- ordénase por `CustomerId` para facilitar a lectura\n",
    "- selecciónanse columnas orixinais e columnas calculadas por xanela\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deec3126-ce40-403d-b0f2-8a95f9da2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostra de df_con_fecha:\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Amosar unha mostra do DataFrame de entrada\n",
    "\n",
    "print(\"Mostra de df_con_fecha:\")\n",
    "df_con_fecha.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1ad3ed-8798-4060-bb60-c981557d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de xanela por CustomerId e date (rank, dense_rank, maxPurchaseQuantity):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Aplicar columnas de xanela nun select() para ver rank, dense_rank e máximo por marco\n",
    "\n",
    "resultado = (\n",
    "    df_con_fecha.where(col(\"CustomerId\").isNotNull())\n",
    "        .orderBy(\"CustomerId\")\n",
    "        .select(\n",
    "            col(\"CustomerId\"),\n",
    "            col(\"date\"),\n",
    "            col(\"Quantity\"),\n",
    "            purchaseRank.alias(\"quantityRank\"),\n",
    "            purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "            maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")\n",
    "        )\n",
    ")\n",
    "\n",
    "print(\"Valores de xanela por CustomerId e date (rank, dense_rank, maxPurchaseQuantity):\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7e1df-5777-4463-9b6e-72bc6d83ce96",
   "metadata": {},
   "source": [
    "Un dos usos máis habituais das funcións de xanela é calcular **valores acumulados**, por exemplo:\n",
    "- vendas acumuladas\n",
    "- cantidades acumuladas por cliente\n",
    "- métricas ao longo do tempo\n",
    "\n",
    "Isto non se pode facer cun `groupBy()` clásico sen perder o detalle das filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd19997-12e0-4734-ac51-5ef1ff4c550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidade acumulada por país ao longo do tempo:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+------------------+\n",
      "|  Country|      date|Quantity|quantity_acumulada|\n",
      "+---------+----------+--------+------------------+\n",
      "|Australia|2010-12-01|       6|                 6|\n",
      "|Australia|2010-12-01|       8|                14|\n",
      "|Australia|2010-12-01|      12|                26|\n",
      "|Australia|2010-12-01|       6|                32|\n",
      "|Australia|2010-12-01|       4|                36|\n",
      "|Australia|2010-12-01|       6|                42|\n",
      "|Australia|2010-12-01|       3|                45|\n",
      "|Australia|2010-12-01|       2|                47|\n",
      "|Australia|2010-12-01|       4|                51|\n",
      "|Australia|2010-12-01|       4|                55|\n",
      "+---------+----------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Calcular a cantidade acumulada por Country ao longo do tempo\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Definimos unha xanela por país, ordenada por data\n",
    "window_acumulado = (\n",
    "    Window\n",
    "        .partitionBy(\"Country\")\n",
    "        .orderBy(\"date\")\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "# Calculamos a suma acumulada de Quantity\n",
    "df_acumulado = (\n",
    "    df_con_fecha.withColumn(\n",
    "        \"quantity_acumulada\",\n",
    "        sum(col(\"Quantity\")).over(window_acumulado)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Cantidade acumulada por país ao longo do tempo:\")\n",
    "df_acumulado.select(\"Country\", \"date\", \"Quantity\", \"quantity_acumulada\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23ab51-e3c7-453b-b02b-8a7d5788ee09",
   "metadata": {},
   "source": [
    "Outro caso moi habitual é querer **un único rexistro por grupo**, por exemplo:\n",
    "- a última compra dun cliente\n",
    "- o valor máximo por día\n",
    "- o rexistro máis recente por clave\n",
    "\n",
    "Para isto emprégase `row_number()` xunto cunha xanela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c4fb8d-c1a9-4697-9d8a-bb77350a94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Última compra por cliente:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|CustomerId|      date|Quantity|\n",
      "+----------+----------+--------+\n",
      "|      NULL|2011-12-09|       2|\n",
      "|     12346|2011-01-18|   74215|\n",
      "|     12347|2011-12-07|      12|\n",
      "|     12348|2011-09-25|     120|\n",
      "|     12349|2011-11-21|       2|\n",
      "|     12350|2011-02-02|      12|\n",
      "|     12352|2011-11-03|      12|\n",
      "|     12353|2011-05-19|       2|\n",
      "|     12354|2011-04-21|      10|\n",
      "|     12355|2011-05-09|      24|\n",
      "+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Obter a última compra por CustomerId (a máis recente por data)\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Definimos unha xanela por cliente, ordenada por data descendente\n",
    "window_ultimo = (\n",
    "    Window\n",
    "        .partitionBy(\"CustomerId\")\n",
    "        .orderBy(col(\"date\").desc())\n",
    ")\n",
    "\n",
    "# Calculamos o número de fila dentro de cada partición\n",
    "df_rank = df_con_fecha.withColumn(\n",
    "    \"row_num\",\n",
    "    row_number().over(window_ultimo)\n",
    ")\n",
    "\n",
    "# Quedámonos só coa fila máis recente de cada cliente\n",
    "ultimas_compras = df_rank.where(col(\"row_num\") == 1)\n",
    "\n",
    "print(\"Última compra por cliente:\")\n",
    "ultimas_compras.select(\"CustomerId\", \"date\", \"Quantity\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc6ddd-ec29-4b53-8566-ed9283a50d4a",
   "metadata": {},
   "source": [
    "## Grouping sets\n",
    "\n",
    "Ata agora víronse agrupacións “simples” nas que se agrega usando un único conxunto fixo de columnas (por exemplo `GROUP BY customerId, stockCode`). Porén, ás veces é necesario obter **varios niveis de agregación nunha única consulta**, por exemplo:\n",
    "\n",
    "- agregación por (customerId, stockCode)\n",
    "- e tamén un total xeral sen ningunha columna de agrupación\n",
    "\n",
    "Para iso existen os **grouping sets**, unha ferramenta de baixo nivel de SQL que permite combinar distintos conxuntos de agrupación dentro dun mesmo `GROUP BY`. O resultado inclúe filas correspondentes a cada conxunto indicado.\n",
    "\n",
    "A idea é:\n",
    "- `GROUPING SETS ((a,b), (a), ())`\n",
    "  - agrega por (a,b)\n",
    "  - agrega por (a)\n",
    "  - agrega total xeral con `()`\n",
    "\n",
    "Nos exemplos seguintes prepárase primeiro un DataFrame sen nulos nas columnas clave e créase unha vista temporal para consultar con SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11561ca-3a9f-457b-a7dc-8fd0319f2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostra de dfNoNulo (sen nulos en CustomerId e StockCode):\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Preparar un DataFrame sen nulos nas columnas clave e crear unha vista temporal\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Eliminamos filas con nulos nas columnas que se van usar no GROUP BY\n",
    "# (CustomerId pode ter nulos no dataset, polo que se filtra explicitamente)\n",
    "df_no_nulo = df_con_fecha.where(\n",
    "    col(\"CustomerId\").isNotNull() & col(\"StockCode\").isNotNull()\n",
    ")\n",
    "\n",
    "# Rexistramos a vista temporal para consultas SQL\n",
    "df_no_nulo.createOrReplaceTempView(\"dfNoNulo\")\n",
    "\n",
    "print(\"Mostra de dfNoNulo (sen nulos en CustomerId e StockCode):\")\n",
    "df_no_nulo.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9cbc1-8e39-46be-b3a3-6e45f21d19a4",
   "metadata": {},
   "source": [
    "### Agrupación estándar (sen grouping sets)\n",
    "\n",
    "Nesta consulta agrúpase por `CustomerId` e `StockCode` e calcúlase a suma de `Quantity`. O resultado contén unha fila por combinación (CustomerId, StockCode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27939854-8d43-40d7-84e6-30214e69b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de Quantity por (CustomerId, StockCode) usando GROUP BY estándar:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|CustomerId|StockCode|total_quantity|\n",
      "+----------+---------+--------------+\n",
      "|     18287|    85173|            48|\n",
      "|     18287|   85040A|            48|\n",
      "|     18287|   85039B|           120|\n",
      "|     18287|   85039A|            96|\n",
      "|     18287|    84920|             4|\n",
      "|     18287|    84584|             6|\n",
      "|     18287|   84507C|             6|\n",
      "|     18287|   72351B|            24|\n",
      "|     18287|   72351A|            24|\n",
      "|     18287|   72349B|            60|\n",
      "|     18287|    47422|            24|\n",
      "|     18287|    47421|            48|\n",
      "|     18287|    35967|            36|\n",
      "|     18287|    23445|            20|\n",
      "|     18287|    23378|            24|\n",
      "|     18287|    23376|            48|\n",
      "|     18287|    23310|            36|\n",
      "|     18287|    23274|            12|\n",
      "|     18287|    23272|            12|\n",
      "|     18287|    23269|            36|\n",
      "+----------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: GROUP BY estándar por CustomerId e StockCode\n",
    "\n",
    "print(\"Suma de Quantity por (CustomerId, StockCode) usando GROUP BY estándar:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity) AS total_quantity\n",
    "    FROM dfNoNulo\n",
    "    GROUP BY CustomerId, StockCode\n",
    "    ORDER BY CustomerId DESC, StockCode DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db7a8b-08f6-49c0-97d2-98ea9ac943f5",
   "metadata": {},
   "source": [
    "### O mesmo empregando `GROUPING SETS`\n",
    "\n",
    "Se se usa `GROUPING SETS((CustomerId, StockCode))` o resultado é equivalente ao `GROUP BY` estándar anterior. Isto serve para entender que `GROUPING SETS` permite especificar explicitamente cal é o conxunto (ou conxuntos) de agrupación que se queren combinar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f91053-e649-462c-a7cc-a68d30c9352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O mesmo resultado usando GROUPING SETS((CustomerId, StockCode)):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|CustomerId|StockCode|total_quantity|\n",
      "+----------+---------+--------------+\n",
      "|     13256|    84826|         12540|\n",
      "|     17949|    22197|         11692|\n",
      "|     16333|    84077|         10080|\n",
      "|     16422|    17003|         10077|\n",
      "|     16333|    21915|          8120|\n",
      "|     16308|    16014|          8000|\n",
      "|     17306|    22616|          6624|\n",
      "|     18102|    22189|          5946|\n",
      "|     12901|    84077|          5712|\n",
      "|     14609|    18007|          5586|\n",
      "|     12931|    22197|          5340|\n",
      "|     17450|    22469|          5286|\n",
      "|     12931|    84879|          5048|\n",
      "|     14646|    23084|          4801|\n",
      "|     16029|    22693|          4800|\n",
      "|     16210|    21137|          4728|\n",
      "|     17381|    22616|          4704|\n",
      "|     15769|   85099B|          4700|\n",
      "|     12901|    21787|          4632|\n",
      "|     14646|    22629|          4492|\n",
      "+----------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: GROUPING SETS cun único conxunto (equivalente ao GROUP BY normal)\n",
    "\n",
    "print(\"O mesmo resultado usando GROUPING SETS((CustomerId, StockCode)):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity) AS total_quantity\n",
    "    FROM dfNoNulo\n",
    "    GROUP BY CustomerId, StockCode\n",
    "    GROUPING SETS ((CustomerId, StockCode))\n",
    "    ORDER BY total_quantity DESC, CustomerId DESC, StockCode DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addcd88-8856-4548-aa6a-c28c43684945",
   "metadata": {},
   "source": [
    "### Engadir o total xeral con `GROUPING SETS((CustomerId, StockCode), ())`\n",
    "\n",
    "Se se quere obter tamén o **total xeral** (sen agrupar por ningunha columna), engádese o conxunto baleiro `()` dentro de `GROUPING SETS`.\n",
    "\n",
    "Isto fai que no resultado apareza unha fila adicional co total de `Quantity` de todo o DataFrame. Nesa fila, as columnas `CustomerId` e `StockCode` aparecen como `NULL`, xa que non forman parte do conxunto de agrupación dese nivel.\n",
    "\n",
    "Este tipo de resultado (agregación por combinación + total xeral na mesma consulta) é precisamente unha das utilidades principais dos grouping sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634f7c67-bd84-4420-97d6-30b163289e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma por (CustomerId, StockCode) e total xeral usando GROUPING SETS((...), ()):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|CustomerId|StockCode|total_quantity|\n",
      "+----------+---------+--------------+\n",
      "|      NULL|     NULL|       4906888|\n",
      "|     13256|    84826|         12540|\n",
      "|     17949|    22197|         11692|\n",
      "|     16333|    84077|         10080|\n",
      "|     16422|    17003|         10077|\n",
      "|     16333|    21915|          8120|\n",
      "|     16308|    16014|          8000|\n",
      "|     17306|    22616|          6624|\n",
      "|     18102|    22189|          5946|\n",
      "|     12901|    84077|          5712|\n",
      "|     14609|    18007|          5586|\n",
      "|     12931|    22197|          5340|\n",
      "|     17450|    22469|          5286|\n",
      "|     12931|    84879|          5048|\n",
      "|     14646|    23084|          4801|\n",
      "|     16029|    22693|          4800|\n",
      "|     16210|    21137|          4728|\n",
      "|     17381|    22616|          4704|\n",
      "|     15769|   85099B|          4700|\n",
      "|     12901|    21787|          4632|\n",
      "+----------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: GROUPING SETS para obter por (CustomerId, StockCode) e tamén o total xeral\n",
    "\n",
    "print(\"Suma por (CustomerId, StockCode) e total xeral usando GROUPING SETS((...), ()):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity) AS total_quantity\n",
    "    FROM dfNoNulo\n",
    "    GROUP BY CustomerId, StockCode\n",
    "    GROUPING SETS ((CustomerId, StockCode), ())\n",
    "    ORDER BY total_quantity DESC, CustomerId DESC, StockCode DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711aad1-9e1f-4a30-b88f-02c8fe919c1a",
   "metadata": {},
   "source": [
    "Cando se usan `GROUPING SETS`, `ROLLUP` ou `CUBE`, aparecen valores `NULL` que representan subtotais.  \n",
    "Para poder **identificar explicitamente** que tipo de fila é cada unha, Spark ofrece a función `grouping()`.\n",
    "\n",
    "- `grouping(col) = 0` → a columna participa na agrupación\n",
    "- `grouping(col) = 1` → a columna é un subtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce08180-69ca-4735-bf1d-c58bc9d87a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping sets con indicadores de subtotal:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+----------+-------+\n",
      "|CustomerId|StockCode|total_quantity|g_customer|g_stock|\n",
      "+----------+---------+--------------+----------+-------+\n",
      "|     15358|    22935|            12|         0|      0|\n",
      "|     14646|    22624|            81|         0|      0|\n",
      "|     13873|    23243|             4|         0|      0|\n",
      "|     13873|    21391|            12|         0|      0|\n",
      "|     14534|    22470|             3|         0|      0|\n",
      "|     18237|    23298|            15|         0|      0|\n",
      "|     12431|    23268|            12|         0|      0|\n",
      "|     17049|    22355|            30|         0|      0|\n",
      "|     16115|    22993|            27|         0|      0|\n",
      "|     13799|    22522|            12|         0|      0|\n",
      "|     12656|    22899|            50|         0|      0|\n",
      "|     14498|   84030E|             1|         0|      0|\n",
      "|     15023|    23199|             1|         0|      0|\n",
      "|     16133|    22497|             4|         0|      0|\n",
      "|     14918|   84926E|            48|         0|      0|\n",
      "|     14232|    16237|            60|         0|      0|\n",
      "|     17637|    84879|             8|         0|      0|\n",
      "|     17097|    23255|             1|         0|      0|\n",
      "|     12473|    22178|            12|         0|      0|\n",
      "|     14194|    22383|            60|         0|      0|\n",
      "+----------+---------+--------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Identificar subtotais nun GROUPING SETS usando grouping()\n",
    "\n",
    "from pyspark.sql.functions import grouping\n",
    "\n",
    "resultado = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity) AS total_quantity,\n",
    "        grouping(CustomerId) AS g_customer,\n",
    "        grouping(StockCode) AS g_stock\n",
    "    FROM dfNoNulo\n",
    "    GROUP BY CustomerId, StockCode\n",
    "    GROUPING SETS ((CustomerId, StockCode), ())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Grouping sets con indicadores de subtotal:\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6940b-9176-4172-b4f5-1348b7d9ab30",
   "metadata": {},
   "source": [
    "## Rollups\n",
    "\n",
    "Un **rollup** é unha agregación multidimensional que realiza varios cálculos tipo `groupBy` nunha única operación. Permite obter subtotais xerárquicos seguindo a orde das columnas indicadas.\n",
    "\n",
    "Se se executa:\n",
    "`rollup(\"date\", \"Country\")`\n",
    "\n",
    "Spark xera agregacións para estes niveis:\n",
    "- (`date`, `Country`) → detalle por data e país\n",
    "- (`date`) → subtotal por data (todos os países)\n",
    "- (`)`) → total xeral (todas as datas e todos os países)\n",
    "\n",
    "No resultado aparecen valores `NULL` nas columnas de agrupación para representar eses subtotais:\n",
    "- `Country = NULL` indica “subtotal por date”\n",
    "- `date = NULL` indica “total xeral”\n",
    "- se `date = NULL` e `Country = NULL`, trátase do total para todo o DataFrame\n",
    "\n",
    "A continuación móstrase un exemplo calculando a suma de `Quantity`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0de5043-77f4-45ae-9002-e5cc7c1da511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema de df_no_nulo:\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "Rollup por (date, Country) coa suma de Quantity (inclúe subtotais e total xeral):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      NULL|          NULL|       4906888|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-01|          NULL|         24032|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-02|United Kingdom|         20705|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|          NULL|         20855|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|          NULL|         11548|\n",
      "|2010-12-03|        Poland|           140|\n",
      "|2010-12-03|          EIRE|          2375|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un rollup por date e Country e calcular a suma de Quantity\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Amosamos o esquema para confirmar que existen as columnas date, Country e Quantity\n",
    "print(\"Esquema de df_no_nulo:\")\n",
    "df_no_nulo.printSchema()\n",
    "\n",
    "# Aplicamos rollup(\"date\", \"Country\") e calculamos a suma de Quantity\n",
    "# Renomeamos a columna agregada a total_quantity e ordenamos por date para facilitar a lectura\n",
    "df_enroscado = (\n",
    "    df_no_nulo.rollup(\"date\", \"Country\")\n",
    "        .agg(sum(\"Quantity\"))\n",
    "        .selectExpr(\n",
    "            \"date\",\n",
    "            \"Country\",\n",
    "            \"`sum(Quantity)` as total_quantity\"\n",
    "        )\n",
    "        .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Rollup por (date, Country) coa suma de Quantity (inclúe subtotais e total xeral):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "df_enroscado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5522ae-dd02-4b6f-8e7c-5aadcf6047e9",
   "metadata": {},
   "source": [
    "Os valores `NULL` nas columnas de agrupación representan subtotais.\n",
    "\n",
    "- Cando `Country` é `NULL`, a fila representa o subtotal por `date` (sumando todos os países dese día).\n",
    "- Cando `date` é `NULL`, a fila representa o total xeral (todas as datas).\n",
    "- Cando `date` e `Country` son `NULL`, é o total xeral para todo o DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09b1850-b8cf-4dd1-bf5e-3c201754b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas do rollup onde Country é NULL (subtotal por date):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      NULL|   NULL|       4906888|\n",
      "|2010-12-01|   NULL|         24032|\n",
      "|2010-12-02|   NULL|         20855|\n",
      "|2010-12-03|   NULL|         11548|\n",
      "|2010-12-05|   NULL|         16394|\n",
      "|2010-12-06|   NULL|         16095|\n",
      "|2010-12-07|   NULL|         19351|\n",
      "|2010-12-08|   NULL|         21275|\n",
      "|2010-12-09|   NULL|         16904|\n",
      "|2010-12-10|   NULL|         15388|\n",
      "|2010-12-12|   NULL|         10561|\n",
      "|2010-12-13|   NULL|         15234|\n",
      "|2010-12-14|   NULL|         17108|\n",
      "|2010-12-15|   NULL|         18169|\n",
      "|2010-12-16|   NULL|         29482|\n",
      "|2010-12-17|   NULL|         10517|\n",
      "|2010-12-19|   NULL|          3735|\n",
      "|2010-12-20|   NULL|         12617|\n",
      "|2010-12-21|   NULL|         10888|\n",
      "|2010-12-22|   NULL|          3053|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Ver as filas onde Country é NULL (subtotais por date)\n",
    "\n",
    "resultado = df_enroscado.where(\"Country IS NULL\")\n",
    "\n",
    "print(\"Filas do rollup onde Country é NULL (subtotal por date):\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdbbb030-d69f-4ebd-baf6-422975a7e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas do rollup onde date é NULL (total xeral):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|NULL|   NULL|       4906888|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Ver as filas onde date é NULL (total xeral)\n",
    "\n",
    "resultado = df_enroscado.where(\"date IS NULL\")\n",
    "\n",
    "print(\"Filas do rollup onde date é NULL (total xeral):\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1fc55-5674-4509-9068-69c2584be73c",
   "metadata": {},
   "source": [
    "## Cubo (`cube`)\n",
    "\n",
    "Un **cubo** (*cube*) é un paso máis respecto ao `rollup`. Mentres que o `rollup` calcula subtotais seguindo unha orde xerárquica das columnas, o `cube` calcula agregacións para **todas as combinacións posibles** das dimensións indicadas.\n",
    "\n",
    "Se se executa:\n",
    "`cube(\"date\", \"Country\")`\n",
    "\n",
    "Spark xera agregacións para estes niveis:\n",
    "- (`date`, `Country`) → detalle por data e país\n",
    "- (`date`) → subtotal por data (todos os países)\n",
    "- (`Country`) → subtotal por país (todas as datas)\n",
    "- (`)`) → total xeral (todas as datas e todos os países)\n",
    "\n",
    "No resultado aparecen valores `NULL` nas columnas de agrupación para representar os subtotais:\n",
    "- `Country = NULL` indica “subtotal por date”\n",
    "- `date = NULL` indica “subtotal por Country”\n",
    "- `date = NULL` e `Country = NULL` indica o total xeral\n",
    "\n",
    "Este tipo de operación é útil para análises multidimensionais, semellante a un escenario de OLAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "845631b0-bef9-41ac-a265-2d82920ad26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube por (date, Country) coa suma de Quantity (inclúe todos os subtotais posibles):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "|date|             Country|total_quantity|\n",
      "+----+--------------------+--------------+\n",
      "|NULL|              France|        109848|\n",
      "|NULL|         Unspecified|          1789|\n",
      "|NULL|               Italy|          7999|\n",
      "|NULL|              Poland|          3653|\n",
      "|NULL|               Japan|         25218|\n",
      "|NULL|               Malta|           944|\n",
      "|NULL|      United Kingdom|       4008533|\n",
      "|NULL|             Denmark|          8188|\n",
      "|NULL|                NULL|       4906888|\n",
      "|NULL|           Singapore|          5234|\n",
      "|NULL|                 RSA|           352|\n",
      "|NULL|             Germany|        117448|\n",
      "|NULL|              Cyprus|          6317|\n",
      "|NULL|             Finland|         10666|\n",
      "|NULL|             Austria|          4827|\n",
      "|NULL|United Arab Emirates|           982|\n",
      "|NULL|           Australia|         83653|\n",
      "|NULL|              Sweden|         35637|\n",
      "|NULL|              Norway|         19247|\n",
      "|NULL|         Switzerland|         29778|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Crear un cubo por date e Country e calcular a suma de Quantity\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Aplicamos cube(\"date\", \"Country\") e calculamos a suma de Quantity\n",
    "# Renomeamos a columna agregada e ordenamos por date para facilitar a lectura\n",
    "df_cubo = (\n",
    "    df_no_nulo.cube(\"date\", \"Country\")\n",
    "        .agg(sum(col(\"Quantity\")).alias(\"total_quantity\"))\n",
    "        .select(\"date\", \"Country\", \"total_quantity\")\n",
    "        .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Cube por (date, Country) coa suma de Quantity (inclúe todos os subtotais posibles):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "df_cubo.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fb484-e98b-4996-bec7-c2bce7aa4097",
   "metadata": {},
   "source": [
    "As filas con valores `NULL` representan subtotais:\n",
    "\n",
    "- `date = NULL` → subtotal por Country (todas as datas)\n",
    "- `Country = NULL` → subtotal por date (todos os países)\n",
    "- `date = NULL` e `Country = NULL` → total xeral\n",
    "\n",
    "A continuación amósanse as filas nas que `date` é `NULL`, que corresponden aos subtotais por país e ao total xeral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c01d8ea-f1f5-4d29-a3c4-2de39d3236c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas do cubo onde date é NULL (subtotal por Country e total xeral):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "|date|             Country|total_quantity|\n",
      "+----+--------------------+--------------+\n",
      "|NULL|             Finland|         10666|\n",
      "|NULL|              Poland|          3653|\n",
      "|NULL|             Iceland|          2458|\n",
      "|NULL|               Malta|           944|\n",
      "|NULL|United Arab Emirates|           982|\n",
      "|NULL|           Singapore|          5234|\n",
      "|NULL|              Greece|          1556|\n",
      "|NULL|             Germany|        117448|\n",
      "|NULL|                 USA|          1034|\n",
      "|NULL|              France|        109848|\n",
      "|NULL|               Italy|          7999|\n",
      "|NULL|               Japan|         25218|\n",
      "|NULL|      United Kingdom|       4008533|\n",
      "|NULL|      Czech Republic|           592|\n",
      "|NULL|              Sweden|         35637|\n",
      "|NULL|              Canada|          2763|\n",
      "|NULL|             Denmark|          8188|\n",
      "|NULL|                NULL|       4906888|\n",
      "|NULL|                 RSA|           352|\n",
      "|NULL|              Cyprus|          6317|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Ver as filas onde date é NULL (subtotais por Country e total xeral)\n",
    "\n",
    "resultado = df_cubo.where(\"date IS NULL\")\n",
    "\n",
    "print(\"Filas do cubo onde date é NULL (subtotal por Country e total xeral):\")\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca449c6f-4966-4924-9809-81f2d8ff8984",
   "metadata": {},
   "source": [
    "## Pivote (`pivot`)\n",
    "\n",
    "Pivotar permite transformar valores dunha columna en **novas columnas**. É dicir, unha columna categórica (por exemplo `Country`) convértese en múltiples columnas, unha por cada valor posible.\n",
    "\n",
    "No DataFrame de exemplo:\n",
    "- agrúpase por `date`\n",
    "- pívtase por `Country`\n",
    "- e calcúlase unha agregación (por exemplo `sum`) para cada país\n",
    "\n",
    "O resultado é un DataFrame onde:\n",
    "- hai unha fila por `date`\n",
    "- e unha columna por país (ou por cada país + métrica agregada)\n",
    "- os valores son os resultados da agregación\n",
    "\n",
    "Este tipo de operación é útil para:\n",
    "- crear táboas tipo “matriz” (datas en filas, países en columnas)\n",
    "- comparar facilmente categorías\n",
    "- preparar datos para gráficos ou informes\n",
    "\n",
    "É importante ter en conta que:\n",
    "- se hai moitos países distintos, o número de columnas pode medrar moito\n",
    "- por rendemento, é habitual pivotar só sobre un subconxunto de valores cando hai alta cardinalidade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e02d183-a770-40e6-9c9b-ebce63b4e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema do DataFrame pivotado (unha columna por país e métrica):\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- Australia_sum(Quantity): long (nullable = true)\n",
      " |-- Australia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Australia_sum(CustomerID): long (nullable = true)\n",
      " |-- Austria_sum(Quantity): long (nullable = true)\n",
      " |-- Austria_sum(UnitPrice): double (nullable = true)\n",
      " |-- Austria_sum(CustomerID): long (nullable = true)\n",
      " |-- Bahrain_sum(Quantity): long (nullable = true)\n",
      " |-- Bahrain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Bahrain_sum(CustomerID): long (nullable = true)\n",
      " |-- Belgium_sum(Quantity): long (nullable = true)\n",
      " |-- Belgium_sum(UnitPrice): double (nullable = true)\n",
      " |-- Belgium_sum(CustomerID): long (nullable = true)\n",
      " |-- Brazil_sum(Quantity): long (nullable = true)\n",
      " |-- Brazil_sum(UnitPrice): double (nullable = true)\n",
      " |-- Brazil_sum(CustomerID): long (nullable = true)\n",
      " |-- Canada_sum(Quantity): long (nullable = true)\n",
      " |-- Canada_sum(UnitPrice): double (nullable = true)\n",
      " |-- Canada_sum(CustomerID): long (nullable = true)\n",
      " |-- Channel Islands_sum(Quantity): long (nullable = true)\n",
      " |-- Channel Islands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Channel Islands_sum(CustomerID): long (nullable = true)\n",
      " |-- Cyprus_sum(Quantity): long (nullable = true)\n",
      " |-- Cyprus_sum(UnitPrice): double (nullable = true)\n",
      " |-- Cyprus_sum(CustomerID): long (nullable = true)\n",
      " |-- Czech Republic_sum(Quantity): long (nullable = true)\n",
      " |-- Czech Republic_sum(UnitPrice): double (nullable = true)\n",
      " |-- Czech Republic_sum(CustomerID): long (nullable = true)\n",
      " |-- Denmark_sum(Quantity): long (nullable = true)\n",
      " |-- Denmark_sum(UnitPrice): double (nullable = true)\n",
      " |-- Denmark_sum(CustomerID): long (nullable = true)\n",
      " |-- EIRE_sum(Quantity): long (nullable = true)\n",
      " |-- EIRE_sum(UnitPrice): double (nullable = true)\n",
      " |-- EIRE_sum(CustomerID): long (nullable = true)\n",
      " |-- European Community_sum(Quantity): long (nullable = true)\n",
      " |-- European Community_sum(UnitPrice): double (nullable = true)\n",
      " |-- European Community_sum(CustomerID): long (nullable = true)\n",
      " |-- Finland_sum(Quantity): long (nullable = true)\n",
      " |-- Finland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Finland_sum(CustomerID): long (nullable = true)\n",
      " |-- France_sum(Quantity): long (nullable = true)\n",
      " |-- France_sum(UnitPrice): double (nullable = true)\n",
      " |-- France_sum(CustomerID): long (nullable = true)\n",
      " |-- Germany_sum(Quantity): long (nullable = true)\n",
      " |-- Germany_sum(UnitPrice): double (nullable = true)\n",
      " |-- Germany_sum(CustomerID): long (nullable = true)\n",
      " |-- Greece_sum(Quantity): long (nullable = true)\n",
      " |-- Greece_sum(UnitPrice): double (nullable = true)\n",
      " |-- Greece_sum(CustomerID): long (nullable = true)\n",
      " |-- Hong Kong_sum(Quantity): long (nullable = true)\n",
      " |-- Hong Kong_sum(UnitPrice): double (nullable = true)\n",
      " |-- Hong Kong_sum(CustomerID): long (nullable = true)\n",
      " |-- Iceland_sum(Quantity): long (nullable = true)\n",
      " |-- Iceland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Iceland_sum(CustomerID): long (nullable = true)\n",
      " |-- Israel_sum(Quantity): long (nullable = true)\n",
      " |-- Israel_sum(UnitPrice): double (nullable = true)\n",
      " |-- Israel_sum(CustomerID): long (nullable = true)\n",
      " |-- Italy_sum(Quantity): long (nullable = true)\n",
      " |-- Italy_sum(UnitPrice): double (nullable = true)\n",
      " |-- Italy_sum(CustomerID): long (nullable = true)\n",
      " |-- Japan_sum(Quantity): long (nullable = true)\n",
      " |-- Japan_sum(UnitPrice): double (nullable = true)\n",
      " |-- Japan_sum(CustomerID): long (nullable = true)\n",
      " |-- Lebanon_sum(Quantity): long (nullable = true)\n",
      " |-- Lebanon_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lebanon_sum(CustomerID): long (nullable = true)\n",
      " |-- Lithuania_sum(Quantity): long (nullable = true)\n",
      " |-- Lithuania_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lithuania_sum(CustomerID): long (nullable = true)\n",
      " |-- Malta_sum(Quantity): long (nullable = true)\n",
      " |-- Malta_sum(UnitPrice): double (nullable = true)\n",
      " |-- Malta_sum(CustomerID): long (nullable = true)\n",
      " |-- Netherlands_sum(Quantity): long (nullable = true)\n",
      " |-- Netherlands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Netherlands_sum(CustomerID): long (nullable = true)\n",
      " |-- Norway_sum(Quantity): long (nullable = true)\n",
      " |-- Norway_sum(UnitPrice): double (nullable = true)\n",
      " |-- Norway_sum(CustomerID): long (nullable = true)\n",
      " |-- Poland_sum(Quantity): long (nullable = true)\n",
      " |-- Poland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Poland_sum(CustomerID): long (nullable = true)\n",
      " |-- Portugal_sum(Quantity): long (nullable = true)\n",
      " |-- Portugal_sum(UnitPrice): double (nullable = true)\n",
      " |-- Portugal_sum(CustomerID): long (nullable = true)\n",
      " |-- RSA_sum(Quantity): long (nullable = true)\n",
      " |-- RSA_sum(UnitPrice): double (nullable = true)\n",
      " |-- RSA_sum(CustomerID): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(Quantity): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Saudi Arabia_sum(CustomerID): long (nullable = true)\n",
      " |-- Singapore_sum(Quantity): long (nullable = true)\n",
      " |-- Singapore_sum(UnitPrice): double (nullable = true)\n",
      " |-- Singapore_sum(CustomerID): long (nullable = true)\n",
      " |-- Spain_sum(Quantity): long (nullable = true)\n",
      " |-- Spain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Spain_sum(CustomerID): long (nullable = true)\n",
      " |-- Sweden_sum(Quantity): long (nullable = true)\n",
      " |-- Sweden_sum(UnitPrice): double (nullable = true)\n",
      " |-- Sweden_sum(CustomerID): long (nullable = true)\n",
      " |-- Switzerland_sum(Quantity): long (nullable = true)\n",
      " |-- Switzerland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Switzerland_sum(CustomerID): long (nullable = true)\n",
      " |-- USA_sum(Quantity): long (nullable = true)\n",
      " |-- USA_sum(UnitPrice): double (nullable = true)\n",
      " |-- USA_sum(CustomerID): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(Quantity): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Arab Emirates_sum(CustomerID): long (nullable = true)\n",
      " |-- United Kingdom_sum(Quantity): long (nullable = true)\n",
      " |-- United Kingdom_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Kingdom_sum(CustomerID): long (nullable = true)\n",
      " |-- Unspecified_sum(Quantity): long (nullable = true)\n",
      " |-- Unspecified_sum(UnitPrice): double (nullable = true)\n",
      " |-- Unspecified_sum(CustomerID): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Pivotar por Country e sumar todas as columnas numéricas por cada date\n",
    "\n",
    "# Agrupamos por date, pivotamos por Country e aplicamos sum() ás columnas numéricas\n",
    "pivotado = df_con_fecha.groupBy(\"date\").pivot(\"Country\").sum()\n",
    "\n",
    "# Amosamos o esquema para ver as columnas xeradas polo pivote\n",
    "print(\"Esquema do DataFrame pivotado (unha columna por país e métrica):\")\n",
    "pivotado.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf52fa6-fde7-43f2-8073-72a0d4032ead",
   "metadata": {},
   "source": [
    "O DataFrame pivotado xera columnas para cada país e para cada medida numérica agregada.\n",
    "\n",
    "Por exemplo, poden aparecer columnas do estilo:\n",
    "- `USA_sum(Quantity)`\n",
    "- `USA_sum(UnitPrice)`\n",
    "- `France_sum(Quantity)`\n",
    "- etc.\n",
    "\n",
    "Como os nomes levan parénteses, en `select()` é necesario empregar comiñas invertidas (backticks) para referenciar a columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e497fe96-a9b0-44f9-b034-c9913693aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de Quantity para USA por date (despois do pivote):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             NULL|\n",
      "|2011-12-09|             NULL|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             NULL|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Consultar unha columna concreta do pivote para un país (USA) e unha métrica (sum(Quantity))\n",
    "\n",
    "# Filtramos por datas e seleccionamos a columna agregada de USA\n",
    "resultado = (\n",
    "    pivotado.where(\"date > '2011-12-05'\")\n",
    "            .select(\"date\", \"`USA_sum(Quantity)`\")\n",
    ")\n",
    "\n",
    "# Imprimimos a descrición do resultado\n",
    "print(\"Suma de Quantity para USA por date (despois do pivote):\")\n",
    "\n",
    "# Amosamos o resultado\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed4eeb-1087-4068-bec4-d47c483ee06e",
   "metadata": {},
   "source": [
    "Un dos principais problemas do uso de `pivot()` é a **alta cardinalidade** da columna pivotada.  \n",
    "Se unha columna ten moitos valores distintos (por exemplo, centos de países), o resultado terá centos de columnas, o que pode:\n",
    "\n",
    "- aumentar moito o consumo de memoria\n",
    "- degradar o rendemento\n",
    "- xerar DataFrames difíciles de manexar\n",
    "\n",
    "Por este motivo, é habitual **limitar explicitamente os valores do pivote** e **controlar os valores nulos** resultantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b9128f3-3227-4292-a245-aaaf4e2685ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema do pivote limitado a certos países:\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- United Kingdom: long (nullable = true)\n",
      " |-- France: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "Mostra do pivote limitado con nulos substituídos por 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+---+\n",
      "|      date|United Kingdom|France|USA|\n",
      "+----------+--------------+------+---+\n",
      "|2011-01-27|          9171|    -9|  0|\n",
      "|2011-01-23|          5068|   126|  0|\n",
      "|2011-01-25|         11873|   -15|  0|\n",
      "|2011-08-16|         11052|     0|  0|\n",
      "|2011-06-26|          3023|     0|  0|\n",
      "+----------+--------------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Exemplo: Pivotar só para un subconxunto de países e controlar nulos\n",
    "\n",
    "# Definimos explicitamente os países que nos interesan\n",
    "paises = [\"United Kingdom\", \"France\", \"USA\"]\n",
    "\n",
    "# Pivotamos só para eses países e calculamos a suma de Quantity\n",
    "pivotado_limitado = (\n",
    "    df_con_fecha.groupBy(\"date\")\n",
    "        .pivot(\"Country\", paises)\n",
    "        .sum(\"Quantity\")\n",
    ")\n",
    "\n",
    "print(\"Esquema do pivote limitado a certos países:\")\n",
    "pivotado_limitado.printSchema()\n",
    "\n",
    "# Enchemos os valores nulos con 0 (cando nunha data non hai rexistros dese país)\n",
    "pivotado_limitado = pivotado_limitado.fillna(0)\n",
    "\n",
    "print(\"Mostra do pivote limitado con nulos substituídos por 0:\")\n",
    "pivotado_limitado.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809df9e-9b95-4b01-9156-5d2ad74e8b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
