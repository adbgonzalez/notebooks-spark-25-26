{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e373dd0-9a41-4742-98f8-5842779d7d74",
   "metadata": {},
   "source": [
    "# Uso avanzado de RDDs\n",
    "\n",
    "En Big Data √© habitual traballar con datos en formato **clave‚Äìvalor**.  \n",
    "Por este motivo, Spark ofrece transformaci√≥ns e acci√≥ns espec√≠ficas dese√±adas para operar eficientemente con este tipo de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73569b94-4023-42c8-a0db-ce766fbcbad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1583e940-cd56-479d-8587-2537b6c2d9e7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      ":: resolution report :: resolve 3426ms :: artifacts dl 2393ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   3   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1583e940-cd56-479d-8587-2537b6c2d9e7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/115ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.7\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos SparkSession y SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"02-rdd2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.version)  # Verifica la versi√≥n de Spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d384bb-3a41-49b8-ac78-1cb895f85637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: creamos RDD de pares a partir de lista de tuplas\n",
    "rdd_pares1 = sc.parallelize([('a', 1), ('b', 1), ('c', 1)])\n",
    "print (\"RDD de pares 1: \",rdd_pares1.collect())\n",
    "\n",
    "# Exemplo 2: creamos RDD de pares a partir dun RDD simple con map\n",
    "rdd_st = sc.parallelize (\"Big Data aplicado. Curso de especializaci√≥n de Inteligencia Artificial y Big Data\".split())\n",
    "rdd_pares2 = rdd_st.map(lambda palabra: (palabra,1))\n",
    "print (\"RDD de pares 2: \",rdd_pares2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543059a-7a60-4631-8f5a-b0980aa44ebc",
   "metadata": {},
   "source": [
    "> O interesante dos RDD's de pares clave-valor √© que presentan unha serie de transformaci√≥ns e acci√≥ns adicionais.\n",
    "\n",
    "## Operaci√≥ns clave-valor b√°sicas\n",
    "### keyBy\n",
    "Transformaci√≥n que converte un RDD de elementos nun RDD de pares (clave, valor), xerando a clave a partir de cada elemento mediante unha funci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220c006-861a-4f88-89b9-3dde91fe0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: A clave √© a inicial de cada palabra\n",
    "rdd_pares = rdd_st.keyBy(lambda palabra: palabra[0])\n",
    "print(\"Palabras por inicial: \",rdd_pares.collect())\n",
    "\n",
    "# Exemplo 2: A clave √© a lonxitude da palabra\n",
    "rdd_pares2 = rdd_st.keyBy (lambda palabra: len(palabra))\n",
    "print(\"Palabras por lonxitude: \",rdd_pares2.collect())\n",
    "\n",
    "# Exemplo 3: A clave √© o feito de se o elemento √© par ou non\n",
    "rdd_int = sc.parallelize(range(10))\n",
    "rdd_pares3 = rdd_int.keyBy(lambda x: x%2 )\n",
    "print (\"N√∫meros por ser par ou non: \", rdd_pares3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d596-d983-4670-a440-d5bab54100ed",
   "metadata": {},
   "source": [
    "`keyBy` √© moi √∫til para preparar RDD's para as seguintes operaci√≥ns:\n",
    "- `groupByKey`\n",
    "- `reduceByKey`\n",
    "- `countByKey`\n",
    "- `join`\n",
    "- `partitionBy`\n",
    "\n",
    "Xa que estas operaci√≥ns s√≥ funcionan con RDD's tipo (k,v).\n",
    "\n",
    "### mapValues\n",
    "Realiza una operaci√≥n map s√≥ sobre os valores do RDD, deixando a clave tal como est√°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce58c8-7b65-4eac-9a41-6783e119575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: pasomos o valores a mai√∫sculas, sen tocar a clave\n",
    "print ( \"Palabras por inicial en mai√∫sculas: \", rdd_pares.mapValues( lambda x: x.upper()).collect())\n",
    "\n",
    "# Exemplo 2: Operaci√≥n matem√°tica sobre os valores sen modificar as claves\n",
    "rdd_x10 = rdd_pares3.mapValues(lambda x: x*10)\n",
    "print (\"N√∫meros multiplicados por 10: \", rdd_x10.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e4545-c8a4-49b4-8e97-fa176928a4ef",
   "metadata": {},
   "source": [
    "`mapValues` √© util cando:\n",
    "- Temos unha clave que nos interesa conservar.\n",
    "- Queremos facer operaci√≥ns sobre os valores.\n",
    "\n",
    "**Exemplos reais**:\n",
    "- Despois dun `reduceByKey`, para escalar ou normalizar os resultados.\n",
    "- Trasformar listas, estat√≠sticas ou estruturas asociadas a unha clave.\n",
    "- Prepara datos para un join mantendo unha clave com√∫n.\n",
    "\n",
    "**Diferencia con map**\n",
    "- A funci√≥n especificada en `map` apl√≠case tanto √° clave como ao valor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f116a55-a84b-465b-8b8d-3b7dfe0e8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: pasamos todo a mai√∫sculas\n",
    "print ( \"Palabras por inicial en mai√∫sculas: \", rdd_pares.map( lambda par: (par[0].upper(), par[1].upper())).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a0219-5620-4ab2-b7d8-6e1725dee38f",
   "metadata": {},
   "source": [
    "### keys\n",
    "Transformaci√≥n que devolve un RDD formado s√≥ polas claves dun RDD de pares (K, V), eliminando os valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ae619-26c8-4c88-9053-1d0acf6d719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Creamos un RDD s√≥ coas claves\n",
    "print(\"RDD de claves: \", rdd_pares.keys().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea828e-7a25-4081-8e68-079fb625dafe",
   "metadata": {},
   "source": [
    "`keys()` √© √∫til cando queres traballar s√≥ coas claves, por exemplo:\n",
    "- ver que tipos de claves hai\n",
    "- facer un `distinct()` para obter claves √∫nicas\n",
    "- contar cantas claves hai ou analizar a s√∫a distribuci√≥n\n",
    "- cruzalo con outro RDD ou facer operaci√≥ns de filtrado\n",
    "\n",
    "En realidade, p√≥dese facer unha transformaci√≥n equivalente empregando `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f120c-4ac4-499a-a6f3-db46ca1c0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"S√≥ claves (map): \", rdd_pares.map(lambda par: par[0]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c8e15-60ea-4f41-9ac8-dd3aae48719d",
   "metadata": {},
   "source": [
    "### values\n",
    "Transformaci√≥n que devolve un RDD formado s√≥ polos valores dun RDD de pares (K, V), eliminando as claves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c763a91-3cfb-4958-babf-e5ae289272b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Creamos un RDD formado s√≥ polos valores\n",
    "print(\"RDD de valores: \",rdd_pares.values().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e177f58-f41d-4d1e-a5b4-34a91611b2de",
   "metadata": {},
   "source": [
    "`values` √© √∫til cando:\n",
    "- s√≥ interesa o dato asociado √° clave.\n",
    "- qu√©rense facer operaci√≥ns estat√≠sticas sobre os valores.\n",
    "\n",
    "En realidade, ao igual que con `keys`, p√≥dese facer unha transformaci√≥n equivalente empregando `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e2503-f23e-42f5-91d1-822ab38414d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: qudearse s√≥ cos valores mediante map\n",
    "print(\"S√≥ valores (map): \", rdd_pares.map(lambda par: par[1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88567cbb-286c-4881-ae1e-fa0baa319b5c",
   "metadata": {},
   "source": [
    "### lookup\n",
    "`lookup()` √© unha acci√≥n dispo√±ible s√≥ para **Pair RDDs** (RDDs de pares `(K, V)`), que permite recuperar todos os valores asociados a unha clave concreta.\n",
    "\n",
    "- **Entrada:** un `RDD[(K, V)]`\n",
    "- **Sa√≠da:** unha lista de valores `List[V]` (en PySpark, unha lista Python)\n",
    "- **Comportamento:** devolve *t√≥dolos valores* que te√±an exactamente esa clave (poden ser varios)\n",
    "\n",
    "**Transformaci√≥n conceptual:**\n",
    "\n",
    "`RDD[(K, V)] ‚Üí lookup(k) ‚Üí List[V]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf7e0d-7a7c-4e32-b4fd-351aec2b1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: obtemos todos os elementos coa clave \"B\"\n",
    "print(\"Palabras que empezan por B: \",rdd_pares.lookup('B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a5859-7a13-48fe-a6bc-822e282bed62",
   "metadata": {},
   "source": [
    "`lookup` √© √∫til:\n",
    "- Cando se quere obter rapidamente os valores dun elemento concreto sen facer un `filter()` + `collect()`.\n",
    "### sampleByKey\n",
    "`sampleByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que permite obter unha mostra (sample) dos elementos, pero **facendo a mostra por clave**.\n",
    "\n",
    "√â dicir: en lugar de mostrear todo o RDD ao azar, **Spark aplica unha fracci√≥n de mostra diferente para cada clave**, mantendo as√≠ un control por grupos.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** outro `RDD[(K, V)]` cun subconxunto dos pares orixinais\n",
    "- **Idea:** ‚ÄúColler unha mostra estratificada por clave‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### Par√°metros\n",
    "- `withReplacement` (`bool`)\n",
    "  - `False`: selecci√≥n sen reposici√≥n (un elemento s√≥ pode aparecer unha vez)\n",
    "  - `True`: selecci√≥n con reposici√≥n (pode repetirse)\n",
    "- `fractions` (`dict`)\n",
    "  - Un dicionario co formato `{clave: fracci√≥n}`\n",
    "  - A fracci√≥n indica a proporci√≥n aproximada de elementos que se seleccionan para esa clave (0.0 ‚Üí 0%, 1.0 ‚Üí 100%)\n",
    "- `seed` (opcional)\n",
    "  - Semente para reproducibilidade no proceso aleatorio\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99bca0-8c49-4e88-8deb-ea4fa96aa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: mostras proporcionadas.\n",
    "\n",
    "rdd = sc.parallelize([\n",
    "    (\"A\", 1), (\"A\", 2), (\"A\", 3), (\"A\", 4),\n",
    "    (\"B\", 10), (\"B\", 20), (\"B\", 30),\n",
    "    (\"C\", 100), (\"C\", 200)\n",
    "])\n",
    "\n",
    "# De A collemos apr√≥ximadamente 0 50% dos elementos. De B todos (100%) e d e C ning√∫n (0%).\n",
    "fractions = {\"A\": 0.5, \"B\": 1.0, \"C\": 0.0}\n",
    "\n",
    "sample = rdd.sampleByKey(withReplacement=False, fractions=fractions, seed=42)\n",
    "\n",
    "print(sample.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ba589-2bcd-45e0-825b-6d193e4e9be6",
   "metadata": {},
   "source": [
    "`sampleByKey` server para:\n",
    "- facer mostras mantendo a proporci√≥n por grupos/claves\n",
    "- equilibrar datasets (ex.: coller menos exemplos dunha clase maioritaria)\n",
    "- crear subconxuntos para test/validaci√≥n mantendo distribuci√≥n por clave\n",
    "\n",
    "## Agregaci√≥ns\n",
    "### groupByKey\n",
    "`groupByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que agrupa todos os valores que te√±en a mesma clave, devolvendo un RDD onde cada clave queda asociada a unha colecci√≥n (iterable) de valores.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** `RDD[(K, Iterable[V])]`\n",
    "- **Idea:** ‚ÄúPara cada clave, xunta todos os valores nunha lista/iterable‚Äù\n",
    "\n",
    "**Transformaci√≥n conceptual**:\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, Iterable[V])]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f3053-5329-4bc7-af7d-e220ce0e5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Agrupamos palabras por inicial:\n",
    "for key, values_iterable in rdd_pares.groupByKey().collect():\n",
    "    # Convertir el iterable de resultados a una lista\n",
    "    values_list = list(values_iterable)\n",
    "    # Imprimir la clave y los valores\n",
    "    print(f\"Clave: {key}, Valores: {values_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc93fa-fd04-4f5f-9626-0a7c79aab99e",
   "metadata": {},
   "source": [
    "`groupByKey` √© √∫til cando se necesita:\n",
    "- Obter todos os valores dunha clave e procesalos como xunto.\n",
    "- Aplicar operaci√≥ns complexas por grupo.\n",
    "\n",
    "**Advertencia importante (rendemento)**\n",
    "\n",
    "En moitos casos `groupByKey()` **non √© a mellor opci√≥n**, porque:\n",
    "\n",
    "- obriga a mover moitos datos pola rede (**shuffle**)\n",
    "- garda todos os valores dunha clave na memoria (pode causar **OutOfMemoryError**)\n",
    "\n",
    "Se o que se quere √© **agregar** (sumar, contar, m√°ximo, m√≠nimo...), √© mellor usar:\n",
    "\n",
    "- `reduceByKey()`\n",
    "- `aggregateByKey()`\n",
    "- `combineByKey()`\n",
    "- \n",
    "### reduceByKey\n",
    "`reduceByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que combina os valores que comparten a mesma clave aplicando unha funci√≥n de reduci√≥n (por exemplo suma, m√°ximo, concatenaci√≥n, etc.).\n",
    "\n",
    "A diferenza de `groupByKey()`, **non crea unha lista con todos os valores**, sen√≥n que os vai combinando de forma incremental, o que o fai moito m√°is eficiente.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** `RDD[(K, V)]`\n",
    "- **Requisito:** a funci√≥n debe ser do tipo `(V, V) ‚Üí V` (mesmo tipo de entrada e sa√≠da)\n",
    "\n",
    "**Transformaci√≥n conceptual**:\n",
    "\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, V)]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698b429-56fb-40cc-a57a-fa9f0687d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_pares.reduceByKey(lambda x,y:len(x)+len(y)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685fb48-feeb-4430-9f46-e9662fcc25b8",
   "metadata": {},
   "source": [
    "`reduceBykey` serve para:\n",
    "- sumar valores por clave (contaxes, totais‚Ä¶)\n",
    "- obter m√°ximos ou m√≠nimos por clave\n",
    "- combinar valores reducindo (p.ex. concatenar cadeas)\n",
    "- facer agregaci√≥ns eficientes a gran escala\n",
    "\n",
    "> Sempre que a operaci√≥n sexa reducible, d√©bese priorizar `reduceByKey()` fronte a `groupByKey()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5f0b9-a48e-46ae-b61a-e053a066ca31",
   "metadata": {},
   "source": [
    "### sortByKey\n",
    "`sortByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que ordena os elementos do RDD **segundo a clave**.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** `RDD[(K, V)]` cos pares ordenados pola clave\n",
    "- **Idea:** ‚ÄúOrdenar o RDD polo primeiro elemento do par (a clave)‚Äù\n",
    "\n",
    "üìå Transformaci√≥n conceptual:\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, V)] (ordenado por K)`\n",
    "\n",
    "#### Par√°metros:\n",
    "- `ascending`: orde ascendente ou descentente (por defecto `True`).\n",
    "- `nunPartitions`: n√∫mero de partici√≥ns do RDD resultante (√∫til para controlar o paralelismo).\n",
    "- `keyfunc`: Permite aplicar unha funci√≥n √° clave antes de ordenar.\n",
    "\n",
    "**Recom√©ndase usar**:\n",
    "- s√≥ cando realmente se precise ordenaci√≥n total\n",
    "- con datasets xa reducidos/agruapados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cc6fb-5c9b-4c21-9d0c-a312f32c9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Ordenar por clave descendentemente\n",
    "print(\"Por clave descendente: \", rdd_pares3.sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1e8e6-5b33-4541-bc0c-59dc6ffb5b64",
   "metadata": {},
   "source": [
    "### countByKey\n",
    "`countByKey()` √© unha **acci√≥n** dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que conta cantos elementos hai para cada clave, devolvendo un dicionario co n√∫mero de aparici√≥ns de cada clave.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** `dict` (en Python) co formato `{clave: n√∫mero_de_elementos}`\n",
    "- **Idea:** ‚ÄúCantas veces aparece cada clave?‚Äù\n",
    "\n",
    "Transformaci√≥n conceptual:\n",
    "\n",
    "`RDD[(K, V)] ‚Üí dict(K ‚Üí count)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6e18c-e75b-426d-97d9-747e1ae36096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: contar as ocorrencias de cada clave\n",
    "\n",
    "rdd_pares.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483d663-2dda-404c-aedc-f06ea61b2844",
   "metadata": {},
   "source": [
    "`countByKey` √© √∫til cando se quere:\n",
    "- Saber a distribuci√≥n de elementos por clave.\n",
    "- Facer unha an√°lise r√°pida de claves.\n",
    "- Validar datos (ver se hai claves con poucos elementos).\n",
    "\n",
    "**Advertencia**\n",
    "Como `countByKey` √© unha acci√≥n, trae o resultado ao *driver*. Se o dicionario √© demasiado grande pode xerar problemas de memoria.\n",
    "\n",
    "### aggregate\n",
    "`aggregate()` √© unha **acci√≥n** que permite reducir un RDD a un √∫nico resultado, aplicando:\n",
    "\n",
    "- unha funci√≥n para combinar elementos dentro de cada partici√≥n (`seqOp`)\n",
    "- outra funci√≥n para combinar resultados entre partici√≥ns (`combOp`)\n",
    "- partindo dun valor inicial (`zeroValue`)\n",
    "\n",
    "√â m√°is flexible que `reduce()`, porque:\n",
    "- o valor acumulado pode ser dun tipo distinto ao do RDD\n",
    "- permite definir unha l√≥xica diferente para o c√°lculo local e global\n",
    "\n",
    "#### Par√°metros\n",
    "- `zeroValue`: valor inicial do acumulador\n",
    "- `seqOp(acc, x)`: como se acumula un elemento `x` no acumulador dentro dunha partici√≥n\n",
    "- `combOp(acc1, acc2)`: como se combinan acumuladores de distintas partici√≥ns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfae0e-07f5-47c5-9021-84c966d34017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: c√°lculo da media\n",
    "rdd = sc.parallelize([10, 20, 30, 40])\n",
    "\n",
    "# Establecemos o valor inicial\n",
    "zero = (0, 0)  # (suma, conta)\n",
    "\n",
    "# Definimos a operaci√≥n de acumulaci√≥n\n",
    "seqOp = lambda acc, x: (acc[0] + x, acc[1] + 1)\n",
    "\n",
    "# Definimos a operaci√≥n de combinaci√≥n\n",
    "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "\n",
    "# Obtemos a suma total e o n√∫mero de elementos\n",
    "suma, conta = rdd.aggregate(zero, seqOp, combOp)\n",
    "\n",
    "# Xa no driver, calculamos e amosamos a media.\n",
    "media = suma / conta\n",
    "print(media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e29956-492e-42a9-a7b6-ad1e7809a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: C√°lculo de m√°ximo e suma simult√°neamente\n",
    "\n",
    "# Creamos un RDD con 4 elementos e 2 partici√≥ns\n",
    "rdd_pruebas = sc.parallelize([1, 2, 3, 4], 2)\n",
    "\n",
    "# glom() agrupa os elementos de cada partici√≥n nunha lista\n",
    "# collect() tr√°eo ao driver para poder velo\n",
    "print(rdd_pruebas.glom().collect())\n",
    "\n",
    "# seqOp (funci√≥n de secuencia): comb√≠na un acumulador coa seguinte entrada do RDD\n",
    "# acumulador x = (suma_parcial, contador_parcial)\n",
    "# elemento y = valor do RDD\n",
    "seqOp = lambda x, y: (x[0] + y, x[1] + 1)\n",
    "\n",
    "# combOp (funci√≥n de combinaci√≥n): combina dous acumuladores (de partici√≥ns distintas)\n",
    "# x e y son acumuladores: (suma, contador)\n",
    "combOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "# zero √© o valor inicial\n",
    "zero = (0,0)\n",
    "\n",
    "# aggregate(zeroValue, seqOp, combOp)\n",
    "# zeroValue = (0,0) √© o acumulador inicial\n",
    "# Resultado: (suma_total, conta_total)\n",
    "resultado = rdd_pruebas.aggregate(zero, seqOp, combOp)\n",
    "\n",
    "print(\"(suma, conteo): \", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cfd3e-2136-405a-9b9b-e2fb0f1b11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: c√°lculo do produto e do n√∫mero de elementos\n",
    "\n",
    "# Definimos a seqOp\n",
    "# calc√∫lanse pares coa seguinte forma (produto_parcial, contador_parcial)\n",
    "seqOp = (lambda x,y: (x[0] * y, x[1]+1))\n",
    "\n",
    "# Definimos a CombOp\n",
    "# Multiplicamos os produtos e sumamos os contadores\n",
    "combO = (lambda x,y: (x[0] * y[0], x[1]+y[1]))\n",
    "\n",
    "# Definimos o zero\n",
    "# Hai que ter en conta que o valor neutro do produto e o 1.\n",
    "zero = (1,0)\n",
    "\n",
    "# Calculamos o resultado final\n",
    "mult, count = rdd_pruebas.aggregate(zero,seqOp, combOp)\n",
    "print (\"Produto: \", mult,\", Conteo: \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f08ec5-65fc-40fa-b264-63412cfdc6c9",
   "metadata": {},
   "source": [
    "**`aggregate` √∫sase...**\n",
    "- Cando s necesita m√°is dunha m√©trica.\n",
    "- Cando se pretende empregar acumuladores m√°is complexos.\n",
    "- Cando `reduceByKey` non √© suficiente, xa que o resultado ten que ser do mesmo tipo que o orixinal.\n",
    "\n",
    "> √â moito m√°is eficiente que usar `gropByKey` + c√°lculo\n",
    "\n",
    "### aggregateByKey\n",
    "aggregateByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que permite combinar os valores asociados a cada clave usando un acumulador, de forma similar a `aggregate()`, pero **aplicado por clave**.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]`\n",
    "- **Sa√≠da:** `RDD[(K, U)]` onde `U` √© o tipo do acumulador\n",
    "- **Idea:** ‚ÄúPara cada clave, acumular e combinar valores usando un valor inicial e d√∫as funci√≥ns (local e global)‚Äù\n",
    "\n",
    "üìå Transformaci√≥n conceptual:\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, U)]`\n",
    "\n",
    "---\n",
    "\n",
    "#### Par√°metros\n",
    "- `zeroValue`\n",
    "  - valor inicial do acumulador **para cada clave** en cada partici√≥n\n",
    "  - debe ser un valor neutro (por exemplo `(0,0)` para suma+conta)\n",
    "- `seqOp(acc, v)`\n",
    "  - combina o acumulador `acc` cun valor `v` dentro dunha partici√≥n\n",
    "- `combOp(acc1, acc2)`\n",
    "  - combina dous acumuladores parciais (de partici√≥ns distintas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6cfa0-0938-484b-8226-639c434ded81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: suma e conteo por clave\n",
    "\n",
    "# Creamos un RDD de pares (k,v)\n",
    "rdd = sc.parallelize([\n",
    "    (\"a\", 10), (\"a\", 20),\n",
    "    (\"b\", 5), (\"b\", 15), (\"b\", 10)\n",
    "])\n",
    "\n",
    "# Definimos o valor zero\n",
    "zero = (0, 0)  # (suma, conta)\n",
    "\n",
    "# Definimos a SecOp\n",
    "seqOp = lambda acc, v: (acc[0] + v, acc[1] + 1)\n",
    "\n",
    "# Definimos a combOp\n",
    "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "\n",
    "# Obtemos e amosamos o resultado\n",
    "res = rdd.aggregateByKey(zero, seqOp, combOp)\n",
    "\n",
    "print(\"Resultado: \",res.collect())\n",
    "\n",
    "# Calculamos as medias:\n",
    "medias = res.mapValues(lambda sc: sc[0] /sc[1])\n",
    "print(\"Medias: \",medias.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e18aab-4f1a-4250-8f1d-da195c0033ff",
   "metadata": {},
   "source": [
    "\n",
    "**`aggregateByKey` √∫sase...**\n",
    "- Cando se necesita obter varias m√©tricas por clave.\n",
    "- Cando se necesitan acumuladores complexos (tuplas, listas, conxuntos...).\n",
    "- Cando `reduceByKey` non √© suficiente.\n",
    "\n",
    "### combineByKey\n",
    "`combineByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que permite crear e combinar acumuladores por clave de forma totalmente flexible.\n",
    "\n",
    "√â a operaci√≥n m√°is xeral de agregaci√≥n por clave en Spark e serve de base para outras funci√≥ns como:\n",
    "- `reduceByKey()`\n",
    "- `aggregateByKey()`\n",
    "\n",
    "**Transformaci√≥n conceptual**:\n",
    "\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, C)]`  \n",
    "onde `C` √© o tipo do acumulador (combiner).\n",
    "\n",
    "---\n",
    "\n",
    "#### Como funciona?\n",
    "Para cada clave, Spark:\n",
    "1. **Crea un acumulador inicial** a partir do primeiro valor que aparece para esa clave (`createCombiner`)\n",
    "2. Vai engadindo novos valores a ese acumulador dentro da mesma partici√≥n (`mergeValue`)\n",
    "3. Combina acumuladores parciais de partici√≥ns distintas (`mergeCombiners`)\n",
    "\n",
    "---\n",
    "\n",
    "#### Par√°metros\n",
    "- `createCombiner(v)`\n",
    "  - transforma un primeiro valor `v` nun acumulador inicial\n",
    "- `mergeValue(acc, v)`\n",
    "  - engade un valor `v` ao acumulador `acc` dentro da mesma partici√≥n\n",
    "- `mergeCombiners(acc1, acc2)`\n",
    "  - combina dous acumuladores procedentes de partici√≥ns diferentes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29add534-29c0-4920-9b01-b4f8693f6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Calcular a media de valores por clave usando combineByKey (sen groupByKey)\n",
    "\n",
    "# Creamos un RDD de pares (k, v) con varias entradas para as claves \"a\" e \"b\"\n",
    "rdd = sc.parallelize([\n",
    "    (\"a\", 10), (\"a\", 20),\n",
    "    (\"b\", 5), (\"b\", 15), (\"b\", 10)\n",
    "])\n",
    "\n",
    "# Definimos como crear o acumulador inicial a partir do primeiro valor dunha clave\n",
    "# Neste caso o acumulador ser√° unha tupla (suma, conta)\n",
    "createCombiner = lambda v: (v, 1)\n",
    "\n",
    "# Definimos como engadir un novo valor ao acumulador dentro da mesma partici√≥n\n",
    "# Engadimos o valor √° suma e incrementamos o contador\n",
    "mergeValue = lambda acc, v: (acc[0] + v, acc[1] + 1)\n",
    "\n",
    "# Definimos como combinar acumuladores de partici√≥ns distintas (durante o shuffle)\n",
    "# Sumamos as sumas parciais e tam√©n os contadores parciais\n",
    "mergeCombiners = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "\n",
    "# Aplicamos combineByKey para obter, para cada clave, a tupla (suma_total, conta_total)\n",
    "sum_count = rdd.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "\n",
    "# Calculamos a media por clave dividindo suma_total entre conta_total\n",
    "medias = sum_count.mapValues(lambda sc: sc[0] / sc[1])\n",
    "\n",
    "# Amosamos o resultado final: media dos valores asociados a cada clave\n",
    "print(\"Medias por clave: \", medias.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702d22b-f864-41cd-bd40-7d154005d281",
   "metadata": {},
   "source": [
    "**`combineByKey` √∫sase...**\n",
    "- Cando se necesita control total sobre como se crea e combina o acumulador.\n",
    "- Cando se quere transformar o tipo de acumuladro.\n",
    "- Cando se queren implementar agregaci√≥ns avanzadas.\n",
    "\n",
    "#### Por que √© importante?\n",
    "\n",
    "`combineByKey()` √© a funci√≥n m√°is potente para agregaci√≥n por clave.  \n",
    "As demais son versi√≥ns simplificadas:\n",
    "\n",
    "##### `reduceByKey(func)` equivale a:\n",
    "```python\n",
    "createCombiner = lambda v: v\n",
    "mergeValue = func\n",
    "mergeCombiners = func\n",
    "```\n",
    "##### `aggregateByKey(func)` equivale a:\n",
    "```python\n",
    "createCombiner = lambda v: seqOp(zero, v)\n",
    "mergeValue = seqOp\n",
    "mergeCombiners = combOp\n",
    "```\n",
    "\n",
    "### foldByKey\n",
    "`foldByKey()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que combina os valores asociados a cada clave aplicando unha funci√≥n de reduci√≥n, partindo dun valor inicial com√∫n (`zeroValue`).\n",
    "\n",
    "√â moi semellante a `reduceByKey()`, pero coa diferenza de que:\n",
    "- usa un **valor inicial (`zeroValue`)**\n",
    "- este valor inicial debe ser **neutro** respecto da funci√≥n de combinaci√≥n\n",
    "- a funci√≥n `func` debe ser do tipo `(V, V) -> V` (mesmo tipo de entrada e sa√≠da)\n",
    "\n",
    "üìå Transformaci√≥n conceptual:\n",
    "`RDD[(K, V)] ‚Üí RDD[(K, V)]`\n",
    "\n",
    "---\n",
    "\n",
    "#### Par√°metros\n",
    "- `zeroValue`\n",
    "  - valor inicial que se aplica para cada clave en cada partici√≥n\n",
    "  - debe ser un elemento neutro (por exemplo `0` para suma, `1` para multiplicaci√≥n)\n",
    "- `func(x, y)`\n",
    "  - funci√≥n que combina dous valores do mesmo tipo\n",
    "  - debe ser asociativa e conmutativa para garantir resultados consistentes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9635a7e-4587-468b-b67f-d72bf2101813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Sumar os valores por clave usando foldByKey cun valor inicial neutro (0)\n",
    "\n",
    "# Creamos un RDD de pares (k, v) con varias entradas por clave\n",
    "rdd = sc.parallelize([\n",
    "    (\"a\", 10), (\"a\", 20),\n",
    "    (\"b\", 5), (\"b\", 15), (\"b\", 10)\n",
    "])\n",
    "\n",
    "# Aplicamos foldByKey usando 0 como valor inicial (neutro para a suma)\n",
    "# A funci√≥n combina os valores asociados √° mesma clave sum√°ndoos\n",
    "resultado = rdd.foldByKey(0, lambda x, y: x + y)\n",
    "\n",
    "# Amosamos o resultado final: suma total dos valores por clave\n",
    "print(\"Suma por clave: \", resultado.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66465f72-817c-40bc-bcf0-747e8c6524bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Calcular o produto dos valores por clave usando foldByKey cun valor inicial neutro (1)\n",
    "\n",
    "# Creamos un RDD de pares (k, v) con varias entradas por clave\n",
    "rdd = sc.parallelize([\n",
    "    (\"a\", 2), (\"a\", 3),\n",
    "    (\"b\", 4), (\"b\", 5)\n",
    "])\n",
    "\n",
    "# Aplicamos foldByKey usando 1 como valor inicial (neutro para o produto)\n",
    "# A funci√≥n combina os valores asociados √° mesma clave multiplic√°ndoos\n",
    "resultado = rdd.foldByKey(1, lambda x, y: x * y)\n",
    "\n",
    "# Amosamos o resultado final: produto total dos valores por clave\n",
    "print(\"Produto por clave: \", resultado.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c725952-571d-441f-b108-3886fa92073d",
   "metadata": {},
   "source": [
    "**`foldByKey` √∫sase...**\n",
    "- Cando se quere facer unha reduci√≥n por clave que precisa un valor inicial.\n",
    "- Cando o operador ten un elemento neutro natural.\n",
    "- Cando se quere unha alternativa m√°is expl√≠cita a `reduceByKey`.\n",
    "\n",
    "< **Advertencia**: Se o `zeroValue` non √© neutro apl√≠case en cada partici√≥n.\n",
    "\n",
    "### coGroup\n",
    "`cogroup()` √© unha transformaci√≥n dispo√±ible para **Pair RDDs** (RDDs de pares `(K, V)`) que permite agrupar dous (ou m√°is) RDDs **pola mesma clave**, devolvendo para cada clave unha parella de colecci√≥ns:  \n",
    "- os valores desa clave no primeiro RDD\n",
    "- os valores desa clave no segundo RDD\n",
    "\n",
    "√â √∫til para comparar ou combinar datos por clave cando **non se quere facer un `join` directo**, xa que `cogroup()` conserva **t√≥dolos valores** en cada lado.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]` e outro `RDD[(K, W)]`\n",
    "- **Sa√≠da:** `RDD[(K, (Iterable[V], Iterable[W]))]`\n",
    "\n",
    "üìå Transformaci√≥n conceptual:\n",
    "`RDD[(K, V)]` + `RDD[(K, W)] ‚Üí RDD[(K, (Iterable[V], Iterable[W]))]`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf99f3-f491-4a2a-9312-b5670ef693bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Agrupar dous RDDs pola mesma clave usando cogroup\n",
    "\n",
    "# Creamos o primeiro RDD de pares (k, v)\n",
    "rdd1 = sc.parallelize([\n",
    "    (\"a\", 1), (\"b\", 2), (\"a\", 3)\n",
    "])\n",
    "\n",
    "# Creamos o segundo RDD de pares (k, w)\n",
    "rdd2 = sc.parallelize([\n",
    "    (\"a\", 10), (\"b\", 20), (\"b\", 30)\n",
    "])\n",
    "\n",
    "# Aplicamos cogroup para agrupar por clave os valores de ambos RDDs\n",
    "resultado = rdd1.cogroup(rdd2)\n",
    "\n",
    "# Convertimos os iterables a listas s√≥ para poder visualizar mellor o contido\n",
    "resultado_listas = resultado.mapValues(lambda x: (list(x[0]), list(x[1])))\n",
    "\n",
    "# Amosamos o resultado final: para cada clave, lista de valores do rdd1 e lista de valores do rdd2\n",
    "print(\"Valores agrupados por clave en ambos RDDs: \", resultado_listas.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83800249-b9ed-46a9-82a1-1e6ea38aeb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "distinctChars = rdd_st.flatMap(lambda word: word.lower()).distinct()\n",
    "charRDD = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD2 = distinctChars.map(lambda c: (c, random.random()))\n",
    "resultado = charRDD.cogroup(charRDD2).take(5)\n",
    "\n",
    "for clave, valores in resultado:\n",
    "    print(\"Clave:\", clave)\n",
    "    print(\"Valores de charRDD:\", list(valores[0]))  # Convertimos el iterable de valores a lista\n",
    "    print(\"Valores de charRDD2:\", list(valores[1])) # Convertimos el iterable de valores a lista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f223c1-d766-45d8-ae8d-cfdaab07d320",
   "metadata": {},
   "source": [
    "**`cogroup()` √© √∫til cando se quere**\n",
    "- agrupar dous datasets pola mesma clave mantendo todos os valores de cada lado\n",
    "- comparar listas de valores por clave (por exemplo, elementos presentes nun dataset e non noutro)\n",
    "- preparar operaci√≥ns personalizadas onde un join non encaixa ben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661934a-7492-475d-a42b-1732debcf56a",
   "metadata": {},
   "source": [
    "## Joins en Pair RDDs (RDDs de pares)\n",
    "\n",
    "En Spark existen varios tipos de **join** para combinar dous **Pair RDDs** (`RDD[(K, V)]`) usando a **clave K** como criterio de uni√≥n.\n",
    "\n",
    "Todos os joins devolven un RDD onde cada elemento ten a forma:\n",
    "`(K, (V, W))` (ou variantes con `None` no caso de outer joins)\n",
    "\n",
    "---\n",
    "\n",
    "### 1) `join()` ‚Äî Inner join\n",
    "Combina s√≥ as claves que aparecen en **ambos RDDs**.\n",
    "\n",
    "- **Entrada:** `RDD[(K, V)]` e `RDD[(K, W)]`\n",
    "- **Sa√≠da:** `RDD[(K, (V, W))]`\n",
    "- S√≥ devolve claves com√∫ns.\n",
    "\n",
    "> √ötil cando se quere a intersecci√≥n de claves.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) `fullOuterJoin()` ‚Äî Full outer join\n",
    "Combina claves de **ambos RDDs**, mesmo se s√≥ aparecen nun deles.\n",
    "\n",
    "- **Sa√≠da:** `RDD[(K, (Optional[V], Optional[W]))]`\n",
    "- Se falta un valor nun lado, aparece como `None`.\n",
    "\n",
    "> √ötil cando se quere manter toda a informaci√≥n posible.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) `leftOuterJoin()` ‚Äî Left outer join\n",
    "Mant√©n todas as claves do **RDD esquerdo**, e s√≥ combina as que existan no dereito.\n",
    "\n",
    "- **Sa√≠da:** `RDD[(K, (V, Optional[W]))]`\n",
    "- Se unha clave non existe no dereito, o valor dereito ser√° `None`.\n",
    "\n",
    ">  √ötil cando se quere manter todo o dataset principal (esquerdo).\n",
    "\n",
    "---\n",
    "\n",
    "### 4) `rightOuterJoin()` ‚Äî Right outer join\n",
    "Mant√©n todas as claves do **RDD dereito**, e s√≥ combina as que existan no esquerdo.\n",
    "\n",
    "- **Sa√≠da:** `RDD[(K, (Optional[V], W))]`\n",
    "- Se unha clave non existe no esquerdo, o valor esquerdo ser√° `None`.\n",
    "\n",
    ">  √ötil cando o dataset principal √© o dereito.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) `cartesian()` ‚Äî Produto cartesiano (non recomendado)\n",
    "Xera todas as combinaci√≥ns posibles entre os elementos de dous RDDs:\n",
    "\n",
    "- **Entrada:** `RDD[A]` e `RDD[B]`\n",
    "- **Sa√≠da:** `RDD[(A, B)]`\n",
    "- Non usa claves: combina todo con todo.\n",
    "\n",
    "**Non se recomenda o seu uso**, porque:\n",
    "- pode xerar un n√∫mero enorme de combinaci√≥ns (`n * m`)\n",
    "- require moitos recursos e pode facer fallar o proceso por falta de memoria\n",
    "\n",
    "> S√≥ ten sentido en casos moi controlados ou con datasets moi pequenos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb3912-3823-4fe5-b5e3-ba9cc3138014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Comparar distintos tipos de join entre dous Pair RDDs (inner, outer e cartesian)\n",
    "\n",
    "# Creamos o primeiro RDD de pares (k, v) que actuar√° como RDD esquerdo nos joins\n",
    "rdd1 = sc.parallelize([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n",
    "\n",
    "# Creamos o segundo RDD de pares (k, v) que actuar√° como RDD dereito nos joins\n",
    "rdd2 = sc.parallelize([('a', 4), ('b', 5), ('c', 6), ('e', 5)])\n",
    "\n",
    "# Realizamos un inner join: devolve s√≥ as claves que existen en ambos RDDs (\"a\", \"b\" e \"c\")\n",
    "rdd3 = rdd1.join(rdd2)\n",
    "\n",
    "# Amosamos o resultado do inner join: (clave, (valor_rdd1, valor_rdd2))\n",
    "print(\"Inner join:\")\n",
    "print(\"Resultado do inner join: \", rdd3.collect())\n",
    "\n",
    "\n",
    "# Realizamos un full outer join: devolve todas as claves de ambos RDDs (\"a\", \"b\", \"c\", \"d\" e \"e\")\n",
    "# Se unha clave non existe nun dos lados, o valor aparece como None\n",
    "rdd4 = rdd1.fullOuterJoin(rdd2)\n",
    "\n",
    "# Amosamos o resultado do full outer join: (clave, (valor_ou_None, valor_ou_None))\n",
    "print(\"Full Outer join:\")\n",
    "print(\"Resultado do full outer join: \", rdd4.collect())\n",
    "\n",
    "\n",
    "# Realizamos un left outer join: devolve todas as claves do RDD esquerdo (rdd1)\n",
    "# Para claves que non existen no dereito, o valor dereito ser√° None (por exemplo, \"d\")\n",
    "rdd5 = rdd1.leftOuterJoin(rdd2)\n",
    "\n",
    "# Amosamos o resultado do left outer join: (clave, (valor_rdd1, valor_rdd2_ou_None))\n",
    "print(\"Left Outer join:\")\n",
    "print(\"Resultado do left outer join: \", rdd5.collect())\n",
    "\n",
    "\n",
    "# Realizamos un right outer join: devolve todas as claves do RDD dereito (rdd2)\n",
    "# Para claves que non existen no esquerdo, o valor esquerdo ser√° None (por exemplo, \"e\")\n",
    "rdd6 = rdd1.rightOuterJoin(rdd2)\n",
    "\n",
    "# Amosamos o resultado do right outer join: (clave, (valor_rdd1_ou_None, valor_rdd2))\n",
    "print(\"Right Outer join:\")\n",
    "print(\"Resultado do right outer join: \", rdd6.collect())\n",
    "\n",
    "\n",
    "# Realizamos un produto cartesiano: combina todos os elementos de rdd1 con todos os elementos de rdd2\n",
    "# O resultado ter√° tama√±o len(rdd1) * len(rdd2) (neste caso 4 * 4 = 16 pares)\n",
    "# Non se recomenda para datasets grandes porque pode xerar un n√∫mero enorme de combinaci√≥ns\n",
    "rdd7 = rdd1.cartesian(rdd2)\n",
    "\n",
    "# Amosamos o resultado do cartesian: ((k1, v1), (k2, v2))\n",
    "print(\"Cartesian:\")\n",
    "print(\"Resultado do produto cartesiano: \", rdd7.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fefc2-7499-4109-b190-9d2892f7aba3",
   "metadata": {},
   "source": [
    "### union\n",
    "`union()` √© unha transformaci√≥n que permite unir dous RDDs nun √∫nico RDD, concatenando os seus elementos.\n",
    "\n",
    "- **Entrada:** dous RDDs do mesmo tipo (`RDD[T]` + `RDD[T]`)\n",
    "- **Sa√≠da:** un novo `RDD[T]` con todos os elementos dos dous RDDs\n",
    "- **Comportamento:** non elimina duplicados, simplemente xunta contidos\n",
    "\n",
    "üìå Transformaci√≥n conceptual:\n",
    "`RDD[T] + RDD[T] ‚Üí RDD[T]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0474f7-92eb-435e-8efd-6ddb27a2cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Unir dous RDDs de n√∫meros usando union()\n",
    "\n",
    "# Creamos o primeiro RDD de n√∫meros\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Creamos o segundo RDD de n√∫meros\n",
    "rdd2 = sc.parallelize([3, 4, 5])\n",
    "\n",
    "# Aplicamos union para unir ambos RDDs\n",
    "resultado = rdd1.union(rdd2)\n",
    "\n",
    "# Amosamos o resultado final: cont√©n todos os elementos dos dous RDDs (inclu√≠ndo duplicados)\n",
    "print(\"Uni√≥n dos dous RDDs: \", resultado.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681dce8-de6e-4b1a-9f3a-7fb14400ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Unir dous Pair RDDs usando union()\n",
    "\n",
    "# Creamos o primeiro Pair RDD\n",
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "\n",
    "# Creamos o segundo Pair RDD\n",
    "rdd2 = sc.parallelize([(\"b\", 3), (\"c\", 4)])\n",
    "\n",
    "# Aplicamos union para unir ambos Pair RDDs\n",
    "resultado_pair = rdd1.union(rdd2)\n",
    "\n",
    "# Amosamos o resultado final: cont√©n todos os pares, inclu√≠ndo claves repetidas\n",
    "print(\"Uni√≥n de dous Pair RDDs: \", resultado_pair.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4935b-90af-41dd-8abd-5e008b91ae80",
   "metadata": {},
   "source": [
    "**`union()` √© √∫til cando se quere**:\n",
    "- combinar datasets que te√±en o mesmo esquema/tipo\n",
    "- xuntar datos de varias fontes\n",
    "- constru√≠r un dataset maior a partir de varios RDDs\n",
    "\n",
    "> `union` non elimina duplicados. Para facelo pode empregarse `distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6b5f6-1b7d-44b9-abdb-f49eeb0c39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Union de dous RDD sen duplicados: \", resultado.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02ded2-1449-4a4a-a19e-78644537c053",
   "metadata": {},
   "source": [
    "### zip\n",
    "`zip()` √© unha transformaci√≥n que permite combinar dous RDDs elemento a elemento, formando pares cos elementos da mesma posici√≥n.\n",
    "\n",
    "- **Entrada:** dous RDDs `RDD[A]` e `RDD[B]`\n",
    "- **Sa√≠da:** `RDD[(A, B)]`\n",
    "- **Comportamento:** o elemento `i` do primeiro RDD comb√≠nase co elemento `i` do segundo RDD\n",
    "\n",
    "**Transformaci√≥n conceptual**:\n",
    "\n",
    "`RDD[A] + RDD[B] ‚Üí RDD[(A, B)]`\n",
    "\n",
    "---\n",
    "\n",
    "#### Requisitos importantes\n",
    "Para que `zip()` funcione, ambos RDDs deben cumprir:\n",
    "\n",
    "- ter o **mesmo n√∫mero de elementos**\n",
    "- ter o **mesmo n√∫mero de partici√≥ns**\n",
    "- ter o **mesmo n√∫mero de elementos en cada partici√≥n**\n",
    "\n",
    "> Se non se cumpre alg√∫n destes requisitos, Spark lanza un erro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc0a08-81c4-4419-9bc8-41f940ca6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Combinar dous RDDs elemento a elemento usando zip()\n",
    "\n",
    "# Creamos o primeiro RDD de n√∫meros\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4], 2)\n",
    "\n",
    "# Creamos o segundo RDD de letras co mesmo n√∫mero de elementos e partici√≥ns\n",
    "rdd2 = sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 2)\n",
    "\n",
    "# Aplicamos zip para combinar elemento a elemento\n",
    "resultado = rdd1.zip(rdd2)\n",
    "\n",
    "# Amosamos o resultado final: cada n√∫mero queda emparellado coa letra da mesma posici√≥n\n",
    "print(\"RDD combinado con zip(): \", resultado.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4515a2-90a5-4859-b7d7-520d8d79a309",
   "metadata": {},
   "source": [
    "## Controlando partici√≥ns en RDDs\n",
    "\n",
    "A API de RDDs permite controlar como se distrib√∫en fisicamente os datos a trav√©s do cl√∫ster. O n√∫mero de partici√≥ns infl√∫e directamente no paralelismo e no rendemento: pode axudar a repartir mellor a carga de traballo, pero tam√©n pode provocar sobrecarga se hai demasiadas partici√≥ns ou custes elevados de shuffle se hai demasiada redistribuci√≥n.\n",
    "\n",
    "### `coalesce(numPartitions, shuffle=False)`\n",
    "`coalesce()` permite reducir o n√∫mero de partici√≥ns colapsando partici√≥ns existentes, normalmente sen mover datos entre nodos. Por defecto non realiza shuffle (`shuffle=False`), polo que √© unha opci√≥n eficiente cando se quere diminu√≠r partici√≥ns despois de filtros ou operaci√≥ns que deixan poucos datos. Se se usa `shuffle=True`, ent√≥n si pode redistribu√≠r datos e o custo aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607fce-73ac-482a-b582-8422664b47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Reducir o n√∫mero de partici√≥ns dun RDD usando coalesce()\n",
    "\n",
    "rdd_st = sc.parallelize (\"Big Data aplicado. Curso de especializaci√≥n de Inteligencia Artificial y Big Data\".split(), 4)\n",
    "\n",
    "print(\"N√∫mero inicial de partici√≥ns: \", rdd_st.getNumPartitions())\n",
    "\n",
    "# Reducimos o n√∫mero de partici√≥ns a 2 sen shuffle (por defecto)\n",
    "rdd2 = rdd_st.coalesce(2)\n",
    "\n",
    "# Amosamos o n√∫mero final de partici√≥ns\n",
    "print(\"N√∫mero de partici√≥ns tras coalesce(2): \", rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf961b-4695-4f9d-b051-f9fc39bf0460",
   "metadata": {},
   "source": [
    "### `repartition(numPartitions)`\n",
    "`repartition()` permite modificar o n√∫mero de partici√≥ns dun RDD, pero sempre realiza un shuffle completo. Isto implica mover datos entre nodos para equilibrar e repartir de novo a carga. √â √∫til cando se quere aumentar o paralelismo ou cando as partici√≥ns quedaron descompensadas, pero √© m√°is custoso que `coalesce()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e2675-9bc9-4a6d-8d49-31b7c9052470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Modificar o n√∫mero de partici√≥ns dun RDD usando repartition()\n",
    "\n",
    "# Supo√±emos un RDD chamado rdd_st\n",
    "rdd4 = rdd_st.repartition(4)\n",
    "\n",
    "# Amosamos o n√∫mero final de partici√≥ns\n",
    "print(\"N√∫mero de partici√≥ns tras repartition(4): \", rdd4.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aafaae-438a-4791-a272-05c0692e52df",
   "metadata": {},
   "source": [
    "### `repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=None, ascending=True, keyfunc=None)`\n",
    "`repartitionAndSortWithinPartitions()` permite reparticionar os datos (shuffle) e, ademais, ordenar os elementos dentro de cada partici√≥n segundo a clave. Isto resulta √∫til cando se quere un reparto controlado por clave e se pretende preparar o dataset para operaci√≥ns posteriores que se benefician de datos xa ordenados dentro de cada partici√≥n.\n",
    "\n",
    "Par√°metros:\n",
    "- `numPartitions` (opcional): n√∫mero de partici√≥ns do resultado.\n",
    "- `partitionFunc` (opcional): funci√≥n que determina o √≠ndice da partici√≥n a partir da clave.\n",
    "- `ascending` (opcional): ordenaci√≥n ascendente (`True`, por defecto) ou descendente (`False`).\n",
    "- `keyfunc` (opcional): funci√≥n para transformar a clave antes de ordenar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3dcd6-879e-4c6c-973c-218cdb2fb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemplo: Reparticionar e ordenar dentro de cada partici√≥n usando repartitionAndSortWithinPartitions()\n",
    "\n",
    "# Creamos un Pair RDD (clave, valor)\n",
    "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "\n",
    "# Aplicamos repartitionAndSortWithinPartitions con 2 partici√≥ns\n",
    "# A funci√≥n de particionamento asigna partici√≥n segundo a paridade da clave (x % 2)\n",
    "# O ordenamento ser√° ascendente dentro de cada partici√≥n\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(\n",
    "    numPartitions=2,\n",
    "    partitionFunc=lambda x: x % 2,\n",
    "    ascending=True\n",
    ")\n",
    "\n",
    "# Amosamos o contido de cada partici√≥n como listas para observar o reparto e a orde interna\n",
    "resultado = rdd2.glom().collect()\n",
    "\n",
    "print(\"RDD reparticionado e ordenado dentro das partici√≥ns: \", resultado)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
