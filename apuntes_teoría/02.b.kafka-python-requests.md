# Kafka e Python — Inxesta de datos en tempo real desde APIs HTTP


O obxectivo destes apuntamentos é aprender a **inxestar datos en tempo real desde unha API HTTP externa** empregando Python e **publicalos nun topic de Kafka**, de maneira que poidan ser reutilizados posteriormente en prácticas de streaming (por exemplo con Spark Structured Streaming).

O foco non está en Kafka como ferramenta illada, senón en **construír unha fonte de datos en streaming** a partir dun servizo externo, aplicando un fluxo típico de enxeñaría de datos:
1) consulta periódica dunha API,
2) transformación da resposta,
3) xeración de eventos,
4) publicación en Kafka.

---

## Requisitos

- Kafka operativo cun broker accesible en `kafka:9092`.
- Topic creado previamente en Kafka.
- Python 3.x.
- Paquetes de Python:
  - `requests`
  - `kafka-python`

Instalación das dependencias:

```bash
pip install requests kafka-python
```

Nota:
- O hostname do broker é `kafka`.

---

## Guía xeral do proceso

1) Defínense unha ou varias fontes de datos (cidades, sensores, entidades).
2) Realízanse peticións HTTP periódicas a unha API externa.
3) Valídase e parsease a resposta.
4) Extráense os campos relevantes.
5) Constrúese un evento estruturado (JSON).
6) Publícase o evento nun topic de Kafka.
7) Kafka queda listo para ser consumido por outras aplicacións.

---

## Referencia de `requests`

### Que é `requests`

`requests` é unha libraría de Python para realizar peticións HTTP de forma sinxela. É moi utilizada para consumir APIs REST e para tarefas de inxesta de datos desde servizos externos.

---

### Métodos básicos necesarios

- `requests.get()`
- `response.status_code`
- `response.json()`
- `response.raise_for_status()`
- `timeout`

---

### Sintaxe básica de `requests.get()`

```python
requests.get(
    url,
    params=None,
    headers=None,
    timeout=None,
    verify=True,
    proxies=None,
    auth=None
)
```

#### Parámetros habituais
- `url` (obrigatorio)  
  Endpoint ao que se fai a petición.

- `params` (opcional)  
  Dicionario (ou lista de tuplas) cos parámetros da query string.  
  Por exemplo, `{"q": "test", "page": 2}` convértese en `?q=test&page=2`.

- `headers` (opcional)  
  Dicionario coas cabeceiras HTTP. Útil para:
  - pedir JSON (`Accept: application/json`)
  - enviar tokens (`Authorization: Bearer ...`)
  - indicar user-agent, etc.

- `timeout` (moi recomendado)  
  Tempo máximo de espera en segundos. Pode ser:
  - un número: `timeout=10` (máximo total)
  - unha tupla: `timeout=(connect_timeout, read_timeout)` por exemplo `timeout=(3, 10)`  
    (ata 3s para conectar e ata 10s para recibir a resposta)

- `verify` (opcional, por defecto `True`)  
  Verificación TLS/SSL cando a URL é HTTPS.  
  En xeral **non se debe desactivar** (`verify=False`) salvo casos moi concretos en laboratorio.

- `proxies` (opcional)  
  Configuración de proxy HTTP/HTTPS (pouco habitual en aula).

- `auth` (opcional)  
  Autenticación básica (`(user, pass)`) ou obxectos de auth (pouco habitual en APIs públicas).


---

### Exemplo 1: Open-Meteo

```python

import requests

# Endpoint da API de Open-Meteo.
# Neste caso imos consultar un "forecast", pero só pedimos o tempo actual (current_weather).
url = "https://api.open-meteo.com/v1/forecast"

# Parámetros da query string que se engaden á URL.
# Open-Meteo usa latitude/longitude para identificar a localización.
params = {
    "latitude": 42.8782,         # latitude da localización (ex.: Santiago aprox.)
    "longitude": -8.5448,        # lonxitude da localización
    "current_weather": True      # indica que queremos incluír o bloque "current_weather" na resposta
}

# Realiza a petición HTTP GET.
# params -> convértese en query string (ex.: ?latitude=...&longitude=...&current_weather=true)
# timeout -> evita que o programa quede bloqueado se a API tarda demasiado en responder
r = requests.get(url, params=params, timeout=10)

# Comproba o código de estado HTTP.
# Se a resposta é un erro (4xx/5xx), lanza unha excepción e non seguimos procesando.
r.raise_for_status()

# Parseo do corpo da resposta como JSON.
# Devolve un dict/list de Python (dependendo da estrutura da resposta).
data = r.json()

# Extrae a parte da resposta que nos interesa: as condicións meteorolóxicas actuais.
# Se a clave non existe, lanzará KeyError; en produción convén validar antes con:
# if "current_weather" in data: ...
current_weather = data["current_weather"]
```



---

### Exemplo 2: CoinGecko

```python
import requests

# Endpoint da API de CoinGecko para obter prezos simples.
# Este endpoint devolve un JSON cos prezos dunha ou varias moedas en moedas fiat (EUR, USD, etc.).
url = "https://api.coingecko.com/api/v3/simple/price"

# Parámetros da query string.
params = {
    "ids": "bitcoin",        # identificador(s) da criptomoeda en CoinGecko (pode ser lista separada por comas: "bitcoin,ethereum")
    "vs_currencies": "eur"   # moeda(s) nas que queremos o prezo (tamén pode ser lista: "eur,usd")
}

# Realiza a petición HTTP GET.
# params -> convértese en query string (ex.: ?ids=bitcoin&vs_currencies=eur)
# timeout -> evita que o programa quede bloqueado se a API non responde
r = requests.get(url, params=params, timeout=10)

# Verifica que a resposta HTTP non é un erro (4xx/5xx).
# Se hai erro, lanza excepción e evita procesar un JSON incompleto ou unha páxina de erro.
r.raise_for_status()

# Parseo do corpo como JSON.
# A resposta adoita ter un formato similar a:
# {"bitcoin": {"eur": 12345.67}}
price = r.json()
```


---

## Referencia de `kafka-python`

### Descrición da biblioteca

`kafka-python` é un cliente Kafka escrito en Python que permite producir e consumir mensaxes en Kafka.  
Non crea topics automaticamente salvo que o broker teña habilitada a autocreación.

Documentación:  
https://kafka-python.readthedocs.io/en/master/

---

### KafkaProducer

Kafka traballa con **bytes**, polo que é habitual empregar *serializers*.

#### Creación dun producer

```python
from kafka import KafkaProducer
from json import dumps

producer = KafkaProducer(
    bootstrap_servers=["kafka:9092"],
    value_serializer=lambda v: dumps(v).encode("utf-8")
)
```

Parámetros principais:
- `bootstrap_servers`
- `value_serializer`
- (opcional) `key_serializer`

---

#### Envío de mensaxes

```python
# Enviamos unha mensaxe ao topic "open-meteo-weather".
# - O parámetro value é un obxecto Python (dict) que será convertido a bytes
#   mediante o value_serializer definido ao crear o KafkaProducer.
# - send() é ASÍNCRONO: devolve inmediatamente un "Future" e non bloquea esperando confirmación.
#   (Se se quere validar a entrega, pódese gardar o Future e chamar a future.get(...)).
producer.send("open-meteo-weather", value={"temp": 12.3})

# flush() bloquea ata que todas as mensaxes pendentes no buffer do producer sexan enviadas a Kafka.
# Úsase para:
# - garantir que os envíos se completaron antes de rematar o programa,
# - ou en laboratorio, para asegurar que o que enviamos xa se pode consumir inmediatamente.
# Nota: chamar a flush() en cada mensaxe reduce o rendemento; para produción adoita chamarse
# cada certo tempo ou só ao final (ou confiar no buffer).
producer.flush()
```


Notas:
- `send()` é asíncrono.
- `flush()` garante que as mensaxes se envían.

---

### KafkaConsumer (validación básica)

#### Sintaxe

```python
KafkaConsumer(
    *topics,
    bootstrap_servers=None,
    group_id=None,
    auto_offset_reset="latest",
    enable_auto_commit=True,
    auto_commit_interval_ms=5000,
    key_deserializer=None,
    value_deserializer=None,
    consumer_timeout_ms=None
)
```

---

#### Explicación dos parámetros

- `*topics`  
  Un ou máis topics aos que subscribirse. Pódense pasar como argumentos posicionais:
  - `KafkaConsumer("t1")`
  - `KafkaConsumer("t1", "t2")`

- `bootstrap_servers`  
  Lista de brokers (host:porto) para conectarse ao clúster e descubrir metadatos.  
  Exemplo: `["kafka:9092"]`.

- `group_id`  
  Identificador do consumer group. Kafka usa este valor para:
  - coordinar varios consumers no mesmo grupo,
  - asignar particións,
  - gardar offsets asociados ao grupo (cando hai commits).  
  Se `group_id=None`, o consumo non se coordina como consumer group (modo máis “simple”).

- `auto_offset_reset`  
  Que facer **se non existe offset gardado** para ese `group_id` e topic/partición:
  - `"earliest"`: comezar dende o inicio do topic
  - `"latest"`: comezar dende o final (só novas mensaxes)  
  Nota: só aplica cando non hai offset previo gardado.

- `enable_auto_commit`  
  Se `True` (por defecto), o consumer fai commits automáticos de offsets periodicamente.  
  Se `False`, os commits deben facerse manualmente con `consumer.commit()`.

- `auto_commit_interval_ms`  
  Cada canto tempo se fai o commit automático (en ms), se `enable_auto_commit=True`.

- `key_deserializer`  
  Función para transformar a `key` desde `bytes` a un tipo Python (por exemplo `str`).  
  Exemplo: `lambda k: k.decode("utf-8") if k is not None else None`

- `value_deserializer`  
  Función para transformar o `value` desde `bytes` a un tipo Python.  
  Exemplo típico para JSON: `lambda v: loads(v.decode("utf-8"))`

- `consumer_timeout_ms`  
  Tempo máximo (en ms) que o iterador `for msg in consumer` espera sen recibir mensaxes.  
  Se se supera, o iterador remata (útil para scripts de proba sen bucles infinitos).

---

#### Exemplo comentado

```python
from kafka import KafkaConsumer
from json import loads

# Creamos un consumer e subscribímonos ao topic "open-meteo-weather".
consumer = KafkaConsumer(
    "open-meteo-weather",

    # Broker do clúster ao que nos conectamos.
    bootstrap_servers=["kafka:9092"],

    # Deserialización do value:
    # - Kafka entrega bytes
    # - decode("utf-8") -> string
    # - loads(...) -> dict (porque é JSON)
    value_deserializer=lambda v: loads(v.decode("utf-8")),

    # Se é a primeira vez que este grupo le o topic (non hai offsets gardados),
    # comeza dende o inicio para ver todas as mensaxes.
    auto_offset_reset="earliest",

    # Identificador do consumer group.
    # Kafka gardará offsets asociados a este grupo (se hai commits).
    group_id="demo-group"

    # enable_auto_commit=True é o valor por defecto,
    # polo que o consumer irá gardando offsets automaticamente.
)

# Iteramos indefinidamente lendo mensaxes a medida que chegan.
# Cada "msg" inclúe metadatos (topic, partición, offset...) e o contido (value).
for msg in consumer:
    # Como definimos value_deserializer, msg.value xa é un dict (non bytes).
    print(msg.value)

    # Se queres ver tamén metadatos:
    # print(msg.topic, msg.partition, msg.offset, msg.value)
```

```

---

## Exemplo completo: producer Open-Meteo


```python
from kafka import KafkaProducer
from json import dumps
import time
import requests
from datetime import datetime

# Lista de localizacións das que imos obter datos meteorolóxicos.
# Cada elemento inclúe un nome (para identificar o evento) e coordenadas.
cities = [
    {"name": "Santiago", "latitude": 42.8782, "longitude": -8.5448},
    {"name": "Madrid", "latitude": 40.4168, "longitude": -3.7038},
    {"name": "New York", "latitude": 40.7128, "longitude": -74.0060},
]

# Creamos un produtor de Kafka que se conecta ao broker "kafka:9092".
# - bootstrap_servers: lista de brokers para descubrir o clúster.
# - value_serializer: función que converte o value (dict) a bytes.
#   Neste caso: dict -> JSON (dumps) -> bytes (encode UTF-8).
# Nota: Kafka almacena bytes, polo que sen serializer habería que enviar bytes manualmente.
producer = KafkaProducer(
    bootstrap_servers=["kafka:9092"],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)

# Bucle infinito: o script actúa como "fonte de datos" e publica eventos continuamente.
while True:
    # Iteramos polas cidades definidas e xeramos un evento por cidade.
    for city in cities:
        # Construímos a URL da API Open-Meteo.
        # Solicitamos o tempo actual con current_weather=true para reducir o volume de datos.
        url = (
            "https://api.open-meteo.com/v1/forecast"
            f"?latitude={city['latitude']}"
            f"&longitude={city['longitude']}"
            "&current_weather=true"
        )
        try:
            # Facemos a petición HTTP cun timeout para evitar bloqueos.
            r = requests.get(url, timeout=10)

            # Se a resposta é un erro (4xx/5xx), lanza excepción e salta ao except.
            r.raise_for_status()

            # Convertimos o corpo da resposta a un dict de Python.
            weather_data = r.json()

            # Comprobamos que existe o campo "current_weather" na resposta.
            # (Isto evita KeyError se a API devolve algo distinto do esperado.)
            if "current_weather" in weather_data:
                # Extraemos o bloque de meteoroloxía actual.
                data = weather_data["current_weather"]

                # Engadimos información adicional para contextualizar o evento.
                # - city: nome da cidade para identificar a procedencia do dato.
                data["city"] = city["name"]

                # Engadimos un timestamp local (UTC) no momento de inxesta.
                # Isto é útil para análise e para depuración, independentemente do timestamp da API.
                data["local_timestamp"] = datetime.utcnow().isoformat()

                # Enviamos o evento ao topic "open-meteo-weather".
                # send() é asíncrono: encola a mensaxe no buffer do producer.
                # Para validar entrega en laboratorio, poderías gardar o Future e chamar a future.get().
                producer.send("open-meteo-weather", value=data)

                # Mostramos por consola o que enviamos (útil para verificar rapidamente).
                print(f"[{datetime.now()}] Enviado: {data}")

        # Captura calquera erro (rede, timeout, HTTPError, JSON inválido, etc.)
        # e continúa co resto de cidades sen parar o script completo.
        except Exception as e:
            print(f"Erro obtendo datos de {city['name']}: {e}")

    # Agardamos 10 segundos antes de repetir o ciclo completo.
    # Isto controla a frecuencia de publicación (1 evento por cidade cada 10s aprox.).
    time.sleep(10)
``` 


---

## Bibliografía

- https://kafka-python.readthedocs.io/en/master/
- https://requests.readthedocs.io/en/latest/
- https://open-meteo.com/
- https://kafka.apache.org/documentation/
