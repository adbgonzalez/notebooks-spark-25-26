# 00.B Linguaxes e motores no procesamento distribu√≠do

Este documento integra d√∫as perspectivas complementarias:

- **as linguaxes usadas historicamente e na actualidade** no procesamento distribu√≠do
- **exemplos equivalentes de Word Count** en distintos motores e linguaxes

O obxectivo √© entender que **o que √© distribu√≠do √© o motor**, non a linguaxe en si.

---

## 1. Linguaxes usadas no procesamento distribu√≠do

### 1.1 Java (a linguaxe fundacional)

**Uso hist√≥rico**
- Hadoop MapReduce
- HDFS
- YARN
- HBase
- Hive (UDFs)
- Spark (API nativa)

**Por que se usou**
- Portabilidade (JVM)
- Boa xesti√≥n de memoria
- Madurez en contornos empresariais
- Integraci√≥n sinxela con sistemas distribu√≠dos

**Situaci√≥n actual**
- Segue sendo clave a nivel interno
- Cada vez menos usada directamente polo usuario final

üëâ Linguaxe do sistema, non do analista

---

### 1.2 Scala (a linguaxe ‚Äúnatural‚Äù de Spark)

**Uso**
- Spark (linguaxe orixinal)
- Flink (API principal)
- Kafka Streams

**Por que se usou**
- Programaci√≥n funcional + OO
- Integraci√≥n directa coa JVM
- Ideal para traballar con DAGs

**Situaci√≥n actual**
- Moi potente
- Menos popular en ensino
- Mantense en contornos avanzados

üëâ Linguaxe m√°is potente para Spark, pero non a m√°is accesible

---

### 1.3 Python (a linguaxe dominante na actualidade)

**Uso**
- PySpark
- Hadoop Streaming
- Dask
- Ray
- Flink (API Python limitada)
- ML distribu√≠do

**Por que se usa**
- Sintaxe simple
- Dominio en ciencia de datos
- Gran ecosistema (pandas, numpy, ML)

**Limitaci√≥ns**
- Non √© a linguaxe nativa dos motores
- Overhead pola comunicaci√≥n coa JVM

üëâ Linguaxe principal para ensino e an√°lise

---

### 1.4 SQL (linguaxe declarativa)

**Uso**
- HiveQL
- Spark SQL
- Trino / Presto
- BigQuery / Athena
- ClickHouse

**Por que √© clave**
- Familiar para usuarios
- Permite optimizaci√≥n autom√°tica
- Ideal para BI e anal√≠tica

**Situaci√≥n actual**
- Absolutamente central
- Lingua franca do dato

üëâ Non √© un motor, pero si a interface m√°is usada

---

### 1.5 C++ (alto rendemento)

**Uso**
- Motores MPP
- ClickHouse
- Motores de ML

**Caracter√≠sticas**
- M√°ximo control de rendemento
- Baixa latencia

üëâ Usado internamente, raramente polo usuario final

---

### 1.6 Go (infraestrutura moderna)

**Uso**
- Kubernetes
- Servizos distribu√≠dos
- Infraestrutura de datos

üëâ Linguaxe do contorno, non do ETL

---

### 1.7 Rust (tendencia emerxente)

**Uso**
- DataFusion
- Polars
- Motores anal√≠ticos modernos

üëâ Posible futuro dos motores

---

### 1.8 Linguaxes hist√≥ricas ou residuais
- **Pig Latin**: linguaxe de alto nivel para Hadoop (obsoleta)
- **Scripts Unix (bash, awk)**: valor hist√≥rico e pedag√≥xico

---

### 1.9 Resumo cronol√≥xico

```
2005‚Äì2010 ‚Üí Java
2010‚Äì2015 ‚Üí Java + Pig + Hive
2015‚Äì2020 ‚Üí Scala + SQL
2020‚Äìhoxe ‚Üí Python + SQL
Futuro    ‚Üí SQL + Python + Rust (interno)
```

---

### 1.10 Resumo por perfil

| Perfil | Linguaxes |
|------|-----------|
| Infraestrutura | Java, Go |
| Enxe√±ar√≠a de datos | Python, SQL, Scala |
| Anal√≠tica / BI | SQL |
| ML distribu√≠do | Python |
| Motores internos | C++, Rust |

---

## 2. Word Count en distintos motores e linguaxes

### 2.1 Hadoop MapReduce (Java)

```java
public static class TokenizerMapper
  extends Mapper<Object, Text, Text, IntWritable> {

  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();

  public void map(Object key, Text value, Context context)
      throws IOException, InterruptedException {
    StringTokenizer itr = new StringTokenizer(value.toString().toLowerCase());
    while (itr.hasMoreTokens()) {
      word.set(itr.nextToken().replaceAll("\\W+", ""));
      if (word.getLength() > 0) context.write(word, one);
    }
  }
}

public static class IntSumReducer
  extends Reducer<Text, IntWritable, Text, IntWritable> {

  public void reduce(Text key, Iterable<IntWritable> values, Context context)
      throws IOException, InterruptedException {
    int sum = 0;
    for (IntWritable v : values) sum += v.get();
    context.write(key, new IntWritable(sum));
  }
}
```

---

### 2.2 Hadoop Streaming (Python)

**mapper.py**
```python
#!/usr/bin/env python3
import sys, re
word_re = re.compile(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9']+")
for line in sys.stdin:
    for w in word_re.findall(line.lower()):
        print(f"{w}\t1")
```

**reducer.py**
```python
#!/usr/bin/env python3
import sys
cur, acc = None, 0
for line in sys.stdin:
    w, c = line.rstrip("\n").split("\t", 1)
    c = int(c)
    if cur == w:
        acc += c
    else:
        if cur is not None:
            print(f"{cur}\t{acc}")
        cur, acc = w, c
if cur is not None:
    print(f"{cur}\t{acc}")
```

---

### 2.3 Pig Latin

```pig
lines = LOAD '/ruta/input' USING TextLoader() AS (line:chararray);
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(LOWER(line))) AS word;
grouped = GROUP words BY word;
wc = FOREACH grouped GENERATE group AS word, COUNT(words) AS count;
STORE wc INTO '/ruta/output' USING PigStorage('\t');
```

---

### 2.4 Hive (SQL)

```sql
CREATE EXTERNAL TABLE lines(line STRING)
STORED AS TEXTFILE
LOCATION '/ruta/input';

SELECT word, COUNT(*) AS cnt
FROM (
  SELECT explode(split(lower(line), '\\s+')) AS word
  FROM lines
) t
WHERE word <> ''
GROUP BY word;
```

---

### 2.5 Spark (PySpark DataFrame)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, lower, col

spark = SparkSession.builder.getOrCreate()
df = spark.read.text("hdfs:///ruta/input")

wc = (df.select(explode(split(lower(col("value")), r"\\s+")).alias("word"))
        .where(col("word") != "")
        .groupBy("word").count())

wc.write.mode("overwrite").csv("hdfs:///ruta/output")
```

---

### 2.6 Spark (Scala RDD)

```scala
val rdd = sc.textFile("hdfs:///ruta/input")
val wc = rdd.flatMap(_.toLowerCase.split("\\s+"))
            .filter(_.nonEmpty)
            .map(w => (w, 1))
            .reduceByKey(_ + _)
wc.saveAsTextFile("hdfs:///ruta/output")
```

---

### 2.7 Spark SQL

```sql
SELECT word, COUNT(*) AS cnt
FROM (
  SELECT explode(split(lower(value), '\\s+')) AS word
  FROM lines
) t
WHERE word <> ''
GROUP BY word;
```

---

### 2.8 Flink SQL (conceptual)

```sql
SELECT word, COUNT(*) AS cnt
FROM (
  SELECT LOWER(w) AS word
  FROM lines, LATERAL TABLE(STRING_SPLIT(line, ' ')) AS T(w)
) t
WHERE word <> ''
GROUP BY word;
```

---

### 2.9 Trino / Presto (SQL)

```sql
SELECT word, COUNT(*) cnt
FROM (
  SELECT w AS word
  FROM your_table
  CROSS JOIN UNNEST(regexp_split(lower(line), '\\s+')) AS t(w)
) x
WHERE word <> ''
GROUP BY word;
```

---

### 2.10 Dask (Python)

```python
import dask.bag as db
import re

b = db.read_text("el_quijote.txt")
words = b.flatmap(lambda s: re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9']+", s.lower()))
wc = words.frequencies()
print(wc.compute()[:20])
```

---

### 2.11 Ray (Python)

```python
import ray, re
from collections import Counter

ray.init()

@ray.remote
def count_words(lines):
    c = Counter()
    for s in lines:
        c.update(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9']+", s.lower()))
    return c

lines = open("el_quijote.txt", encoding="utf-8", errors="ignore").read().splitlines()
chunks = [lines[i:i+5000] for i in range(0, len(lines), 5000)]
parts = ray.get([count_words.remote(ch) for ch in chunks])

total = Counter()
for p in parts:
    total.update(p)

print(total.most_common(20))
```

---

## 3. Mensaxe clave para o alumnado

> Non escolles a linguaxe porque sexa distribu√≠da,  
> sen√≥n porque o motor distribu√≠do a executa.


