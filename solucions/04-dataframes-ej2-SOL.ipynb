{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d5669b-b8c9-43c3-9931-5f95ff18782e",
   "metadata": {},
   "source": [
    "# Ejercicios Dataframes 2\n",
    "Realiza los siguientes ejercicios. Soluciona cada uno de ellos empleando la DataFrame API y Spark SQL\n",
    "1. Inicializa la variable spark (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b159a865-1442-4d35-a8dd-056a550455b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-949f2da9-2fd7-45a0-b8b3-c6f1545adf26;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.1.0/delta-spark_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.1.0!delta-spark_2.12.jar (734ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.7/spark-sql-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7!spark-sql-kafka-0-10_2.12.jar (144ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.5.1!kafka-clients.jar (581ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.1.0/delta-storage-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.1.0!delta-storage.jar (84ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (113ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.7/spark-token-provider-kafka-0-10_2.12-3.5.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7!spark-token-provider-kafka-0-10_2.12.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (96ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (104ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (2960ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2031ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.5!snappy-java.jar(bundle) (340ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (87ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (90ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.5.5-1!zstd-jni.jar (667ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (178ms)\n",
      ":: resolution report :: resolve 8925ms :: artifacts dl 8316ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   15  |   15  |   3   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-949f2da9-2fd7-45a0-b8b3-c6f1545adf26\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (68764kB/49ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.7\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print (spark.version)\n",
    "#SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857de07-bca6-4560-8980-63f34adcec18",
   "metadata": {},
   "source": [
    "2. Crea un dataframe llamado mi_df con los datos del archivo *data/retail-data/all/online-retail-dataset.csv* (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8406cbab-5e1d-4b65-a2ec-436422a996c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|       NULL|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|       NULL|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|       NULL|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "# Definir el esquema\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "mi_df = spark.read.csv(\"hdfs:///data/retail-data/all/online-retail-dataset.csv\", header=True, schema=schema)\n",
    "mi_df.show(5)\n",
    "mi_df.printSchema()\n",
    "\n",
    "# Leer el archivo CSV con el esquema especificado\n",
    "\n",
    "#SQL\n",
    "mi_df.createOrReplaceTempView(\"retail\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8acc9b-b1d0-46db-9bcc-6e290827d05c",
   "metadata": {},
   "source": [
    "3. Cuenta el número de celdas totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065e4f9-43d1-4e62-a0e3-113b5afe566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "#DataFrame API\n",
    "mi_df.select(count(\"*\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select count(*) from retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c134111-a5cb-473d-bee7-c15a4d0e5d46",
   "metadata": {},
   "source": [
    "4. Cuenta el número de \"invoiceNo\" distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2da2f-911b-46cd-92b3-e15314dd7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame API\n",
    "print(mi_df.select(\"InvoiceNo\").distinct().count())\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select count(distinct InvoiceNo) from retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69047d7e-2a4f-4536-b238-994e4d8a5686",
   "metadata": {},
   "source": [
    "5. Obtén el número de factura más bajo y el más alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc87d54-2b9e-44ae-8ccc-de8b1c687f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max_factura|min_factura|\n",
      "+-----------+-----------+\n",
      "|     581587|     536365|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>          (9 + 1) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|max(InvoiceNo)|min(InvoiceNo)|\n",
      "+--------------+--------------+\n",
      "|        581587|        536365|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "mi_df.selectExpr(\"max(InvoiceNo) as max_factura\", \"min(InvoiceNo) as min_factura\").show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select max(InvoiceNo), min(InvoiceNo) from retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e648e-e749-4836-83b4-cac727e3b4d8",
   "metadata": {},
   "source": [
    "6. Obtén la suma de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "685348d8-f9cb-4f3b-a44b-437d7816b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             suma|\n",
      "+-----------------+\n",
      "|2245715.474000272|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==============================================>          (9 + 1) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             suma|\n",
      "+-----------------+\n",
      "|2245715.474000272|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "#DataFrame API\n",
    "mi_df.filter(col(\"Country\")==\"United Kingdom\").select(sum(\"UnitPrice\").alias(\"suma\")).show()\n",
    "spark.sql(\"select sum(UnitPrice) as suma from retail where Country = 'United Kingdom'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f81fcd-0908-4ddd-a075-432aafbfb427",
   "metadata": {},
   "source": [
    "7. Obtén la media de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ba9fc37-19e4-469f-968c-c238a7e055bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            media|\n",
      "+-----------------+\n",
      "|4.532422174143497|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=========================================>               (8 + 1) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            media|\n",
      "+-----------------+\n",
      "|4.532422174143497|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "#DataFrame API\n",
    "mi_df.filter(col(\"Country\")==\"United Kingdom\").select(avg(\"UnitPrice\").alias(\"media\")).show()\n",
    "\n",
    "#SQL\n",
    "\n",
    "spark.sql(\"select avg(UnitPrice) as media from retail where Country = 'United Kingdom'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065cca6-b302-4f60-9a45-8d841bea0611",
   "metadata": {},
   "source": [
    "8. Obtén la el número total de productos vendidos (quantity) agrupado por países (muestra los 10 primeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbedc383-4684-4014-b3c7-48e4d6296cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|        Country|sum(quantity)|\n",
      "+---------------+-------------+\n",
      "|      Australia|        83653|\n",
      "|        Denmark|         8188|\n",
      "|    Switzerland|        30325|\n",
      "|        Belgium|        23152|\n",
      "|           EIRE|       142637|\n",
      "|    Netherlands|       200128|\n",
      "|         Israel|         4353|\n",
      "|        Austria|         4827|\n",
      "|          Japan|        25218|\n",
      "|Channel Islands|         9479|\n",
      "|         Cyprus|         6317|\n",
      "|        Finland|        10666|\n",
      "|        Bahrain|          260|\n",
      "|          Spain|        26824|\n",
      "|         Poland|         3653|\n",
      "|        Iceland|         2458|\n",
      "|         Norway|        19247|\n",
      "|        Germany|       117448|\n",
      "|         Sweden|        35637|\n",
      "| United Kingdom|      4263829|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------------+\n",
      "|        Country|sum(Quantity)|\n",
      "+---------------+-------------+\n",
      "|      Australia|        83653|\n",
      "|        Denmark|         8188|\n",
      "|    Switzerland|        30325|\n",
      "|        Belgium|        23152|\n",
      "|           EIRE|       142637|\n",
      "|    Netherlands|       200128|\n",
      "|         Israel|         4353|\n",
      "|        Austria|         4827|\n",
      "|          Japan|        25218|\n",
      "|Channel Islands|         9479|\n",
      "|         Cyprus|         6317|\n",
      "|        Finland|        10666|\n",
      "|        Bahrain|          260|\n",
      "|          Spain|        26824|\n",
      "|         Poland|         3653|\n",
      "|        Iceland|         2458|\n",
      "|         Norway|        19247|\n",
      "|        Germany|       117448|\n",
      "|         Sweden|        35637|\n",
      "| United Kingdom|      4263829|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "mi_df.groupBy(\"Country\").sum(\"quantity\").show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT Country, sum(Quantity) FROM retail GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafc7bc-922f-46a2-a1cb-7c5c78b1014f",
   "metadata": {},
   "source": [
    "9. Obtén la media de los precios unitarios de los productos vendidos agrupada por países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ede03e-6e12-4b14-804a-5deac392647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Dataframes:\n",
      "+---------------+------------------+\n",
      "|        Country|    avg(UnitPrice)|\n",
      "+---------------+------------------+\n",
      "|      Australia|3.2206115965051607|\n",
      "|        Denmark|  3.25694087403599|\n",
      "|    Switzerland|  3.40344155844156|\n",
      "|        Belgium|3.6443354277428726|\n",
      "|           EIRE| 5.911077354807234|\n",
      "|    Netherlands|2.7383171657528482|\n",
      "|         Israel| 3.633131313131314|\n",
      "|        Austria| 4.243192019950122|\n",
      "|          Japan|2.2761452513966476|\n",
      "|Channel Islands| 4.932124010554089|\n",
      "|         Cyprus| 6.302363344051452|\n",
      "|        Finland| 5.448705035971222|\n",
      "|        Bahrain| 4.556315789473685|\n",
      "|          Spain| 4.987544413738653|\n",
      "|         Poland| 4.170879765395893|\n",
      "|        Iceland|2.6440109890109897|\n",
      "|         Norway|6.0120257826887675|\n",
      "|        Germany| 3.966929963138483|\n",
      "|         Sweden| 3.910887445887446|\n",
      "| United Kingdom| 4.532422174143497|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "SQL:\n",
      "+---------------+------------------+\n",
      "|        Country|    avg(UnitPrice)|\n",
      "+---------------+------------------+\n",
      "|      Australia|3.2206115965051607|\n",
      "|        Denmark|  3.25694087403599|\n",
      "|    Switzerland|  3.40344155844156|\n",
      "|        Belgium|3.6443354277428726|\n",
      "|           EIRE| 5.911077354807234|\n",
      "|    Netherlands|2.7383171657528482|\n",
      "|         Israel| 3.633131313131314|\n",
      "|        Austria| 4.243192019950122|\n",
      "|          Japan|2.2761452513966476|\n",
      "|Channel Islands| 4.932124010554089|\n",
      "|         Cyprus| 6.302363344051452|\n",
      "|        Finland| 5.448705035971222|\n",
      "|        Bahrain| 4.556315789473685|\n",
      "|          Spain| 4.987544413738653|\n",
      "|         Poland| 4.170879765395893|\n",
      "|        Iceland|2.6440109890109897|\n",
      "|         Norway|6.0120257826887675|\n",
      "|        Germany| 3.966929963138483|\n",
      "|         Sweden| 3.910887445887446|\n",
      "| United Kingdom| 4.532422174143497|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "#DataFrame API\n",
    "print(\"API Dataframes:\")\n",
    "mi_df.groupBy(\"Country\").avg(\"UnitPrice\").show()\n",
    "\n",
    "#SQL\n",
    "print(\"SQL:\")\n",
    "spark.sql(\"SELECT Country, avg(UnitPrice) FROM retail GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62faa914-1b71-4051-b2e3-e1e1454adada",
   "metadata": {},
   "source": [
    "10. Obtén el importe total (quantity * unit price) agrupado por número de factura (invoiceNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ef1acd-345e-4907-a5ab-107a408b999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Dataframes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|InvoiceNo|        sum(total)|\n",
      "+---------+------------------+\n",
      "|   536533|156.09999999999997|\n",
      "|   537049|            277.65|\n",
      "|   537057|274.66999999999996|\n",
      "|   537155| 75.64000000000001|\n",
      "|   537256|2263.7999999999993|\n",
      "|   537350|            419.06|\n",
      "|   537419|510.94999999999993|\n",
      "|   537800|             69.79|\n",
      "|   537817|             168.6|\n",
      "|   537842|159.51000000000002|\n",
      "|   538077| 686.1499999999999|\n",
      "|   538096|3.7199999999999998|\n",
      "|   538133|               0.0|\n",
      "|   538868|            824.02|\n",
      "|   539064|              58.7|\n",
      "|   539246|104.48000000000002|\n",
      "|   539561| 339.7200000000001|\n",
      "|   539637|112.68000000000002|\n",
      "|   539660|               9.2|\n",
      "|   539958| 5262.059999999995|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "SQL:\n",
      "+---------+---------------------------+\n",
      "|InvoiceNo|sum((Quantity * UnitPrice))|\n",
      "+---------+---------------------------+\n",
      "|   536533|         156.09999999999997|\n",
      "|   537049|                     277.65|\n",
      "|   537057|         274.66999999999996|\n",
      "|   537155|          75.64000000000001|\n",
      "|   537256|         2263.7999999999993|\n",
      "|   537350|                     419.06|\n",
      "|   537419|         510.94999999999993|\n",
      "|   537800|                      69.79|\n",
      "|   537817|                      168.6|\n",
      "|   537842|         159.51000000000002|\n",
      "|   538077|          686.1499999999999|\n",
      "|   538096|         3.7199999999999998|\n",
      "|   538133|                        0.0|\n",
      "|   538868|                     824.02|\n",
      "|   539064|                       58.7|\n",
      "|   539246|         104.48000000000002|\n",
      "|   539561|          339.7200000000001|\n",
      "|   539637|         112.68000000000002|\n",
      "|   539660|                        9.2|\n",
      "|   539958|          5262.059999999995|\n",
      "+---------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "#DataFrame API\n",
    "print(\"API Dataframes:\")\n",
    "mi_df.withColumn(\"total\", expr(\"Quantity * UnitPrice\")).groupBy(\"InvoiceNo\").sum(\"total\").show()\n",
    "#SQL\n",
    "print(\"SQL:\")\n",
    "spark.sql(\"SELECT InvoiceNo, sum (Quantity * UnitPrice) FROM retail GROUP BY InvoiceNo\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125443d3-7d2c-4cec-b97e-181a87ef4574",
   "metadata": {},
   "source": [
    "11. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. (sólo mostrar los alumnos que tengan nota en las 3 asignaturas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2110e4ec-d5e7-4289-98ad-414a8880b0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API DataFrames: \n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n",
      "SQL: \n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "\n",
    "print (\"API DataFrames: \")\n",
    "notas_fisica = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"/data/notas/notas_fisica.txt\").toDF(\"alumno\",\"nota_fisica\")\n",
    "notas_ingles = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"/data/notas/notas_ingles.txt\").toDF(\"alumno\",\"nota_ingles\")\n",
    "notas_matematicas = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"/data/notas/notas_mates.txt\").toDF(\"alumno\",\"nota_matematicas\")\n",
    "\n",
    "\n",
    "notas_join = notas_fisica.join(notas_matematicas, \"alumno\", \"inner\").join(notas_ingles, \"alumno\", \"inner\")\n",
    "notas_join.show()\n",
    "#.join(notas_ingles)\n",
    "#notas_join.show()\n",
    "\n",
    "#SQL\n",
    "print (\"SQL: \")\n",
    "notas_fisica.createOrReplaceTempView(\"fisica\")\n",
    "notas_matematicas.createOrReplaceTempView(\"matematicas\")\n",
    "notas_ingles.createOrReplaceTempView(\"ingles\")\n",
    "spark.sql(\"SELECT fisica.alumno, nota_fisica, nota_matematicas, nota_ingles FROM fisica, matematicas, ingles WHERE fisica.alumno = matematicas.alumno and matematicas.alumno = ingles.alumno\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68b0a9-90b3-408a-be73-80ed93ab913d",
   "metadata": {},
   "source": [
    "12. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. Si a un alumno le falta alguna de las notas aparecerá el valor NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88c5b68-5482-4035-a5a2-d08a30c11776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API DataFrames: \n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|    Pedro|          2|             5.0|       NULL|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n",
      "SQL:\n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|    Pedro|          2|             5.0|       NULL|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "print(\"API DataFrames: \")\n",
    "notas_outer_join = notas_fisica.join(notas_matematicas, \"alumno\", \"full_outer\").join(notas_ingles, \"alumno\", \"full_outer\")\n",
    "notas_outer_join.show()\n",
    "\n",
    "#SQL\n",
    "print (\"SQL:\")\n",
    "spark.sql(\"SELECT fisica.alumno, nota_fisica, nota_matematicas, nota_ingles FROM fisica FULL OUTER JOIN matematicas ON fisica.alumno = matematicas.alumno FULL OUTER JOIN ingles ON coalesce(fisica.alumno, matematicas.alumno) = ingles.alumno\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1fdd6-7722-4069-980f-143c35072281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
